--- 
title: "MSP4092: Biomatemática: aspectos quantitativos da vida"
subtitle: "Aula 9: Probabilidade" 
author: |
  | [José Siqueira](https://sites.google.com/usp.br/profjosesiqueira){target="_blank"}
date: "`r format(Sys.time(), format='%d %B %Y %H:%Mh')`"
output:
  html_document:
    css: style.css
    footer: "Biomatematica_Aula09.Rmd"
    font_adjustment: 1
    df_print: tibble
    highlight: pygments
    theme: cerulean
    number_sections: no
    toc: yes
    toc_title: Sumário
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: no
  slidy_presentation:
    css: style.css
    footer: "Biomatematica_Aula09.Rmd"
    font_adjustment: -1
    df_print: tibble
    highlight: pygments
    theme: cerulean
    number_sections: no
    toc: yes
    toc_title: Sumário
    toc_depth: 4
    toc_float:
      collapsed: yes
      smooth_scroll: no
---

```{r set-options, echo=FALSE, cache=FALSE}
options(width=80)
```

```{css, echo=FALSE}
.code {
  font-size: 18px;
  background-color: white;
  border: 2px solid darkgray;
  font-weight: bold;
  max-width: none !important;
}
.output {
  font-size: 18px;
  background-color: white;
  border: 2px solid black;
  font-weight: bold;
  max-width: none !important;
}
.main-container {
  max-width: none !important;
}
pre {
  max-height: 500px !important;
  overflow-y: auto !important;
  overflow-x: scroll !important;
}
.bgobs {
  background-color: #a0d8d8;
}
.bgcodigo {
  background-color: #eeeeee;
}
.bgsaida {
  background-color: #ecf7db;
}
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=TRUE,
                      echo=TRUE, 
                      fig.width=7, 
                      fig.height=6,
                      fig.align="center",
                      comment=NA,
                      class.source="code",
                      class.output="output")
```

```{r}
invisible(Sys.setlocale("LC_CTYPE", "pt_BR.UTF-8"))
invisible(Sys.setlocale("LC_ALL","pt_BR.UTF-8"))
```

```{r,eval=TRUE,echo=FALSE}
systoper <- Sys.info()[[1]]
if (systoper == "Linux")
{
  # Troque para o executavel de onde esta instalado o scilab em seu computador
  executable <- file.path("","home","silveira","Scilab","bin","scilab")
  parameter <- "-nw"
}
# Windows
if (systoper == "Windows")
{
  # Troque para o executavel de onde esta instalado o scilab em seu computador
  executable <- file.path("D:", "Usuarios", "Jose", "scilab2023", "bin", "Scilex")
  parameter <- ""
}
```

```{r,eval=TRUE,echo=FALSE}
eng_scilab <- function(options) {
code <- stringr::str_c(options$code, collapse = '\n')
if (options$eval) 
{
  cmd <- sprintf("%s %s -e %s",
                 executable,
                 parameter,
                 shQuote(code,type="cmd"))
  out <- system(cmd, intern = TRUE)
}else{out <- "output when eval=FALSE and engine='scilab'"}

knitr::engine_output(options, options$code, out)

}

knitr::knit_engines$set(scilab=eng_scilab)
```

```{r eval=TRUE,  echo=TRUE, warning=FALSE, error=FALSE}
options(warn=-1)
suppressMessages(library(knitr, warn.conflicts=FALSE))
suppressMessages(library(sets, warn.conflicts=FALSE))
suppressMessages(library(DescTools, warn.conflicts=FALSE))
suppressMessages(library(vcd, warn.conflicts=FALSE))
suppressMessages(library(HH, warn.conflicts=FALSE))
suppressMessages(library(PearsonDS, warn.conflicts=FALSE))
suppressMessages(library(distributional, warn.conflicts=FALSE))
```

# Material

* HTML de R Markdown em [`RPubs`](http://rpubs.com/josiqueira/){target="_blank"}
* Arquivos em [`GitHub`](https://github.com/josiqueira/BioMat){target="_blank"}
* [Prof. José Siqueira: ResearchGate](https://www.researchgate.net/profile/Jose-Siqueira-18){target="_blank"}

# Pensamento

"Os organismos são maximizadores de aptidão abrangente causada pela seleção natural. Os organismos comportam-se como se fossem orientados por estatísticos tomando decisões em condição de incerteza."

> Grafen, A (2006) Optimization of inclusive fitness. _Journal of Theoretical Biology_ 238: 541–63.

# Conteúdo

1.  Número real (Capítulo 1)
2.	Função e relação (Capítulo 3)
3.	Funções potência, periódica, exponencial e logarítmica (Capítulos 4, 5 e 6)
4.	Método gráfico (Capítulo 7)
5.	Série e limite (Capítulo 8)
6.	Derivada e integral (Capítulo 9 e 10)
7.	Equação diferencial ordinária (ODE) (Capítulo 11)
8.	Função de duas ou mais variáveis independentes (Capítulo 12)

__9.	Probabilidade (Capítulo 13)__

10.	Matriz e vetor (Capítulo 14)

# Introdução

Considere o processo de reprodução sexuada. Entre os espermatozóides de um homem, as células diferem em sua mensagem genética, e o mesmo vale para as células reprodutivas de um ovário. Como consequência, as características dos descendentes diferem de muitas maneiras. Como depende do acaso qual das células reprodutivas se combinam para se tornar uma célula fertilizada, o resultado não pode ser previsto. No entanto, não há caos. Se contarmos traços particulares em um grande número de descendentes, encontraremos algumas regras. Por exemplo, vemos que um traço conhecido como "heterozigoto" ocorre nos descendentes em uma proporção previsível. As bem conhecidas regras de Mendel podem ser melhor formuladas usando a teoria da probabilidade.

As leis da herança foram a primeira grande aplicação da probabilidade nas ciências da vida. Hoje conhecemos muito mais aplicações: ocorrência de mutações, risco de doenças, chance de sobrevivência, distribuição e interação de espécies, etc.

A aplicação mais importante, no entanto, é feita em estatística. Nenhuma observação e nenhum experimento pode ser planejado e analisado com precisão sem algum método estatístico. Mesmo se mantivermos as condições experimentais tão constantes quanto possível, a repetição de uma observação ou de um experimento dificilmente resultará exatamente no mesmo resultado. Sempre há oscilações. Portanto, todas as conclusões baseadas em dados empíricos são necessariamente infligidas pela incerteza. Tentamos expressar o grau de incerteza em termos de probabilidade. Assim, se um experimentador relata "significância em um nível de cinco por cento", ele admite a possibilidade de uma afirmação errônea, mas ao mesmo tempo afirma que a "probabilidade" de um erro é de no máximo cinco por cento. Um conhecimento básico de probabilidade é necessário antes que os métodos estatísticos possam ser compreendidos.

Os únicos pré-requisitos para o estudo das Seções 13.2-13.8 são os Capítulos 1 e 2. As seções posteriores, entretanto, requerem algum conhecimento de funções e de cálculo.

# Eventos

O resultado de uma única observação ou resultado de uma medição é geralmente chamado de _evento_. Quando uma ave migratória é vista selecionando uma direção, digamos nordeste, ou quando lemos o comprimento de uma célula como 1.2 &mu;m, estamos em ambos os casos observando um evento.

Alguns eventos podem ser decompostos em "eventos simples". Se observarmos cuidadosamente um pássaro desaparecendo no horizonte, somos capazes de distinguir entre os azimutes 0°, 10°, 15° etc. Esses ângulos podem então ser chamados de eventos simples, enquanto "nordeste" é considerado um evento composto.
Compreende o evento simples 35°, 40°, 45°, 50°, 55°. Da mesma forma, se em um estudo de saúde todas as pessoas com pressão arterial sistólica acima de 160 mm são chamadas de doentes, este é um evento composto em comparação com as medições individuais da pressão arterial que são chamadas de eventos simples.

Dois eventos simples diferentes não podem ocorrer ao mesmo tempo. Eles se excluem. Portanto, chamamos esses eventos de mutuamente excludentes.

Por outro lado, eventos compostos podem ocorrer simultaneamente. Por exemplo, os eventos $x>5$ e $x<8$ não são excludentes se $x$ pode assumir valores como 6 e 7. 

Da mesma forma, em um estudo de comportamento animal, um cão pode ser observado como estando acordado e latindo. Os dois eventos "acordar" e "latir" não são mutuamente excludentes.

Agora consideramos um experimento particular e listamos todos os eventos simples possíveis. Chamamos o conjunto desses eventos de espaço de resultados ou amostral.

A terminologia "espaço amostral" é mais freqüentemente usada. No entanto, espera-se que o "espaço de resultado" cause menos confusão. Com a palavra "espaço" associamos geralmente o conceito de espaço euclidiano em duas ou três dimensões. Mas na matemática a palavra "espaço" é frequentemente usada no sentido geral de "conjunto do universo".

Em termos de teoria dos conjuntos, um evento simples é um membro do espaço de resultados. Entretanto, costuma-se identificar um evento simples também com um subconjunto contendo apenas um elemento. Assim, no espaço de resultados {A, B, C, D} podemos falar do evento A ou do evento {A}.

Da mesma forma, como um evento composto compreende alguns eventos simples, podemos identificar o evento composto com um subconjunto do espaço de resultados. Podemos dizer "o evento A ou B" ou "o evento {A, B}".

## Exemplo 13.2.1: Experimento de lançamento de uma moeda 

No lançamento de uma moeda, consideramos apenas dois resultados possíveis: cara (H) e coroa (T). São eventos exclusivos. Portanto, o espaço resultante é o conjunto {H, T}.

Se lançarmos duas moedas distintas ao mesmo tempo, observaremos pares ordenados (H, H), (H, T), (T, H), (T, T) para os quais também escreveremos HH, HT, TH, TT. Os quatro eventos são mutuamente excludentes. Assim, o espaço de resultados é  {HH, HT, TH, TT}.

## Exemplo 13.2.2: Dois alelos em _locus_ gênico

Em genética, suponha que haja dois alelos A e a para preencher um determinado locus gênico. Então, conhecemos apenas três resultados possíveis: AA, Aa, aa. O par Aa não é ordenado, pois não é possível distinguir entre dois genótipos diferentes Aa e aA. Em outras palavras, não importa se a ou A veio do pai ou da mãe. Portanto, o espaço resultante é o conjunto {AA, Aa, aa}.

## Exemplo 13.2.3: Estatura de humano macho

A estatura _H_ dos machos humanos adultos pode variar de 120 cm a 250 cm. Não temos certeza, entretanto, se 120 cm é o menor e 250 cm o maior valor. Por outro lado, para definir um conjunto deve ficar claro se um elemento pertence a um conjunto ou não. Para superar a dificuldade, usamos a palavra "evento possível" de maneira liberal e incluímos eventos que são conceitualmente, mas não possíveis na prática. Permitimos para _H_ todos os valores positivos. Portanto, o espaço de resultados para a estatura de homens adultos é definido por {_H_ | _H_ > 0} .

Este espaço é um conjunto infinito não enumerável (número real) por duas razões. Em primeiro lugar, não há limite superior para _H_. Em segundo lugar, assumimos de forma bastante artificial que as medições podem ser realizadas com qualquer grau de precisão.

Isso implica que todo intervalo finito como [150 cm, 200 cm] contém um número infinito não enumerável de valores. O exemplo também serve para indicar que um espaço de resultado pode frequentemente ser definido de maneiras diferentes para o mesmo tipo de observação ou experimento.

## Exemplo 13.2.4: Homozigoto

Se {AA, Aa, aa} é o espaço de resultado para o modelo genético de dois alelos, o evento "homozigoto" consiste nos eventos simples AA e aa. Assim, o evento é um evento composto e pode ser identificado com o subconjunto {AA, aa}. Usando o símbolo para "contido em" introduzido na Seção 2.3, podemos escrever: {AA, aa} &sub; {AA, Aa, aa}.

$\Diamond$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

* [`Subsets[{H,T}]`](https://www.wolframalpha.com/input?i=Subsets%5B%7BH%2CT%7D%5D){target="_blank"}

* [`Subsets[{a,b,c}]`](https://www.wolframalpha.com/input?i=Subsets%5B%7Ba%2Cb%2Cc%7D%5D){target="_blank"}

* [`Union[{a,a,b,b,b,c,c,c,c}]`](https://www.wolframalpha.com/input?i=Union%5B%7Ba%2Ca%2Cb%2Cb%2Cb%2Cc%2Cc%2Cc%2Cc%7D%5D){target="_blank"}

* [`Union[{a, b, a, c}, {d, a, e, b}, {c, a}]`](https://www.wolframalpha.com/input?i=Union%5B%7Ba%2C+b%2C+a%2C+c%7D%2C+%7Bd%2C+a%2C+e%2C+b%7D%2C+%7Bc%2C+a%7D%5D){target="_blank"}

* [`Intersection[{a, b, a, c}, {d, a, e, b}, {c, a}]`](https://www.wolframalpha.com/input?i=Intersection%5B%7Ba%2C+b%2C+a%2C+c%7D%2C+%7Bd%2C+a%2C+e%2C+b%7D%2C+%7Bc%2C+a%7D%5D){target="_blank"}

* [`(A union B) intersect C`](https://www.wolframalpha.com/input?i=%28A+union+B%29+intersect+C){target="_blank"}

* [`is (X intersect Y) a subset of Y?`](https://www.wolframalpha.com/input?i=is+%28X+intersect+Y%29+a+subset+of+Y%3F){target="_blank"}

* [`is (X intersect Y) a subset of Y?`]((X intersection (Y union Z union W)) subset (Y union X)){target="_blank"}

Quando um dado é lançado, o número de pontos na face superior é um evento simples. Existem seis resultados possíveis. Assim, o espaço resultante é {1, 2, 3, 4, 5, 6}. O evento "número par" é o subconjunto {2, 4, 6}.

Outro subconjunto {3, 4, 5, 6} significa o evento de obtenção de 3, 4, 5 ou 6, ou seja, um número que é pelo menos 3.

Para resumir, eventos simples e compostos são subconjuntos do espaço de resultados.

Para obter mais informações, aplicamos a álgebra de conjuntos. Como exemplo ilustrativo, usamos novamente um dado e os eventos compostos 

$$\begin{align}
E_1&=\{2,4,6\}\\
E_2&=\{3,4,5,6\}
\end{align}$$

A interseção dos dois conjuntos é $E_1 \cap E_2 = \{4, 6\}$.

Ele contém os resultados 4 e 6. Esses dois resultados são membros comuns a ambos os conjuntos. Quando o evento 4 ocorre, então $E_1$ e $E_2$ acontecem simultaneamente. O mesmo é verdadeiro quando 6 ocorre. Inversamente, quando $E_1$ e $E_2$ acontecem simultaneamente, o resultado deve ser 4 ou 6, ou seja, o evento {4, 6}.

Em geral, a interseção de dois eventos $E_1$ e $E_2$ significa outro evento que ocorre quando $E_1$ e $E_2$ acontecem simultaneamente. Brevemente, $E_1 \cap E_2$ é o evento $E_1 \land E_2$.

Observe o caso especial quando $E_1$ e $E_2$ não têm nenhum membro comum. 
Aqui $E_1 \cap E_2=\emptyset$.

sendo que $\emptyset$ denota o conjunto vazio. Os eventos $E_1$ e $E_2$ não podem ocorrer simultaneamente, e $\emptyset$ pode ser interpretado como o evento impossível.

Chamamos $E_1$ e $E_2$ de eventos mutuamente excludentes. Por exemplo, no problema de lançamento de dados, {2, 4, 6} e {1, 3} são eventos mutuamente excludentes.

Considere agora a união de $E_1 = \{2, 4, 6\}$ e $E_2 = \{3, 4, 5, 6\}$. Obtemos $E_1 \cup E_2 = \{2, 3, 4, 5, 6\}$.

A união contém todos os eventos simples, exceto o número 1. Quando um dos eventos de 2 a 6 ocorre, ocorre $E_1$ ou $E_2$ ou ambos ocorrem. Usemos a palavra "ou" no sentido fraco de "e/ou". Então, em geral, a união de $E_1$ e $E_2$ significa que $E_1$ ou $E_2$ ocorre. Brevemente, $E_1 \cup E_2$ é o evento $E_1 \lor E_2$ .

Os leitores já sabem a correspondência de $\cap$ com "e" e de $\cup$ com "ou".

De algum interesse é o caso especial em que a união de dois eventos é igual ao espaço resultante. Com a notação $\Omega$ (grego maiúsculo ômega) para o espaço de resultados, a relação é $E_1 \cup E_2=\Omega$.

Isso significa que $E_1$ ou $E_2$ deve ocorrer em cada tentativa do experimento.

Portanto, $\Omega$ pode ser interpretado como o evento certo. 

Por exemplo, $E_1 = \{1, 3, 5\}$ e $E_2 = \{1, 2, 4, 6\}$ são dois conjuntos que satisfazem (13.2.4) se 0 = {1, 2, 3, 4, 5, 6} é o espaço de resultados de um dado.

Dois eventos que satisfazem $E_1 \cup E_2=\Omega$ são chamados exaustivos. 

Eles "esgotam" todos os resultados possíveis. 

Um caso ainda mais especial é o mais importante. 

Seja $\Omega$ novamente o espaço resultante de um dado. 

Escolhemos o evento $E = \{1, 5\}$ e perguntamos: Qual evento ocorre sempre que E não ocorre? 

Chamamos esse evento de evento complementar e o denotamos por $\bar{E}$. Em nosso exemplo encontramos $\bar{E} = \{2, 3, 4, 6\}$. 

Este conceito corresponde ao conjunto complementar. Em geral, um conjunto $E$ e seu conjunto complementar $\bar{E}$ satisfazem as seguintes condições:

$$E \cup \bar{E}=\Omega\\
E \cap \bar{E}=\emptyset$$

isto é, $E$ e $\bar{E}$ são exaustivos e mutuamente excludentes.

## Conjunto em R

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r}
print(x <- c(sort(sample(1:20, 9)), NA))
print(y <- c(sort(sample(3:23, 7)), NA))
union(x, y)
intersect(x, y)
setdiff(x, y)
setdiff(y, x)
setequal(x, y)

## True for all possible x & y :
setequal(union(x, y),
        c(setdiff(x, y), intersect(x, y), setdiff(y, x)))

is.element(x, y) # length 10
is.element(y, x) # length  8

s <- sets::set(1L, 2L, 3L)
s
length(s) # cardinality
s <- sets::as.set(1:3)
s
sets::set_is_empty(s)
sets::set_is_empty(set())
# sets::set(1L, 2L) <= sets::set(s) # set1 contido em set2?
1:4 %e% sets::set(1L, 2L, 3L) 

s2 <- sets::as.set(2:4)
s2

sets::set_union(s, sets::set("a"))
s | sets::set("a") # union
sets::set_intersection(s, s2)
s & s2 # intersection
sets::set_symdiff(s, s2)
s %D% s2 # symmetric difference
sets::set_complement(sets::set(1L), s)

s * s2 # Cartesian product
s ^ 2L
2 ^ s

!sets::set(s)

# subsets
sets::set_combn(s, 0L)
sets::set_combn(s, 1L)
sets::set_combn(s, 2L)
sets::set_combn(s, length(s))

sets::set_outer(set(1,2), set(1,2,3), "/")
```

# Conceito de probabilidade

Existem experimentos que podem ser repetidos muitas vezes sob condições razoavelmente constantes. Tais experimentos são cara ou coroa, a maioria dos experimentos físicos e muitos experimentos em genética. Eles têm uma propriedade comum que descreveremos usando um exemplo ilustrativo.

Consideremos a ocorrência de nascimentos masculinos e femininos. Se desconsiderarmos os hermafroditas, o espaço resultante é simplesmente {&#9794;, &#9792;}.

Seja $n$ o número total de descendentes considerados e $k$ o número de descendentes masculinos. Chamamos de $k$ a frequência absoluta ou simplesmente a frequência do evento "masculino". Devido a flutuações aleatórias, a frequência pode ser qualquer número $0, 1, 2, \ldots , n$ ou seja, $0\le k \le n$.

Particularmente, quando $n$ é pequeno, digamos $n = 3$, a frequência $k$ poderia facilmente assumir os valores extremos 0 e 3. Basta pensarmos em famílias com três filhos, sendo todos meninas ou todos meninos.

Para nos aproximarmos do conceito de probabilidade introduzimos a frequência relativa $h$ pela fórmula $h=k/n$.

Podemos memorizar esta fórmula na forma:

$$\text{frequência relativa do evento} = \dfrac{\text{frequência observada do evento}}{\text{número de replicações}}$$

Enquanto $k$ varia de 0 a $n$, a frequência relativa $h$ varia de 0 a 1: $0 \le h \le 1$.

A frequência relativa geralmente é expressa em porcentagem. Em seguida, varia de 0% a 100%.

Em nosso exemplo, $h$ significa a frequência relativa de nascimentos masculinos. 

Em uma pesquisa cada vez maior, os seguintes números foram obtidos:

$$\begin{align}
n &= 10   \; 100  \; 1000  \; 10000  \; 100000\\
k &= 7  \;  57  \;  512 \;   5293  \;  52587\\
h &= 0.7 \; 0.57 \; 0.51  \; 0.529 \;  0.526
\end{align}$$

Os números da última linha são adequadamente arredondados. Como $n$ tende ao infinito, a frequência relativa parece aproximar-se de um certo limite. Chamamos essa propriedade empírica de estabilidade da frequência relativa.

Esta propriedade, no entanto, não está bem definida. Observe que a maneira como $h$ se aproxima de um limite é bastante irregular.

Podemos teorizar e assumir que existe um número fixo, digamos $p$, que é aproximado por $h$ em um longo período de observações. O número hipotético $p$ é conhecido como a probabilidade do evento em consideração, no nosso caso a probabilidade de um nascimento masculino. Não sabemos o valor numérico exato de $p$, mas nossas observações indicam que $p$ está próximo de 0.53 ou 53%. De outras pesquisas, sabe-se que a proporção entre os sexos não é exatamente 1:1, mas que os nascimentos masculinos são um pouco mais frequentes. Também é sabido que a proporção muda ligeiramente de uma região para outra.

Em estatística, a frequência relativa de um evento é usada para estimar a probabilidade desse evento. Existem regras que demonstram o quão confiável é tal estimativa. O leitor é encaminhado para livros sobre estatísticas.

Seja $E$ um evento de um espaço de resultados. Então a probabilidade de $E$ é um número associado a $E$ e denotado por $P(E)$.

Observe que $P(E)$ é uma função nos eventos de um espaço de resultados $\Omega$.

## Exemplo 13.3.1: Raio-X no tecido vivo

O tecido vivo é exposto a raios-X para produzir mutação. Com tecido do mesmo tipo e uma dose constante de radiação, o experimento pode ser repetido muitas vezes sob condições quase constantes. A frequência relativa das mutações observadas é chamada de taxa de mutação. Essa taxa varia à medida que o experimento continua, mas permanece estável na medida em que se aproxima de um determinado valor. Assumimos que existe algum tipo de limite que chamamos de probabilidade de uma mutação.

## Exemplo 13.3.2: Lançamento de uma moeda honesta

Em alguns casos, somos capazes de prever o valor de uma probabilidade. No lançamento de uma moeda, por exemplo, não vemos razão para que cara (H) ou coroa (T) ocorram com mais frequência. Portanto, supomos que ambos os eventos, cara e coroa, têm probabilidade 1/2. Não há prova matemática possível, mas a experiência indica que nossa suposição está correta. Em símbolos matemáticos: 

$$P(H) =  \dfrac{1}{2}\\
P(T) = \dfrac{1}{2}$$

## Exemplo 13.3.3: Genética homozigótica

Existem numerosos exemplos em genética onde as probabilidades podem ser previstas. Se cruzarmos indivíduos de genótipo AA e Aa, o gene A de um indivíduo AA encontra o gene A ou o gene a de um indivíduo Aa. As células fertilizadas são do genótipo AA e Aa, ou seja, o espaço resultante é {AA, Aa}. Como não temos motivos para supor que um dos dois eventos ocorre com mais frequência do que o outro, prevemos que a probabilidade de qualquer um dos eventos é 1/2. podemos escrever:

$$P(AA) =  \dfrac{1}{2}\\
P(Aa) = \dfrac{1}{2}$$

A suposição é apoiada por fatos experimentais.

## Exemplo 13.3.4: Uniformidade de inteiros

Deixe o espaço resultante consistir em todos os inteiros de 0
a 99, ou seja, $\Omega = \{0, 1,2, ..., 99\}$.

Suponha que nos seja dado um mecanismo que produz uma sequência praticamente infinita de eventos de $\Omega$. Suponha ainda que cada um dos cem números tenha a mesma chance de ocorrer em cada estágio. Tal sequência pode correr como $37, 95, 11, 18, 48, 07, 22, 65, 23, 99, 50, 11, 80, \ldots$.

Em uma sequência longa, cada evento tem aproximadamente a mesma frequência relativa de 1/100. Podemos idealizar e postular que todos os cem eventos são igualmente prováveis, ou seja, $P(0) = P(1) = P(2) = \cdots = P(99) = 100$.

Uma lista de números baseada na suposição de igual probabilidade é chamada de tabela de números aleatórios. Números aleatórios são muito úteis em uma variedade de aplicações.

## Exemplo 13.3.5: População de ser vivo

Consideramos uma população que consiste em um número finito de indivíduos, como uma população humana, uma população animal ou vegetal ou um conjunto de objetos individuais. Para muitos propósitos, muitas vezes temos que fazer uma "seleção aleatória" de um ou mais de $N$ indivíduos. Cada indivíduo deve ter a mesma probabilidade de ser escolhido. Este requisito dificilmente pode ser cumprido sem empregar o seguinte procedimento:

Todos os $N$ indivíduos são rotulados como $1,2,3, \ldots , N$ em uma ordem arbitrária.

Em seguida, uma tabela de números aleatórios é consultada. Se $N$ tiver $k$ dígitos, combinamos colunas para obter números aleatórios de $k$ dígitos. Simplesmente descartamos os números que não ocorrem na sequência $1, 2, \ldots , N$. Então, o primeiro número admissível (ou qualquer número escolhido sem inspeção de seu valor) nos dá uma seleção aleatória de um indivíduo.

Por exemplo, seja $N = 38$. Na Tabela K (Apêndice do livro adotado), o primeiro número que satisfaz o requisito é 12 (primeira coluna, quinta linha). Portanto, nossa seleção aleatória é individual rotulada como 12.

Se um de $N$ indivíduos for escolhido dessa forma, dizemos que ele é selecionado aleatoriamente. O procedimento é chamado de seleção aleatória ou amostragem aleatória.

Cada indivíduo tem a mesma probabilidade $1/N$ de ser selecionado.

Se um segundo indivíduo estiver sendo selecionado, devemos levar em conta que a população diminuiu para $N - 1$ indivíduos na primeira seleção. Portanto, a probabilidade de um segundo indivíduo selecionado aleatoriamente é $1/(N - 1)$. 

Uma observação correspondente é apropriada para um terceiro, quarto, ... indivíduo. Este procedimento é chamado de seleção sem reposição. Com menos frequência, a seleção é realizada substituindo cada indivíduo selecionado. Neste caso de seleção com reposição a probabilidade de um indivíduo ser selecionado permanece $1/N$.

Até que ponto a probabilidade é aplicável nas ciências da vida não pode ser decidido nem por métodos matemáticos nem por fatos experimentais.

É hoje e talvez continue sendo uma questão de controvérsia. Existem cientistas que vão muito além da interpretação de frequência da probabilidade.

Eles aplicam a probabilidade para o grau de crença. Para um evento que é improvável, mas não impossível, eles atribuem uma probabilidade próxima de zero, digamos 0.01, e para um evento quase certo, um valor próximo de 1, digamos 0.99.

Também existem regras para atribuir probabilidades. Esses cientistas acham que perguntas como as seguintes são significativas: "Qual é a probabilidade de vida em Marte?" ou "Qual é a probabilidade de que o problema do câncer possa ser resolvido na próxima década?"

# Axiomas da teoria da probabilidade

Espaço de resultados é o conjunto finito ou infinito de todos os resultados possíveis ou eventos simples.

Seja $\Omega$ denotando tal espaço de resultado, e seja $E_i$, $i = 1,2, \ldots$, alguns eventos pertencentes a $\Omega$. Cada evento pode ser interpretado como um subconjunto de $\Omega$.

Por isso:

$$E_i \subset \Omega$$ 

Associamos uma probabilidade $p_i$ a cada evento que pertence a $\Omega$.

Portanto, podemos escrever:

$$P\left(E_i\right)=p_i$$

Cada probabilidade é uma idealização de uma frequência relativa. 

Portanto, temos que postular que:

$$0 \le p_i \le 1$$

Este é o conteúdo do nosso primeiro axioma.

_Axioma 1_. A cada evento pertencente a um espaço de resultados está associado um número, denominado probabilidade do evento. Este número está restrito ao intervalo de 0 a 1.

Casos especiais são o evento impossível que nunca ocorre e o evento certo que sempre ocorre. As frequências relativas para os dois casos são $h= 0$ e $h= 1$, respectivamente. Postulamos que as probabilidades correspondentes também são 0 e 1. O evento impossível é caracterizado pelo conjunto vazio $\emptyset$ e o evento certo pelo espaço de resultados $\Omega$. Portanto, obtemos o seguinte axioma:

_Axioma 2_. Ao evento impossível está associada a probabilidade 0 e ao evento certo a probabilidade 1. Em símbolos

$$P(\emptyset)=0\\
P(\Omega)=1$$

Sejam $E_1$ e $E_2$ dois eventos pertencentes a um espaço de resultados $\Omega$ e assuma que eles são mutuamente excludentes, ou seja, eventos que não podem ocorrer simultaneamente. 

Em $n$ execuções do experimento observamos a frequência com que $E_1$ e $E_2$ ocorrem. Seja $k_1$ a frequência de $E_1$ e $k_2$ a frequência de $E_2$. As frequências relativas correspondentes são:

$$h_1=\dfrac{k_1}{n}\\
h_2=\dfrac{k_2}{n}$$

Agora perguntamos quantas vezes $E_1$ ou $E_2$ ocorreram. A frequência total é $k_1 + k_2$, pois $E_1$ e $E_2$ não podem acontecer simultaneamente.

Com $E_1 \lor E_2$ definimos um novo evento. Este evento é $E_1 \cup E_2$.

Concluímos que a frequência relativa de $E_1 \cup E_2$ é $h = (k_1 + k_2)/n$. Isso implica

$$h=h_1+h_2$$

Como as probabilidades são frequências relativas idealizadas, postulamos uma fórmula. Isso é expresso pelo seguinte axioma.

_Axioma 3_. Sejam $E_1$ e $E_2$ dois eventos mutuamente excludentes pertencentes a um espaço de resultados Q. Sejam $P_1 = P(E_1)$, $P_2 = P(E_2)$ e $P = (E_1 \cup E_2)$.

Então:

$$p=p_1+p_2$$

O axioma 3 também é chamado de regra da adição. O diagrama de Venn da Fig. 13.1 representa a situação do Axioma 3.

```{r echo=FALSE, out.width="80%", fig.cap="Fig. 13.1. Dois eventos mutuamente excludentes são representados por dois conjuntos de pontos não sobrepostos. Para tais eventos, o Axioma 3 é válido."}
knitr::include_graphics("./image/Fig13.1.png")
```

## Exemplo 13.4.1: Dado de seis faces honesto 1

Chamamos um dado de seis faces honesto se cada um dos seis eventos {1}, {2}, ..., {6} tem probabilidade 1/6. O evento composto {1,2} é a união dos dois eventos mutuamente excludentes {1} e {2}. Por isso:

$$P\left(\{1,2\}\right)=\dfrac{1}{6}+\dfrac{1}{6}=\dfrac{1}{3}$$

## Exemplo 13.4.2: Dado de seis faces honesto 2

A suposição de que $E_1$ e $E_2$ são mutuamente excludentes é mais importante, conforme mostrado no contra-exemplo a seguir. Quando um dado é lançado, os dois eventos $E_1 = {1, 2}$ e $E_2 = {2, 3}$ não são mutuamente excludentes, pois 2 é um resultado comum. O novo evento $E_1 \cup E_2$ é {1, 2, 3}. Para um dado honesto, obtemos $p_1 = P(E_1)= 1/3$, $p_2 = P(E_2)= 1/3$ e $p=P(E_1 \cup E_2)=1/2$. Porém, $p \ne p_1 +p_2$.

## Exemplo 13.4.3: Genética heterozigótica

Em um problema de genética, assumimos que existem apenas dois alelos diferentes, A e a, em um determinado locus. Cruzamos o genótipo Aa e Aa de acordo com a seguinte regra:

```{r echo=FALSE, out.width="85%"}
knitr::include_graphics("./image/pm.png")
```

Uma das regras genéticas afirma que as quatro recombinações AA, Aa, aA, aa são igualmente prováveis. Portanto, atribuímos a cada um deles a probabilidade 1/4. No entanto, as duas recombinações Aa e aA não podem ser distinguidas biologicamente. Assim, perguntamos: Qual é a probabilidade do evento composto "Aa ou aA" para o qual simplesmente escrevemos Aa? Como os dois resultados são mutuamente excludentes, obtemos

$$P(Aa)=\dfrac{1}{4}+\dfrac{1}{4}=\dfrac{1}{2}$$

O resultado do cruzamento Aa x Aa pode então ser resumido na forma

$$\begin{align}
P(AA)&=\dfrac{1}{4}=0.25\\
P(Aa)&=\dfrac{1}{2}=0.50\\
P(aa)&=\dfrac{1}{4}=0.25
\end{align}$$

Voltando à teoria, sejam $E$ e $\bar{E}$ dois eventos complementares.

Aplicando o Axioma 3 obtemos:

$$P(E \cup \bar{E}) = P(E) + P(\bar{E})$$

Mas o evento $E \cup \bar{E}$ é idêntico a $\Omega$ e $P(\Omega) = 1$ de acordo com Axioma 2. Portanto:

$$P(E) + P(\bar\{E\}) = 1$$

Habitualmente, $P(E)$ é denotado por $p$, e $P(\bar{E})$ por $q$. Por isso

$$p+q= 1$$

A propriedade pode ser facilmente generalizada para um número, digamos $m$, de eventos mutuamente excludentes e exaustivos $E_1, E_2, \ldots, E_m$. O resultado é expresso na seguinte proposição:

Sejam $E_1, E_2, \ldots, E_m$, Em $m$ eventos mutuamente excludentes e exaustivos de um espaço de resultados Q com probabilidades $p_1, p_2, \ldots, p_m$, respectivamente. 

Então:

$$p_1 + p_2 + \cdots + p_m=1$$

Às vezes, os eventos $E_1, E_2, \ldots, E_m$ formam uma partição do espaço resultante. A Figura 13.2 apresenta um diagrama de Venn dessa partição.

```{r echo=FALSE, out.width="80%", fig.cap="Fig. 13.2. Partição de um espaço de resultados &Omega; em m eventos mutuamente excludentes E<sub>1</sub> , E<sub>2</sub> , ..., E<sub>m</sub>."}
knitr::include_graphics("./image/Fig13.2.png")
```

## Exemplo 13.4.4: Estatura de homem adulto

Medimos a estatura $H$ (cm) de pessoas adultas do sexo masculino. Para fins mais práticos, subdividimos o espaço amostral nos chamados grupos.

Tal subdivisão pode ser:

$$\begin{align}
E_1 &= \{H|H < 150\} \\
E_2 &= \{H|150 \le H < 160\} \\
E_3 &= \{H|160 \le H < 170\} \\
\vdots \\
E_6 &= \{H|190 \le H < 200\} \\
E_7 &= \{H|H \ge 200\}
\end{align}$$

Os sete eventos ou grupos são mutuamente excludentes e exaustivos. Por isso:

$$P(E_1) + P(E_2) + \cdots + P(E_7) = 1$$

# Probabilidade condicional

Apresentaremos o conceito de probabilidade condicional por meio de dois exemplos ilustrativos.

## Exemplo 13.5.1 Distribuição de genótipo

Considere a distribuição dos genótipos AA, Aa, aa em uma planta ou uma população animal. Suponha que os indivíduos sejam selecionados aleatoriamente. Isso significa que cada indivíduo tem a mesma chance de ser selecionado. O espaço resultante é {AA, Aa, aa}. Cada um dos três eventos tem sua própria probabilidade. Por exemplo, em uma população de 500 indivíduos, 180 são do genótipo AA, 240 do genótipo Aa e 80 do genótipo aa. Então, a probabilidade de selecionar aleatoriamente um indivíduo AA é 180/500 = 0.36, um indivíduo Aa é 240/500 = 0.48 e um indivíduo aa é 80/500 = 0.16.

Brevemente:

$$P(AA) = 0.36, \;P(Aa) = 0.48, \;P(aa) = 0.16$$

As três probabilidades somam 1.

Vamos fazer mais uma suposição: A distribuição é verdadeira apenas para indivíduos jovens. A partir de certa idade, o gene a causa a morte. Se A é dominante sobre a, os indivíduos AA e Aa permanecem saudáveis, enquanto os indivíduos aa morrem. 

A questão agora surge: Quais são as probabilidades de indivíduos AA e Aa após a remoção de todos os indivíduos aa? 

Não podemos mais dizer que a frequência de indivíduos AA é de 36% e a frequência de indivíduos Aa de 48%, pois a soma é inferior a 100%. Somos forçados a fazer um ajuste. Considere a ocorrência relativa expressa pela razão 0.36:0.48.

Pode ser reduzido a 3:4. Temos que encontrar duas probabilidades que satisfaçam esta razão que somam um. O ajuste é feito simplesmente dividindo 3 e 4 por sua soma 7. Assim, $P(AA) = 3/7$ e $P(Aa) = 4/7$.

A mesma operação pode ser realizada com as frações decimais originais.

Assim, as probabilidades ajustadas para os eventos AA e Aa são:

$$\dfrac{0.36}{0.36+0.48}\approx 0.43\\
\dfrac{0.48}{0.36+0.48}\approx 0.57$$

respectivamente. Os valores ajustados são chamados de probabilidades condicionais.

Juntamente com as probabilidades originais, elas são mostradas na Fig. 13.3.

```{r echo=FALSE, out.width="80%", fig.cap="Fig. 13.3. As probabilidades de três genótipos. Quando os indivíduos aa morrem, as probabilidades devem ser ajustadas."}
knitr::include_graphics("./image/Fig13.3.png")
```

A palavra "condicional" requer uma explicação. Em nosso exemplo
a condição é que apenas indivíduos AA e Aa sobrevivam. Esta condição é caracterizada pelo evento composto $E = \{AA, Aa\}$.

O conjunto E pode ser considerado como um novo espaço de resultados.

Dentro deste espaço definimos novas probabilidades que são denotadas por $P(AA|E)$ e $P(Aa|E)$.

Elas são chamadas de probabilidades condicionais. A barra vertical é usada de maneira semelhante à da teoria dos conjuntos. Podemos ler a barra "sob a condição de que" ou brevemente "dado que". Com a nova notação em mente, o ajuste pode ser escrito na forma:

$$P(AA|E)=\dfrac{P(AA)}{P(E)}\\
P(Aa|E)=\dfrac{P(Aa)}{P(E)}$$

As probabilidades do lado direito, $P(AA)$, $P(Aa)$, $P(E)$, pertencem ao espaço de resultado original $\{AA, Aa, aa\}$, enquanto $P(AA|E)$ e $P(Aa|E)$ referem-se ao espaço reduzido espaço de resultado $\{AA, Aa\}$

## Exemplo 13.5.2: Dado de seis faces

As probabilidades condicionais ocorrem também em uma situação um pouco mais geral. Considere um dado carregado com seis faces numeradas 1, 2, ..., 6. O espaço de resultados é {l, 2, 3, 4, 5, 6}, mas as probabilidades dos seis resultados não são iguais a 1/6. Nós os denotamos por $p_1, p_2, \ldots, p_6$ respectivamente. Apresentamos os dois eventos compostos a seguir:

$$A = \{2, 3, 4, 5\}, \;B = \{4, 5, 6\}$$

Evento A significa: A face superior mostra 2 ou 3 ou 4 ou 5. 

Evento B significa: A face superior mostra 4 ou 5 ou 6. 

Os dois eventos são parcialmente "sobrepostos", ou seja, a interseção é $A \cap B= \{4, 5\}$. As probabilidades dos eventos são:

$$\begin{align}
P(A) &= p_2 + p_3 + p_4 + p_5\\
P(B) &= p_4 + p_5 + p_6\\
P(A \cap B) &= p_4 + p_5
\end{align}$$

Agora pensamos em um jogador que está interessado em uma pontuação alta, digamos no evento B. No entanto, quando o dado é lançado, ele recebe apenas um conhecimento parcial: ele é informado de que o evento A ocorreu. Sua pergunta é: Qual é a probabilidade do evento B dado que A ocorreu? O problema é encontrar $P(B|A)$.

Como A ocorreu, as probabilidades ajustadas ou condicionais de
os resultados especiais 1 e 6 são zero. Portanto, para obter as probabilidades dos resultados 2, 3, 4, 5, temos que dividir $p_2, p_3, p_4, p_5$ pela soma $p_2 + p_3 + p_4 + p_5 = P(A)$. 

As probabilidades individuais são: $p_2/P(A)$, $p_3/P(A)$, $p_4/P(A)$ e$p_5/P(A)$.

Sua soma é $(p_2 + p_3 + p_4 + p_5)/P(A)=1$.

O jogador está interessado apenas no evento $\{4, 5\} =A \cap B$. 

Portanto:

$$P(B|A) = \dfrac{p_4+p_5}{P(A)}=\dfrac{P(A\cap B)}{P(A)}$$.

O resultado é aplicável em uma grande variedade de problemas: Dado um espaço de resultados, sejam A e B quaisquer dois de seus eventos. Então a probabilidade do evento B, dado que o evento A ocorreu, é a probabilidade de ocorrência simultânea de A e B, dividida pela probabilidade de A (desde que o denominador não desapareça).

$$P(A|B)=\dfrac{P(A\cap B)}{P(A)}$$

É essencial não confundir $P(B|A)$ com $P(A \cap B)$. A probabilidade $P(A \cap B)$ refere-se ao espaço de resultado original $\Omega$, enquanto $P(B|A)$ é uma probabilidade definida no espaço de resultado reduzido A. Descrevemos essa relação por um diagrama de Venn (Fig. 13.4a).

```{r echo=FALSE, out.width="80%", fig.cap="Figura 13.4a. A probabilidade condicional P(B|A) está relacionada ao novo espaço de resultado A, enquanto P(A&cap;B) pertence ao espaço de resultado original &Omega;."}
knitr::include_graphics("./image/Fig13.4a.png")
```

Assuma que $B_1, B_2, B_3, B_4$ formam uma partição de D como mostrado na Fig. 13.4b e que A é qualquer evento de $\Omega$ com $P(A) \ne 0$. Podemos calcular as probabilidades condicionais $P(B_i|A)$, $i = 1, 2, 3, 4$. Como todas são probabilidades no espaço de resultado reduzido A, e como $B_1, B_2, B_3, B_4$ são exaustivas, as quatro probabilidades condicionais devem somar 1. De fato,

```{r echo=FALSE, out.width="80%", fig.cap="Figura 13.4b. Diagrama de Venn ilustrando as interseções de um conjunto A (evento A) com quatro conjuntos não sobrepostos B<sub>1</sub>, B<sub>2</sub>, B<sub>3</sub>, B<sub>4</sub> (eventos mutuamente excludentes)."}
knitr::include_graphics("./image/Fig13.4b.png")
```

$$\begin{align}
P(B_1|A)+P(B_2|A)+P(B_3|A)+P(B_4|A)&=\dfrac{P(A\cap B_1)}{P(A)}\\
&+\dfrac{P(A\cap B_2)}{P(A)}\\
&+\dfrac{P(A\cap B_3)}{P(A)}\\
&+\dfrac{P(A\cap B_4)}{P(A)}\\
&=\dfrac{P(A\cap \Omega)}{P(A)}\\
&=\dfrac{P(A)}{P(A)}\\
P(B_1|A)+P(B_2|A)+P(B_3|A)+P(B_4|A)&=1
\end{align}$$

## Exemplo 13.5.3: Probabilidade de morte

Considere a probabilidade de morte em nossa sociedade. A Tabela 13.1 contém algumas das informações pertinentes. O espaço do resultado consiste nos eventos mutuamente excludentes e exaustivos "morte na primeira década" (entre o nascimento e o décimo aniversário), "morte na segunda década" etc. O último evento é "morte após o 80º aniversário".

As probabilidades são valores estimados. Aqui não estamos preocupados com o problema de estimar as probabilidades de dados observados. Em vez disso, tomamos as probabilidades como garantidas.

Qual é a probabilidade de uma pessoa que agora tem 20 anos morrer antes de completar 30 anos? 

Para responder à pergunta, não podemos simplesmente retirar da tabela a taxa de mortalidade da terceira década (1.21%). Em vez disso, temos que encontrar uma probabilidade condicional. Sabemos que a pessoa já sobreviveu às duas primeiras décadas.

Portanto, temos que ajustar as probabilidades de nosso espaço de resultados.

```{r echo=FALSE, out.width="50%", fig.cap="Tabela 13.1. Probabilidade de morte em diferentes idades para os Estados Unidos."}
knitr::include_graphics("./image/Table13.1.png")
```

O espaço de resultado reduzido consiste no evento "morte após a segunda década", que podemos interpretar como evento A.

Temos:

$$P(A) = 1.21 + 1.84+ \cdots + 33.58 = 96.12$$

Se B denota o evento "morte antes da quarta década", então B|A é o evento de nossa questão. Então:

$$P(B|A)=\dfrac{P(A\cap B)}{P(A)}=\dfrac{1.21}{96.12}=0.0126$$

ou 1.26%.

## Exemplo 13.5.4: Daltonismo

É sabido que o daltonismo é hereditário.

Devido ao fato de que o gene responsável está ligado ao sexo, o daltonismo ocorre com mais frequência em homens do que em mulheres.

Em uma grande população humana, a incidência de daltonismo vermelho-verde foi contada. As frequências relativas estão listadas na tabela a seguir.

```{r echo=FALSE, out.width="80%", fig.cap="Tabela 13.2. A incidência de daltonismo vermelho-verde em uma população humana."}
knitr::include_graphics("./image/Table13.2.png")
```

Desconsiderando problemas estatísticos, assumimos que as frequências relativas na Tabela 13.2 são tão precisas que podem ser usadas como probabilidades.

O espaço de resultado consiste em quatro eventos simples {homem daltônico,
homem normal, mulher daltônica, mulher normal}. 

Podemos obter esses eventos por multiplicação cruzada.

Sejam M e F os eventos "Male" e "Female", respectivamente, e sejam C e N os eventos "daltônico" e "normal", respectivamente. Então formamos o conjunto produto {C,N} x {M,F} = {CM, CF, NM, NF}.

Este é o nosso espaço de resultados. As probabilidades correspondentes satisfazem os três axiomas. Na verdade, são números entre 0% e 100% e somam 100%.

Podemos também considerar eventos compostos. O evento "daltônico" é um evento composto: C = {CM, CF}.

Sua probabilidade é de 4.88%. Da mesma forma, o evento "masculino" independentemente de qualquer outra característica, M = {CM, NM}, é um evento composto. Sua probabilidade é de 52.71%.

Agora podemos perguntar: Qual é a taxa de incidência de daltonismo para a subpopulação de homens? 

Esta questão leva a probabilidades condicionais. A fórmula de probabilidade condicional afirma que 

$$P(C|M) = \dfrac{P(C\cap M)}{P(M)} = \dfrac{4.23}{52.71} = 0.0803$$

ou 8.03%.

Assim, em homens, a prevalência de daltonismo é 8.03%.

Da mesma forma, obtemos para a prevalência da subpopulação de mulheres

$$P(C|F)= \dfrac{P(C\cap F)}{P(F)} = \dfrac{0.65}{47.29} = 0.0137$$

ou 1.37%.

# Regra da multiplicação

Suponha que A e B são dois eventos que pertencem ao mesmo. espaço de resultado. Então:

$$P(A\cap B) = P(A) P(B|A)$$

Em palavras: A probabilidade da ocorrência simultânea de dois eventos A e B é o produto da probabilidade do evento A e a probabilidade condicional do evento B dado A.

Por simetria, A e B podem ser trocados. Portanto, também temos 

$$P(A\cap B) = P(B) P(A|B)$$

## Exemplo: Regra de Bayes

[Bayes' theorem: Wikipedia](https://en.wikipedia.org/wiki/Bayes%27_theorem){target="_blank"}

$$\begin{align}
P(A\cap B) &= P(A) P(B|A)= P(B) P(A|B)\\
P(A|B)&=P(B|A)\dfrac{P(A)}{P(B)}
\end{align}$$

A regra de Bayes é um conceito fundamental na teoria das probabilidades e é amplamente utilizada em muitas áreas, incluindo estatística, aprendizado de máquina e ciência de dados. Vou fornecer um exemplo básico de como aplicar a regra de Bayes em R usando um problema clássico conhecido como "teste de diagnóstico".

Suponha que você esteja realizando um teste médico para determinar se um paciente tem uma certa doença. Você sabe que a doença afeta 1% da população e que o teste médico tem uma taxa de precisão de 95% para identificar corretamente os casos positivos e 90% para identificar corretamente os casos negativos.

Aqui está um exemplo de código em R que usa a regra de Bayes para calcular a probabilidade de um paciente realmente ter a doença, dado um resultado positivo no teste:

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r}
# Probabilidade prévia: P(D) - Probabilidade de ter a doença
prior_probability <- 0.01

# Probabilidade de um resultado positivo dado que a pessoa tem a doença: P(Pos|D)
sensitivity <- 0.95

# Probabilidade de um resultado positivo dado que a pessoa não tem a doença: P(Pos|~D)
false_positive_rate <- 1 - 0.90

# Probabilidade de um resultado positivo: P(Pos)
# Calculado usando o teorema da probabilidade total
probability_positive <- (sensitivity * prior_probability) + (false_positive_rate * (1 - prior_probability))

# Aplicando a regra de Bayes para calcular a probabilidade posterior: P(D|Pos)
posterior_probability <- (sensitivity * prior_probability) / probability_positive

# Imprimir a probabilidade posterior
print(posterior_probability)
```

Neste exemplo, a probabilidade prévia (prior_probability) é definida como 0.01, representando a taxa de prevalência da doença na população. A sensibilidade (sensitivity) é definida como 0.95, representando a probabilidade de um resultado positivo dado que a pessoa realmente tem a doença. O false_positive_rate é definido como 1 - 0.90, representando a probabilidade de um resultado positivo dado que a pessoa não tem a doença.

O cálculo da probabilidade positiva (probability_positive) usa o teorema da probabilidade total para combinar as probabilidades de um resultado positivo dado que a pessoa tem a doença e dado que a pessoa não tem a doença.

Finalmente, a regra de Bayes é aplicada para calcular a probabilidade posterior (posterior_probability), que é a probabilidade de que a pessoa realmente tenha a doença, dado um resultado positivo no teste.

## Exemplo 13.6.1: 70º aniversário

Dado que um homem comemora seu 70º aniversário, qual é a probabilidade de ele atingir a idade de 72 anos? 

Em uma tabela de vida encontramos as seguintes probabilidades condicionais de sobrevivência para os homens:

| idade $x$ | $p_x$ |
|---------:|-------------:|
|       70 |     0.9492 |
| 71 | 0.9444 |
| 72 | 0.9391 |

Aqui p<sub>70</sub> é a probabilidade de um homem de 70 anos viver até seu 71º aniversário, p<sub>71</sub> é a probabilidade de um homem de 71 anos viver até seu 72º aniversário etc. Aplicando a fórmula concluímos que a probabilidade de atingir o 71º, bem como o 72º aniversário é

$$P(71\cap 72) = P(71) P(72|71) = 0.9492 \times 0.9444 = 0.8964$$

ou 89.64%. 

$\Diamond$

Um caso especial da fórmula é o mais importante. Pode acontecer que a ocorrência de um evento A não influencie o resultado de um segundo evento B. Isso significaria que

$$P(B|A)=P(B)$$

Sempre que $P(B|A)=P(B)$ for válido, dizemos que o evento B é independente do evento A.

A relação é simétrica, isto é, se B é independente de A, então A é independente de B. Eles são independentes um do outro. Se esta fórmula não for válida, diz-se que o evento B depende do evento A.

## Exemplo 13.6.2: Sexo e daltonismo 

Voltemos ao daltonismo tratado no Exemplo 13.5.4. Obtivemos $P(C|M) = 8.03\%$, $P(C) = 4.88\%$.

```{r echo=FALSE, out.width="80%", fig.cap="Tabela 13.2. A incidência de daltonismo vermelho-verde em uma população humana."}
knitr::include_graphics("./image/Table13.2.png")
```

Portanto, $P(C|M) \ne P(C)$, ou seja, o daltonismo depende do sexo.

## Exemplo 13.6.3: Daltonismo e surdez

Para ilustrar a independência, imaginamos um cientista que deseja saber se existe alguma dependência entre o daltonismo e a surdez em humanos do sexo masculino. Suponha que ele tenha as seguintes probabilidades:

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("./image/deaf.png")
```

Seja C o evento composto 'color-blind' e D o evento composto 'deaf'. Com $C$ e com $\bar{D}$ denotamos os eventos complementares 'not color-blind' e 'not deaf', respectivamente. Então, obtemos as seguintes probabilidades condicionais:

$$P(D|C)=\dfrac{0.0004}{0.0800}=0.0050\\
P(\bar{D}|C)=\dfrac{0.0796}{0.0800}=0.9950$$

Da última linha da tabela, lemos

$$P(D) = 0.0050\\
P(\bar{D}) = 0.9950$$

Portanto:

$$P(D|C)=P(D)\\
P(\bar{D}|C)=P(\bar{D})$$

ou seja, a surdez independe do daltonismo em homem.

$\Diamond$

Em geral, sempre que $P(B|A)=P(B)$ for válido, segue-se de $P(A\cap B) = P(A) P(B|A)$ que

$$P(A\cap B) = P(A) P(B)$$

e inversamente. Esta é a regra de multiplicação para eventos independentes (fatoração da probabilidade conjunta).

A independência é muitas vezes definida em termos da regra de multiplicação:

_Definição_. Dois eventos A e B do mesmo espaço de resultados são considerados independentes se a regra de multiplicação $P(A\cap B) = P(A) P(B)$ for válida.

Na Seção 13.4 introduzimos a regra da adição (Axioma 3). Existe alguma semelhança formal entre as regras de adição e multiplicação. No entanto, eles devem ser cuidadosamente distinguidos uns dos outros. Se

$$P(A \cup B) = P(A) +P(B)$$

é válido, os dois eventos A e B devem ser mutuamente excludentes, ou seja, $A\cap B=\emptyset$. Portanto, $P(A\cap B)=0$. 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`A union B`](https://www.wolframalpha.com/input?i=A+union+B){target="_blank"}

[`A intersect  B`](https://www.wolframalpha.com/input?i=A+intersect++B){target="_blank"}

[`probability A union B`](https://www.wolframalpha.com/input?i=probability+A+union+B){target="_blank"}

[`probability of the union of two independent events`](https://www.wolframalpha.com/input?i=probability+of+the+union+of+two+independent+events){target="_blank"}

O leitor deve distinguir cuidadosamente entre duas propriedades "independentes" e "mutuamente excludentes". Essas propriedades ocorrem em diferentes contextos.

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("./image/difference.png")
```

[Difference Between Mutually Exclusive and Independent Events](https://byjus.com/maths/difference-between-mutually-exclusive-and-independent-events/#:~:text=The%20difference%20between%20mutually%20exclusive,occurrence%20of%20the%20other%20event.){target="_blank"}

## Exemplo 13.6.4: Lançamento de uma moeda

Um experimento pode consistir no lançamento de uma moeda. Se H denota o evento "cara" e T o evento "coroa", o espaço de resultado para uma única tentativa é {H, T} e para duas tentativas do experimento $\{H, T\} \times \{H, T\} = \{HH , HT, TH, TT\}$.

Os pares HT e TH são ordenados, o que significa que distinguimos cuidadosamente entre os resultados da primeira e da segunda tentativa.

Agora introduzimos o evento composto "cara na primeira tentativa" e o denotamos por H<sub>1</sub>. 

Como a moeda é considerada honesta, obtemos 

$$P(H_1)=\dfrac{1}{2}$$

Apenas para maior clareza, notamos que $H_1 = \{HH, HT\}$. 

Consideramos um segundo evento composto "coroa na segunda tentativa" e o denotamos por T<sub>2</sub>.

Realizamos as duas tentativas de forma que o resultado da segunda tentativa não seja afetado pela primeira tentativa. Portanto, assumimos que os dois eventos H<sub>1</sub> e T<sub>2</sub> são independentes e que

$$P(T_2)=\dfrac{1}{2}$$

Aplicando a regra da multiplicação obtemos 

$$P(H_1\cap T_2)=P(H_1)P(T_2)=\dfrac{1}{2}\dfrac{1}{2}=\dfrac{1}{4}$$

Agora o evento H<sub>1</sub> e T<sub>2</sub> é idêntico a HT. Portanto, 

$$P(HT)=\dfrac{1}{4}$$

um resultado que esperamos intuitivamente muito antes de nos acostumarmos com conceitos como evento composto, interseção e independência. 

Da mesma forma, prova-se que $P(HH) = P(TH) = P(TT) = 1/4$.

A experiência que acabamos de descrever poderia ser realizada de maneira ligeiramente diferente. Poderíamos lançar duas moedas distintas ao mesmo tempo e ler cara e coroa. O espaço resultante seria novamente {HH, HT, TH, TT}. Ao lançar as duas moedas, temos que ter certeza de que elas não "grudam" ou não se atraem, digamos por forças magnéticas. Caso contrário, a independência não pode ser esperada.

A regra da multiplicação leva aos mesmos resultados de antes, quando jogamos a mesma moeda duas vezes.

## Exemplo 13.6.5: Dois pássaros canoros

Suponha que dois pássaros canoros (não necessariamente da mesma espécie) estejam sentados na mesma árvore. Durante uma hora inteira, sua música foi gravada e a duração das músicas e intervalos medidos.

O resultado é apresentado na tabela a seguir:

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("./image/birds.png")
```

Quando interessados em uma possível interação dos dois pássaros, podemos perguntar: 

Quais são as probabilidades de que, em um determinado instante de tempo selecionado aleatoriamente,

a. o pássaro B<sub>1</sub> está cantando,
a. o pássaro B<sub>2</sub> está cantando,
a. os pássaros B<sub>1</sub> e B<sub>2</sub> estão cantando simultaneamente?

Da tabela segue que P(B<sub>1</sub> cantando) = 715/3600 = 0.199, P(B<sub>2</sub> cantando) = 609/3600 = 0.169. 

A questão c só pode ser respondida sob alguma suposição. Suponhamos provisoriamente que os pássaros cantam independentemente uns dos outros. 

Então

$$\begin{align}
P(B_1 \;\text{e}\; B_2 \;\text{cantando}) &= P(B_1 \;\text{cantando})P(B_2 \;\text{cantando}) \\
&= 0.199 \times 0.169 = 0.0336
\end{align}$$

Em 3.36% das vezes os pássaros estariam cantando simultaneamente.

A experiência mostra, no entanto, que os cantos se sobrepõem apenas esporadicamente.

Concluímos que há dependência entre os dois eventos "canto B<sub>1</sub>" e "canto B<sub>2</sub>", i.e., há coordenação: quando um pássaro canta, o outro não canta. As aves procuram evitar interferências acústicas.

$\Diamond$

A definição de independência pode ser generalizada para mais de dois eventos. Por exemplo, três eventos A, B, C pertencentes ao mesmo espaço de resultados são ditos independentes se

$$P(A\cap B\cap C) = P(A)P(B)P(C)$$

Para uma aplicação imediata, assistimos a um jogador. Suponha que ele esteja jogando uma moeda repetidas vezes e que tenha observado apenas um resultado cara em dez tentativas consecutivas. Ele está muito confuso, pois tal evento tem probabilidade de aproximadamente 0.1%.

O décimo primeiro resultado será cara ou coroa? 

Qual resultado tem maiores chances? 

Surpreendentemente, há muitas pessoas que acreditam que, após uma longa série de caras, o resultado oposto deve ter uma chance melhor. Eles esperam algum tipo de "justiça" nos jogos de azar. É provável que apostem uma grande soma e arrisquem seu dinheiro. 

Essas pessoas estão certas? 

A experiência não suporta tal crença. Se a moeda for normal, isto é, nem moeda de duas caras e nem desonesta, e se nenhum mecanismo secreto interferir durante o movimento, as chances de cara e coroa permanecem 1/2. Esta fórmula está então de acordo com a nossa experiência e a independência está estabelecida.

# Contagem

Muitos problemas de probabilidade requerem alguns métodos de contagem.

## Arranjo com repetição

[Arranjo: Wikipedia](https://pt.wikipedia.org/wiki/Combinat%C3%B3ria#Arranjos){target="_blank"}

[Permutation: Wikipedia](https://en.wikipedia.org/wiki/Permutation){target="_blank"}

### Exemplo 13.7.1: Palavras de cinco letras

Quantas palavras de cinco letras, com e sem sentido, podem ser escritas com os 26 caracteres do alfabeto?

Para a primeira letra da palavra, podemos escolher entre 26 caracteres diferentes. Para a segunda letra, temos a mesma escolha.

Combinando as escolhas para as duas primeiras letras, obtemos 26 x 26 = 26<sup>2</sup> possibilidades.

Para a terceira letra, temos novamente uma escolha entre 26 caracteres.

Portanto, o número total de palavras de três letras é 262 x 26 = 26<sup>3</sup>.

O mesmo argumento finalmente leva a 26<sup>5</sup> = 11.881.376 palavras diferentes de cinco letras.

Em geral, suponha que temos $n$ tipos de objetos e que um número ilimitado de objetos de cada tipo está disponível. 

De quantas maneiras diferentes podemos preencher $k$ espaços distintos, cada espaço com um objeto?

Esse problema pode ser resolvido da mesma maneira que o exemplo anterior.

O resultado é

$$\underbrace{n\times n \times \cdots \times n}_{k} = n^k$$

### Exemplo 13.7.2: Alfabeto Morse

Cada letra do alfabeto Morse consiste em dois tipos de símbolo: ponto e traço. 

Quantas letras diferentes podem ser compostas por quatro desses símbolos? 

Neste exemplo, temos quatro espaços para preencher e dois tipos de símbolo. 

Podemos compor 2<sup>4</sup> = 16 letras.

### Exemplo 13.7.3: Anemia hereditária

Existem diferentes tipos de anemia hereditária, como esferocitose, talassemia, anemia falciforme, ovalocitose e síndrome de Fanconi. 

Acredita-se que alelos anormais em cinco _loci_ gênicos diferentes sejam os responsáveis.

Denote os alelos normais nesses cinco _loci_ por A, B, C, D, E e os alelos anormais correspondentes por a, b, c, d, e. 

Em cada _locus_ podemos distinguir entre três genótipos (no primeiro _locus_ AA, Aa, aa, no segundo _locus_ BB, Bb, bb etc.). 

O número total de todos os arranjos genéticos é, portanto, 3<sup>5</sup> = 243. 

Para relacionar este exemplo com a teoria geral, observamos que os _loci_ são os espaços e as três diferentes combinações de genes "ambos normais", "um normal-um anormal", "ambos anormais" são os objetos.

$\Diamond$

Nos problemas que acabamos de discutir, cada objeto está disponível em número ilimitado. Falamos de repetição ilimitada.

Chamamos agora nossa atenção para problemas de contagem de arranjos sem repetição.

## Arranjo sem repetição: Permutação

[Arranjo: Wikipedia](https://pt.wikipedia.org/wiki/Combinat%C3%B3ria#Arranjos){target="_blank"}

[Permutation: Wikipedia](https://en.wikipedia.org/wiki/Permutation){target="_blank"}

### Exemplo 13.7.4: Cinco carros e três vagas

Considere cinco carros que estão competindo por apenas três vagas de estacionamento. 

De quantas maneiras os carros podem ser estacionados?

Vamos numerar os três espaços por 1, 2, 3 em uma ordem arbitrária. 

A primeira vaga pode ser ocupada por qualquer um dos cinco carros. Para a segunda vaga temos apenas quatro possibilidades, pois um carro já está estacionado na primeira vaga.

Temos que combinar as 5 possibilidades de ocupação do primeiro espaço com as 4 possibilidades de ocupação do segundo espaço. Assim obtemos 5 x 4 possibilidades para os dois primeiros espaços juntos. Para estacionar um carro na terceira vaga, temos apenas uma escolha entre os três carros restantes. Combinando as três possibilidades com as possibilidades 5 x 4 anteriores, obtemos 5x4x3 = 60 formas diferentes de estacionar. Dois carros permanecem sem espaço para estacionamento.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`permutation 5 3`](https://www.wolframalpha.com/input?i=permutation+5+3){target="_blank"}

[`permutation {1,2,3,4,5} 3`](https://www.wolframalpha.com/input?i=permutation+%7B1%2C2%2C3%2C4%2C5%7D+3){target="_blank"}

$\Diamond$

Em geral, suponha que recebemos $k$ espaços distintos e $n$ objetos diferentes sem repetição. Para que cada espaço possa ser preenchido com exatamente um objeto, temos que assumir que $n \ge k$.

Agora numeramos os $k$ espaços de maneira arbitrária. O primeiro espaço pode ser preenchido de $n$ maneiras diferentes. Para o segundo espaço, apenas $n - 1$ objetos estão disponíveis. Portanto, os dois primeiros espaços podem ser preenchidos de $n(n - 1)$ maneiras. Para o terceiro espaço, temos uma escolha entre os $n - 2$ objetos restantes. Para cada espaço consecutivo, o número de objetos disponíveis diminui em 1. Para o $k$-ésimo espaço, a escolha é entre os objetos. O número total de possibilidades para preencher os $k$ espaços com $n$ objetos é, portanto, $n(n -1)(n - 2)\cdots(n - k+ 1)$. 

Chamamos cada arranjo de uma permutação de $n$ objetos tomados $k$ de cada vez. Uma notação conveniente para o número total de permutações é $P_{n,k}$.

Nosso resultado é:

$$P_{n,k}=n(n -1)(n - 2)\cdots(n - k+ 1)=\dfrac{n!}{(n-k)!}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`factorial n`](https://www.wolframalpha.com/input?i=factorial+n){target="_blank"}

### Exemplo 13.7.5: Mapa com quatro cores

Imaginamos um mapa geográfico contendo quatro países. Cada país deve ser pintado com uma cor diferente. Há sete cores disponíveis. Então a pintura pode ser feita em $P_{7,4} = 7 \times 6 \times 5 \times 4 = 840$ maneiras diferentes.

$\Diamond$

Se o número $n$ de objetos é igual ao número de espaços, i.e., $n=k$, obtemos um caso especial importante. Cada arranjo é simplesmente chamado de permutação dos $n$ objetos. 

$$P_{n,n}= n(n-1) (n - 2)\cdots3\;2\;1= n!$$

permutações de $n$ objetos. O símbolo $n!$ é leia "n fatorial". 

Observe que 1! = 1, 2! = 2, 3! = 6, 4! = 24, 5! = 120 etc.

### Exemplo 13.7.6: Quatro letras

Consideramos as quatro letras e, n, o, t. De quantas maneiras podemos organizá-los em uma palavra? 

A resposta é 4! = 4 x 3 x 2 x 1 = 24.

Listamos todas as 24 permutações:

```{r echo=FALSE, out.width="60%"}
knitr::include_graphics("./image/enot.png")
```

Observe que muito poucas dessas permutações formam uma palavra da nossa língua.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`permutation 4 4`](https://www.wolframalpha.com/input?i=permutation+4+4){target="_blank"}

[`permutation {e,n,o,t} 4`](https://www.wolframalpha.com/input?i=permutation+%7Be%2Cn%2Co%2Ct%7D+4){target="_blank"}

$\Diamond$

Portanto, o número de permutações de $n$ objetos tomados $k$ de cada vez é:

$$P_{n,k}=\dfrac{n!}{(n-k)!}$$

Esta fórmula compreende o caso especial onde $k = n$ se definirmos
$(n-n)!$ ou $0!$ para ser $0!=1$.

## Combinação

[Combination: Wikipedia](https://en.wikipedia.org/wiki/Combination){target="_blank"}

Um problema um tanto diferente de contagem lida com seleções.

Para um experimento, três animais são selecionados de cinco. De quantas maneiras isso pode ser feito? Podemos pensar em uma analogia: quando três vagas de estacionamento estão disponíveis para cinco carros, encontramos 5 x 4 x 3 = 60 arranjos de estacionamento. No entanto, a analogia não é completa. No problema de estacionamento, as três vagas são distintas, enquanto em nosso problema de seleção a ordem em que os três animais estão dispostos é irrelevante. Represente os três carros estacionados por a, b, c. Em seguida, distinguimos cuidadosamente entre os seis arranjos abc, acb, bac, bca, cab, cba.

Qualquer que seja a seleção dos três carros, contamos cada seleção seis vezes. Inversamente, se não estivermos interessados na ordem dos carros (ou animais), temos que dividir 60 por 6. Assim, encontramos 10 seleções de três carros (ou animais) retiradas de cinco.

O mesmo argumento pode ser usado para obter um resultado geral: de quantas maneiras podemos selecionar $k$ dentre $n$ objetos diferentes?

Se a ordem em que os $k$ objetos aparecem for relevante, o resultado é expresso na fórmula $P_{n,k}=n!/(n-k)!$. Se não, temos que eliminar o $k!$ permutações dos $k$ objetos selecionados. Chamamos cada seleção independente da ordem de combinação e denotamos o número total de combinações por $C_{n,k} = {n \choose k}$. Na fórmula $P_{n,k}=n!/(n-k)!$, cada combinação é contada $k!$ vezes.

Portanto:

$$C_{n,k}k!=P_{n,k}$$

Isto implica:

$$C_{n,k}=\dfrac{P_{n,k}}{k!}=\dfrac{n!}{(n-k)!k!}$$

Não há uma maneira padrão de ler este símbolo. Uma forma adequada seria "n suconjuntos de tamanho k". O símbolo deve ser cuidadosamente diferenciado de "n sobre k", que significa a fração n/k.

### Exemplo 13.7.7: Três de cinco animais

De quantas maneiras três de cinco animais podem ser selecionados? 

Se denotarmos os cinco animais arbitrariamente por a, b, e, d, e, as dez combinações são abc, abd, abe, aed, ace, ade, bcd, bee, bde, ede.

Observe que dee ou ede não ocorrem nesta lista, pois são apenas permutações de ede.

$$C_{5,5}=\dfrac{5!}{(5-3)!3!}=\dfrac{5!}{2!3!}=10$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`combination 5 3`](https://www.wolframalpha.com/input?i=combination+5+3){target="_blank"}

[`combination {a,b,c,d,e} 3` ](https://www.wolframalpha.com/input?i=combination+%7Ba%2Cb%2Cc%2Cd%2Ce%7D+3+){target="_blank"}

$\Diamond$

### Triângulo de Pascal

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics("./image/Pascal.png")
```

```{r echo=FALSE, out.width="90%", fig.cap="Fig. 13.5. A fonte romana. Em uma bacia, a água corre à taxa 1 (o que significa uma unidade de peso por unidade de tempo). Em dois lados da bacia, a água transborda simetricamente a uma taxa de 1/2 cada e corre para duas bacias semelhantes abaixo. A água dessas duas bacias também transborda e corre para três bacias dispostas simetricamente abaixo. A bacia central recebe água a uma taxa de 1/4 + 1/4 = 2/4 = 1/2, enquanto as bacias externas recebem água a uma taxa de apenas 1/4 cada. Os numeradores são 1, 2, 1, ou seja, a segunda linha do triângulo de Pascal. O processo é então repetido uma e outra vez. Em cada linha, a água flui a taxas proporcionais à linha correspondente no triângulo de Pascal."}
knitr::include_graphics("./image/Roman.png")
```

Note que, em cada linha do Triângulo de Pascal:

$$\sum_{i=0}^{n}{C_{n,i}}=2^n$$

### Exemplo 13.7.8: Seis alelos

Acrescentamos um exemplo da genética. Suponha que existam seis alelos diferentes possíveis no mesmo locus gênico. Denotamos por $A_1, A_2, \ldots, A_6$.

Quantas combinações de genes ou genótipos são possíveis?

Primeiro contamos todas as combinações de dois alelos diferentes selecionados dentre os seis alelos, como $A_1A_2$, $A_1A_3$ etc. Existem $C_{6,2} = 15$ deles.

Em segundo lugar, também contamos as combinações com repetição, ou seja, os seis pares $A_1A_1$, $A_2A_2$ etc. O total de todas as combinações é 15 + 6 = 21.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`combination {a,b,c,d,e,f}` 2](https://www.wolframalpha.com/input?i=combination+%7Ba%2Cb%2Cc%2Cd%2Ce%2Cf%7D+2){target="_blank"}

### Exemplo 13.7.9: Camundongos

Em uma gaiola há 20 camundongos rotulados 1, 2, ..., 20.

Cinco camundongos são selecionados aleatoriamente.

Qual é a probabilidade

a. de uma seleção particular,
b. de uma seleção de camundongos com rótulos menores ou iguais a 10?

Para responder a essas perguntas, determinamos o número de combinações de 5 em 20 ratos. 

Este número é $C_{20,5}= 15504$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`combination 20 5`](https://www.wolframalpha.com/input?i=combination+20+5){target="_blank"}

Como todas as combinações são igualmente prováveis, cada combinação particular tem probabilidade 1/15504. Isso responde à primeira pergunta. 

Para a segunda questão, notamos que $C_{10,5}=252$ combinações de cinco ratos têm rótulos menores ou iguais a 10.

Assim, a probabilidade de tal amostra é 252/15504 = 0.0163, ou seja, surpreendentemente pequeno (menos de 2%).

$\Diamond$

### Coeficientes binomiais

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("./image/coefbinomial.png")
```

```{r echo=FALSE, out.width="70%"}
knitr::include_graphics("./image/coefbinomial2.png")
```

$$(a+b)^n=\sum_{i=0}^{n}{C_{n,i}}a^{n-i}b^i$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`coefficients (p-2)^5`](https://www.wolframalpha.com/input?i=coefficients+%28p-2%29%5E5){target="_blank"}

[`coefficients (a+b)^10`](https://www.wolframalpha.com/input?i=coefficients+%28a%2Bb%29%5E10){target="_blank"}

# Distribuição binomial 

[binomial distribution: Wikipedia](https://en.wikipedia.org/wiki/Binomial_distribution){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution`](https://www.wolframalpha.com/input?i=binomial+distribution){target="_blank"}

Nesta seção, desenvolveremos ainda mais a teoria da probabilidade usando métodos de contagem.

## Exemplo 13.8.1: Recombinação de genes

Como exemplo introdutório estudamos a recombinação de genes. Suponha que uma população de peixes reúna suas células reprodutivas. Considere um _locus_ especial com alelos A e a.

Cada célula reprodutiva (espermatozóide ou óvulo) contém exatamente um dos dois alelos, A ou a. Seja $p$ a probabilidade de que um espermatozóide contenha A e $q= 1- p$ seja a probabilidade de a. Suponha ainda que os óvulos tenham a mesma distribuição, isto é,

$$\begin{align}
P(A) &= p\\
P(a) &= q\\
p + q &= 1
\end{align}$$

O espaço amostral é {A, a}.

Quando os espermatozóides fertilizarem os óvulos, temos que considerar o novo espaço de resultado {AA, Aa, aa}. Suponha que as células reprodutivas se encontrem aleatoriamente, ou seja, que o processo seja independente do conteúdo genético de cada célula. A independência é formulada em termos matemáticos pela regra da multiplicação. Daí, obtemos:

$$P(AA) = P(A) P(A) = p^2\\
P(aa) = P(a) P(a) = q^2$$

O genótipo heterozigoto Aa é formado de duas maneiras. O alelo A é do esperma ou do óvulo. Se a ordem contasse, teríamos $P(Aa) = pq$ e $P(aA) = qp$. No entanto, os dois casos não podem ser distinguidos biologicamente. Assim, desconsiderando a ordem temos que somar as probabilidades: $pq + qp = 2pq$. Resumindo, a recombinação de genes leva a:

$$\begin{align}
P(AA) &= p^2\\
P(aa) &= q^2\\
P(Aa)&= 2pq
\end{align}$$

Como {AA, Aa, aa} é nosso espaço de resultados, as três probabilidades devem somar 1. De fato,

$$p^2 + 2pq + q^2 = (p +q)^2 = 1$$

## Exemplo 13.8.2: Meninos e meninas

Consideramos o número de meninos e meninas em uma família. Denotamos um nascimento masculino por M e um nascimento feminino por F. As probabilidades para a ocorrência de M e F não são exatamente 1/2. A proporção entre os sexos varia ligeiramente de país para país. Assumimos que $P(M) = p = 0.52$, $P(F) = q = 0.48$.

Omitimos a possibilidade de nascimentos gêmeos ou múltiplos. Então a experiência mostra que o resultado de cada nascimento é independente do resultado de nascimentos anteriores na mesma família. Portanto, aplicamos a regra da multiplicação. Para duas crianças com o espaço de resultados {MM, MF, FM, FF} obtemos:

$$\begin{align}
P(MM) &= p^2 = 0.52^2 = 0.2704\\
P(MF) &= pq = 0.52 \times 0.48 = 0.2496\\
P(FM) &= qp = 0.48 \times 0.52 = 0.2496\\
P(FF) &= q^2 = 0.48^2 = 0.2304
\end{align}$$

A probabilidade total é 1. Se desconsiderarmos a ordem de nascimento, obtemos

$$\begin{align}
P(MM)&=p^2= 0.2704\\
P(MF)&=2pq=0.4992\\
P(FF)&=q^2= 0.2304
\end{align}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution n=2 p=0.52`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D2+p%3D0.52){target="_blank"}

O exemplo pode ser facilmente estendido a famílias com três filhos.

Desde que observemos a ordem de nascimento, o espaço de resultados é {MMM, MMF, MFM, FMM, MFF, FMF, FFM, FFF}.

A regra de multiplicação produz as probabilidades correspondentes:

$$\begin{align}
P(MMM)&=ppp= p^3\\
P(MFF)&=pqq= pq^2\\
P(MMF)&=ppq=p^2q\\
P(FMF)&=qpq= pq^2\\
P(MFM)&=pqp=p^2q\\
P(FFM)&=qqp= pq^2\\
P(FMM)&=qpp=p^2q\\
P(FFF)&=qqq=q^3
\end{align}$$

No entanto, desconsiderando a ordem de nascimento, o espaço de resultados é reduzido para {MMM, MMF, MFF, FFF}, e as probabilidades correspondentes passam a ser

$$\begin{align}
P(MMM)&=p^3=0.52^3=0.140608\\
P(MMF)&=3p^2q=3\times 0.52^2 \times 0.48=0.389376\\
P(MFF)&=3pq^2=3\times 0.52 \times 0.48^2=0.359424\\
P(FFF)&=q^3=0.48^3=0.110592
\end{align}$$

As quatro probabilidades são os termos da expansão de $(p + q)^3$.

Os coeficientes são os coeficientes binomiais $C_{3,0},C_{3,1},C_{3,2},C_{3,3}$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`coefficients (p+q)^3`](https://www.wolframalpha.com/input?i=coefficients+%28p%2Bq%29%5E3){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution`](https://www.wolframalpha.com/input?i=binomial+distribution){target="_blank"}

[`binomial distribution n=3 p=0.52`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D3+p%3D0.52){target="_blank"}

[`binomial distribution n=4 p=0.52`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D4+p%3D0.52){target="_blank"}

A última observação nos dá uma dica para uma generalização adicional: considere famílias com $n$ filhos. Então perguntamos: 

Qual é a probabilidade de selecionar ao acaso uma família com $k$ meninos e $n - k$ meninas? 

Se respeitarmos a ordem de nascimento, a probabilidade é $p^kq^{n-k}$, pois o fator $p = P(M)$ deve aparecer $k$ vezes e o fator $q = P(F)$ exatamente $n - k$ vezes. Porém, ao desconsiderar a ordem, temos o caso de selecionar "$k$ meninos de $n$ filhos", e isso pode ser feito de tantas maneiras quantas pudermos selecionar $k$ objetos de $n$ objetos distintos. Como a ordem não conta, estamos lidando com o número de combinações de n objetos, $k$ por vez. O número é $C_{n,k}$. Portanto, temos que tomar o termo $p^kq^{n-k}$ quantas vezes for indicado por $C_{n,k}$.

O resultado é:

$$P(k \;\text{meninos}, n-k \;\text{meninas}) = C_{n,k}p^kq^{n-k}$$

Essa probabilidade é um termo na expansão de $(p+q)^n$.

$\Diamond$

Em geral, seja $E$ um evento de um espaço de resultados e $\bar{E}$ o evento complementar. Assim podemos escrever

$$\begin{align}
P(E) &= p\\
P(\bar{E}) &= q\\ 
p + q &= 1
\end{align}$$

Realizamos o experimento $n$ vezes de forma que cada resultado consecutivo seja independente de todos os resultados anteriores.

Então a probabilidade de que $E$ ocorra exatamente $k$ vezes é

$$P(k \;\text{vezes}) = C_{n,k}p^kq^{n-k}\\
0\le k \le n$$

O conjunto de probabilidades para todo $k = 0, 1,2, \ldots, n$ é chamado de distribuição binomial. 

Esta distribuição foi investigada por Jacob Bernoulli (1654-1705, _Ars Conjectandi_, 1713). Experimentos que resultam em uma alternativa simples como "sim-não", "sucesso-falha", "reação positiva-negativa" são frequentemente chamados de ensaios de Bernoulli.

Antes de estudar outras aplicações, investigamos o caso especial $p=1/2$ e $q=1/2$.

Como modelo de experimento, podemos escolher o lançamento de uma moeda com os resultados cara (H) e coroa (T). A fórmula reduz a

$$P(k \;\text{vezes}) = \dfrac{C_{n,k}}{2^n}$$

Para $n$ dado, $P(k \;\text{vezes})$ para $k = 0,1,2, \ldots, n$ é proporcional a $C_{n,k}$, isto é, a uma linha no triângulo de Pascal. 

O gráfico de barras da Fig. 13.6 ilustra essas distribuições especiais.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.6. Gráficos de barras de distribuições binomiais para p = 1/2, q = 1/2 e n= 2, 3, 4."}
knitr::include_graphics("./image/Fig13.6.png")
```

Um mecanismo de chance especialmente adequado para demonstrar as distribuições binomiais particulares com $p=1/2$ e $q=1/2$ é o chamado binômio. Os detalhes são explicados na Fig. 13.7.

```{r echo=FALSE, out.width="40%", fig.cap="Fig. 13.7. Um experimento modelo para demonstrar distribuições binomiais com p=1/2, q=1/2. As bolas estão rolando por uma ladeira. No caminho, eles encontram obstáculos dispostos conforme mostrado pelos polígonos pretos. A cada obstáculo uma bola é lançada para a esquerda ou para a direita com probabilidade 1/2. Em nosso desenho, as probabilidades de atingir os compartimentos abaixo estão nesta ordem: 1/64, 6/64, 15/64, 20/64, 15/64, 6/64, 1/64. Assim, os numeradores formam a sexta linha do triângulo de Pascal. Esse experimento modelo foi inventado por Galton (cf. Fig. 13.20). Relaciona-se com a fonte romana representada na Fig. 13.5."}
knitr::include_graphics("./image/Fig13.7.png")
```

## Exemplo 13.8.3: Risco de efeito fatal

Suponha que a probabilidade de uma pessoa morrer dentro de um mês após uma certa operação de câncer seja de 18%.

Quais são as probabilidades de que em três dessas operações uma, duas ou todas as três pessoas sobrevivam?

A sobrevivência significa, neste contexto, o oposto da morte dentro de um mês após a operação. O espaço resultante é {D, S}, sendo que D significa morte e S para sobrevivência. As probabilidades são $P(D) = 0.18$, $P(S) = 0.82$.

Se tivermos boas razões para supor que o resultado de uma operação é independente do resultado das outras duas operações, podemos aplicar a fórmula $P(k \;\text{vezes}) = C_{n,k}p^kq^{n-k}$. Seja $k$ o número de pacientes sobreviventes. Então

$$\begin{align}
P(k = 0) &= 0.006\\
P(k = 1) &= 0.080\\
P(k = 2) &= 0.363\\
P(k = 3) &= 0.551
\end{align}$$

Assim, a probabilidade de apenas uma pessoa sobreviver é de 8%, de duas sobreviverem 36% e de todas as três sobreviverem 55%, aproximadamente.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution n=3 p=0.82`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D3+p%3D0.82){target="_blank"}

## Exemplo 13.8.4: Bioensaio

Quando um animal recebe um tratamento, podemos perguntar se a reação é positiva ou não, ou seja, se um determinado resultado pode ser observado ou não. Tal resultado é qualitativo e não quantitativo. Quando $n$ animais iguais recebem o mesmo tratamento, podemos perguntar quantos animais reagem positivamente. Cada desempenho único do experimento é chamado de tentativa de Bernoulli.

Seja $E$ o evento "reação positiva" e $\bar{E}$ o evento complementar. Seja $P(E) = p$, $P(\bar{E}) = q$ , $p+ q= 1$.

Sob a suposição de que as $n$ tentativas são independentes, a probabilidade de exatamente $k$ animais reagirem positivamente é $P(k \;\text{reações+}) = C_{n,k}p^kq^{n-k}$.

Por exemplo, considere cinco camundongos da mesma ninhada, todos sofrendo de deficiência de vitamina A. Eles são alimentados com uma certa dose de cenoura.

A reação positiva significa aqui a recuperação da doença.

Suponha que a probabilidade de recuperação seja $p = 0.73$. Então perguntamos:

Qual é a probabilidade de que exatamente três dos cinco camundongos se recuperem?

A resposta é $C_{5,3}0.73^30.27^2 = 0.284$ ou 28.4%.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution n=5 p=0.73`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D5+p%3D0.73){target="_blank"}

## Exemplo 13.8.5: Mutação

A probabilidade de uma mutação por gene e por R (roentgen, unidade de intensidade de radiação) em camundongos é de aproximadamente 2.5 x 10<sup>-7</sup>. 

Qual é a probabilidade de que em dez mil genes ocorra pelo menos uma mutação?

Seja $k$ o número de mutações. A frase "pelo menos um" significa k = 1 ou 2 ou 3 etc. Seria extremamente trabalhoso calcular todas as dez mil probabilidades pela fórmula e somá-las. É muito mais simples proceder da seguinte forma: O evento "pelo menos um" é o evento complementar de "nenhuma mutação". Assim, calculamos primeiro $P(k=0)=(1-2.5 10^{-7})^{10000}=0.99999975^{10000}$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution n=10000 p=2.5 10^(-7)`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D10000+p%3D2.5+10%5E%28-7%29){target="_blank"}

Portanto, $P(k>0)=1- P(k=0)= 1-0.997503=0.002497\approx0.25\%$.

A chance de observar pelo menos uma mutação em dez mil genes que sofreram radiação de dose 1R é de apenas um quarto por cento.

## Exemplo 13.8.6: Efeitos colaterais nocivos

Suponha que uma droga cause um efeito colateral grave em uma proporção de três pacientes em cem. Um laboratório farmacológico quer testar a droga. Qual é a probabilidade de que o efeito colateral ocorra em uma amostra aleatória de dez pacientes tomando o medicamento?

Antes que uma resposta possa ser dada, a questão deve ser formulada com mais precisão. Seja $k$ o número de pessoas que podem sofrer com o efeito colateral. Então, a ocorrência do efeito colateral significa que _k_ = 1 ou 2 ou 3 etc. Como no aplicativo anterior, é mais fácil tratar primeiro o caso $k = 0$. A probabilidade de um paciente não sofrer do efeito colateral é de $1 - 0.03 = 0.97$. Portanto:

$$\begin{align}
P(k = 0) &= 0.97^{10}=0.7374241\\
P(k> 0) &= 1 - P(k = 0) = 0.2625759
\end{align}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution n=10 p=0.97`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D10+p%3D0.97){target="_blank"}

Portanto, em uma amostra de dez pacientes selecionados aleatoriamente, o efeito colateral ocorre apenas com probabilidade de 26.3%. 

## Exemplo 13.8.7 Um contra-exemplo

A distribuição binomial nem sempre é aplicável quando o mesmo experimento é repetido. A razão é que o resultado de uma tentativa pode depender dos resultados das tentativas anteriores. Considere, por exemplo, o clima.

Para simplificar, vamos distinguir entre dias chuvosos e dias secos.

Suponha que em um determinado local a taxa de dias chuvosos seja 1:12. Se estivermos interessados em sequências de dias chuvosos, podemos perguntar: 

Qual é a probabilidade de chover nos dias 1, 2 e 3 de julho?

Somos tentados a aplicar a regra da multiplicação. Isso levaria a $P(3 \;\text{dias chuvosos}) = (1/12)^3 \approx 0.0006$, ou seja, um número muito pequeno. No entanto, o resultado está errado. O clima de um dia depende fortemente do clima dos dias anteriores. O dia seguinte a um dia chuvoso provavelmente será outro dia chuvoso.

De acordo com nossa experiência, a probabilidade de três dias chuvosos consecutivos é consideravelmente maior do que o resultado dado pela regra da multiplicação.

## Distribuição binomial em R

* Distribuição de probabilidade de variável discreta finita $X$
* $n=1,2,3,...$: número de realizações independentes do experimento
* Cada repetição do experimento produz um de dois resultados possíveis, denominados genericamente de sucesso (S) e fracasso (F)
* A probabilidade de sucesso, $P(S) = p$, é constante em cada repetição. 
* A probabilidade de fracasso, $P(F) = 1 – p$, é constante em cada repetição. 
* $X \sim binomial(n, p)$ : variável aleatória $X$ com distribuição binomial representando o número de sucessos em $n$ repetições do experimento; $X=0,1,\ldots,n$
* Média de $X$: $n p$
* Desvio-padrão de $X$: $\sqrt{n p (1 – p)}$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
k <- 0:10
plot(k,dbinom(x=k,size=10,prob=.45),
     type='h',
     main='Distribuição binomial(n=10, p=0.45)',
     ylab='Probabilidade',
     xlab ='Número de sucessos',
     lwd=3)
```

### Fator Rh 

"Além da classificação comum de sangue nos grupos A, B, AB e O, é importante a subdivisão de acordo com uma substância chamada fator Rhesus (Rh), que pode ser positivo (Rh+) ou negativo (Rh-). Podem ocorrer reações de incompatibilidade em transfusões de sangue. Por exemplo, um indivíduo Rh- só deve receber transfusão de sangue Rh-. Caso receba sangue Rh+, haverá sua sensibilização e a formação de anticorpos anti-Rh. Além disso, pode acontecer incompatibilidade Rh entre o sangue materno e o fetal. Aproximadamente 85% da população são Rh+, os outros 15% são Rh-." 

> Siqueira & Tibúrcio, 2011, p. 198-9

* $n = 3$ é o número de pacientes que realizarão um transplante.
* $X$ é o número de pacientes com Rh- que realizarão um transplante (sucesso), variando de 0 e 3.
* $P(Rh^-) = 0.15$
* $X \sim binomial(3, 0.15)$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
n <- 3
p <- 0.15
cat("X ~ binomial (",n,", ",p,")\n",sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X =",m,"\n")
cat("Desvio-padrao de X =",dp,"\n\n")
px0 <- dbinom(x=0,size=n,prob=p)
px1 <- dbinom(x=1,size=n,prob=p)
px2 <- dbinom(x=2,size=n,prob=p)
px3 <- dbinom(x=3,size=n,prob=p)
cat("P(X = 0) =",px0,"\n")
cat("P(X = 1) =",px1,"\n")
cat("P(X = 2) =",px2,"\n")
cat("P(X = 3) =",px3,"\n")
cat("\nP(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) =", 
    px0+px1+px2+px3, "\n")
# lower.tail: logical; if TRUE (default), probabilities are P[X <= x], 
#                      otherwise, P[X > x]
px01 <- pbinom(q=1,size=n,prob=p,lower.tail=TRUE)
cat("P(X = 0) + P(X = 1) =", px01, "\n")
px23 <- pbinom(q=1,size=n,prob=p,lower.tail=FALSE)
cat("P(X = 2) + P(X = 3) =", px23, "\n")
X <- 0:n
probs <- dbinom(x=X,size=n,prob=p)
plot(X,
     probs,
     type="h",
     xlim=c(-1,n+1),
     ylim=c(0,1.1*max(probs)), 
     main=paste0("Distribuicao binomial (",n,", ",p,")"),
     lwd=1,
     col="black",
     ylab="Probabilidade")
points(X,probs,pch=16,cex=1,col="black")
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
n <- 3
p <- 0.15
cat("X ~ binomial (",n,", ",p,")\n",sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X =",m,"\n")
cat("Desvio-padrao de X =",dp,"\n\n")
px0 <- dbinom(x=0,size=n,prob=p)
px1 <- dbinom(x=1,size=n,prob=p)
px2 <- dbinom(x=2,size=n,prob=p)
px3 <- dbinom(x=3,size=n,prob=p)
cat("P(X = 0) =",px0,"\n")
cat("P(X = 1) =",px1,"\n")
cat("P(X = 2) =",px2,"\n")
cat("P(X = 3) =",px3,"\n")
cat("\nP(X = 0) + P(X = 1) + P(X = 2) + P(X = 3) =", 
    px0+px1+px2+px3, "\n")
# lower.tail: logical; if TRUE (default), probabilities are P[X <= x], 
#                      otherwise, P[X > x]
px01 <- pbinom(q=1,size=n,prob=p,lower.tail=TRUE)
cat("P(X = 0) + P(X = 1) =", px01, "\n")
px23 <- pbinom(q=1,size=n,prob=p,lower.tail=FALSE)
cat("P(X = 2) + P(X = 3) =", px23, "\n")
X <- 0:n
probs <- dbinom(x=X,size=n,prob=p)
plot(X,
     probs,
     type="h",
     xlim=c(-1,n+1),
     ylim=c(0,1.1*max(probs)), 
     main=paste0("Distribuicao binomial (",n,", ",p,")"),
     lwd=1,
     col="black",
     ylab="Probabilidade")
points(X,probs,pch=16,cex=1,col="black")
```

## Questão 1

Qual é a probabilidade que nenhum paciente que realizará transplante seja Rh-?

R.: $P(X=0) = 0.61$ 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
dbinom(x=0, size=3, prob=0.15)
```

## Questão 2

Qual é a probabilidade que pelo menos um paciente que realizará transplante seja Rh-?

R.: $1 – P(X=0) = 1 – 0.61 = 0.39$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
1 - dbinom(x=0, size=3, prob=0.15)
```

## Questão 3

Qual é a probabilidade que todos os pacientes que realizarão transplante sejam Rh-?

R.: $P(X=3) = 0.003$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
dbinom(x=3, size=3, prob=0.15) 
```

# Variável aleatória

[Random variable: Wikipedia](https://en.wikipedia.org/wiki/Random_variable){target="_blank"}

[Algebra of random variables: Wikipedia](hhttps://en.wikipedia.org/wiki/Algebra_of_random_variables){target="_blank"}

Os eventos em um experimento aleatório às vezes são de natureza qualitativa.

Por exemplo, em experimentos genéticos com ervilhas, as pétalas podem ser brancas, vermelhas ou rosa. Na ciência, no entanto, a quantificação de propriedades costuma ser vantajosa. Nesta seção, estudaremos vários exemplos de quantificação bem-sucedida. Também introduziremos conceitos como variável aleatória, distribuição de probabilidade, média e desvio-padrão de variáveis aleatórias.

## Exemplo 13.9.1: Cor da pétala de ervilha

Consideramos o espaço resultante {branco, rosa, vermelho} para as pétalas das ervilhas experimentais. De acordo com a genética, essas cores são devidas a dois alelos, digamos W e R, em um determinado _locus_ gênico.

O genótipo WW produz flores brancas, WR flores rosas e RR flores vermelhas.

Assim, podemos mapear o espaço de resultados em {WW, WR, RR}.

Agora podemos quantificar os três resultados de forma simples e natural:

Contamos o número de alelos R em cada resultado, mapeando assim o espaço de resultados em {0, 1, 2}.

O número 0 está associado a Ww, o número 1 a WR e o número dois a RR. Chamamos essa associação de função. Uma notação frequentemente usada é $X$. Em nosso exemplo $X(WW)=0$, $X(WR)=1$, $X(RR)=2$.

O domínio desta função é o conjunto {WW, WR, RR}, e a imagem é o conjunto {0, 1, 2}.

## Exemplo 13.9.2: Dois eventos simples

Consideramos um espaço de resultados que consiste em apenas dois eventos simples, digamos {sucesso, falha}. 

À primeira vista, parece não haver uma forma natural de quantificação.

Podemos mapear o espaço de resultados em {+1, -1} ou em {1, 0}. Mas por que não escolher {5, 1/4}? Obviamente, temos que encontrar um critério para a seleção adequada do mapeamento. Como tantas vezes na ciência, a melhor escolha é aquela que serve melhor. Mostraremos que o mapeamento {1,0} é uma boa escolha na quantificação. Aqui a função $X$ assume 1 ou 0. O número 1 significa sucesso, o número 0, falha. Vamos realizar um experimento com o espaço de resultados {1,0} várias vezes.

Com $X_1,X_2,\ldots,X_n$ denotamos os resultados da primeira, segunda, ..., enésima tentativas, respectivamente. Se, por exemplo, a sequência for sucesso-falha-falha-sucesso-sucesso-falha, obtemos $X_1 = 1, X_2 = 0, X_3 = 0, X_4 = 1, X_5 = 1, X_6 = 0$. Agora formamos a soma $S_n=\sum_{i=1}^{n}{X_i}$.

Cada termo é um zero ou um. Temos tantos 1 quantos são os sucessos.

Portanto, $S_n$ é igual ao número de sucessos em $n$ tentativas (em nosso exemplo $S_6 = 3$).

Esta propriedade simples é usada para justificar o mapeamento de {sucesso, falha} em {1,0}. A quantidade $S_n$ pode assumir qualquer um dos $n+ 1$ valores 0, 1, 2, ... , n. Se estivermos interessados apenas no número de sucessos e não na ordem em que o evento "sucesso" ocorre, o espaço de resultados para $n$ tentativas é {0, 1, ... , n}. 

Agora introduzimos probabilidades para o espaço de resultados {sucesso, falha}.

Em vez de escrever $P(\text{sucesso}) = p$, $P(\text{falha}) = q$, é mais conveniente usar a notação $P(X=1)=p$, $P(X=0)=q$. 

Ao repetir o experimento, podemos perguntar pela probabilidade de $k$ sucessos em $n$ tentativas, ou resumidamente, pela probabilidade do evento $S_n = k$.

Sob a hipótese de ensaios independentes, podemos reescrever o resultado na forma $P(S_n = k) = C_{n,k}p^kq^{n-k}$.

## Esperança

[Expected value: Wikipedia](https://en.wikipedia.org/wiki/Expected_value){target="_blank"}

Outro exemplo em que o espaço de resultados pode ser facilmente quantificado é a tabela de vida da Seção 13.5 (Tabela 13.1). O espaço de desfecho consiste em "morte na primeira década", "morte na segunda década" etc. Podemos associar o número $k$ ao evento "morte na k-ésima década". Assim, mapeamos o espaço resultante em {1, 2, 3, ...}. Em vez da frase "a morte ocorreu na k-ésima década", poderíamos simplesmente escrever $X = k$.

Quando estamos lidando com estatura, massa corporal total, pulsação, pressão arterial etc., os eventos simples do espaço de resultado são medições e, portanto, já são números reais. Um mapeamento para outros números geralmente não é necessário. É comum denotar medidas e contagens variáveis por letras maiúsculas como X, Y, Z, U, V etc.

Em geral, quando um resultado é mapeado em um conjunto de números, introduzimos uma quantidade variável, digamos $X$, que é uma função dos resultados. Tal quantidade é chamada de variável aleatória.

Na Seção 1.2 estudamos os diferentes níveis de quantificação. É precisamente a transição do nível nominal para o nível ordinal, intervalar ou de razão que nos permite introduzir uma variável aleatória. Assim, uma variável aleatória pode assumir as diferentes pontuações atribuídas em um nível ordinal. Por exemplo, $X = 0$ pode significar falha, $X = 1$ sucesso de uma tentativa.

Mais frequentemente, no entanto, variáveis aleatórias são usadas para quantidades definidas em um nível de intervalo, como temperaturas em graus Celsius, tempo, altitude, potencial elétrico ou para quantidades definidas em uma escala de razão, como comprimento, área, volume, massa.

Assumimos agora que uma variável aleatória $X$ só pode assumir um número finito de valores, digamos $x_1, x_2, \dots, x_m$. Assim, o espaço resultante é $\{x_1, x_2, \dots, x_m\}$.

Denotamos a probabilidade associada a $x_i$ por $p_i$, $i = 1,2, \ldots, m$.

Isso significa que $P(X=x_i)=p_i$.

A probabilidade total deve ser 1, isto é, $\sum{p_i}=1$. Chamamos um conjunto de probabilidades associado a um espaço de resultado de distribuição de probabilidade.

Por muitas razões, é desejável definir uma média ou média adequada da variável aleatória $X$. Não podemos simplesmente tomar a média aritmética $\sum{x_i}/m$, pois negligenciaríamos a proporção na qual cada $x_i$ ocorre no experimento aleatório. Para ter uma ideia adequada, voltemos ao conceito de frequência da probabilidade. Cada $p_i$ é uma idealização de uma frequência relativa.

Suponha que o evento $X = x_1$ ocorreu $n_1$ vezes, o evento $X = x_2$ ocorreu $n_2$ vezes etc. em um total de $n$ execuções do experimento.

Então obtemos: 

$$\begin{align}
\bar{x}&=\sum_{i=1}^{m}{\dfrac{n_i}{n}x_i}\\
n&=\sum_{i=1}^{m}{n_i}
\end{align}$$

Aqui $\bar{x}$ é a média aritmética ponderada de $x_1, x_2, \dots, x_m$. Os "pesos" são $n_1, n_2, \dots, n_m$. Substituindo as frequências relativas $n_i/n$ por probabilidades $p_i$ obtemos a seguinte definição do valor médio de $X$:

$$\text{média de}\; X=\sum_{i=1}^{m}{p_ix_i}$$

Se tivéssemos que adivinhar ou estimar o resultado de uma única execução do experimento, a média de $X$ seria uma boa escolha. 

Nosso palpite pode ser muito alto ou muito baixo, mas não muito longe do resultado real. Por esta razão, também chamamos a média de valor esperado ou esperança matemática de $X$. A notação é $\mathbb{E}(X)$. Portanto:

$$\mathbb{E}(X)=\sum_{i=1}^{m}{p_ix_i}$$

Na teoria da probabilidade, as palavras "média" e "esperança" são usadas no mesmo sentido.

A fórmula (13.9.12) tem uma contrapartida interessante na mecânica:

Considere cada $x_i$ como a abscissa de um ponto em uma linha reta e interprete $p_i$ como uma massa ou peso concentrado no ponto $x_i$ (ver Fig. 13.8).

Então o ponto com abcissa $\mathbb{E}(X)$ é idêntico ao centro de gravidade.

```{r echo=FALSE, out.width="80%", fig.cap="Fig. 13.8. A expectativa de X interpretada como um centro de gravidade."}
knitr::include_graphics("./image/Fig13.8.png")
```

### Exemplo 13.9.4: Esperança masculina

Para simplificar, assuma que a razão sexual é 1:1.

Seja a variável aleatória $X$ o número de meninos em famílias com quatro filhos. O espaço resultante é {0, 1, 2, 3, 4}. 

Agora pedimos a média ou o número esperado de meninos nessas famílias.

Por simetria, é fácil adivinhar que essa média é 2. 

Verificamos o resultado:

$$\mathbb{E}(X)=\dfrac{1}{16}0+\dfrac{4}{16}1+\dfrac{6}{16}2+\dfrac{4}{16}3+\dfrac{1}{16}4=2$$

Da mesma forma, o número médio ou esperado de meninos em famílias com cinco filhos é 2.5.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution n=4 p=0.5`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D4+p%3D0.5){target="_blank"}

[`binomial distribution n=5 p=0.5`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D5+p%3D0.5){target="_blank"}

### Exemplo 13.9.5: Esperança de alelo letal

Estudamos a ocorrência de um alelo letal a em uma população com genótipos AA, Aa, aa. Seja $X$ o número de a em cada genótipo, ou seja, $X = 0$ em AA, $X = 1$ em Aa, $X = 2$ em aa. 

Para as probabilidades introduzimos a seguinte notação $P(X=i)=p_{i+1}$, $i=0,1,2$.

O número esperado de a é:

$$\mathbb{E}(X) = p_10+p_21+p_32=p_2+2p_3$$

Se, por exemplo, $p_1 = 0.7$, $p_2 = 0.2$, $p_3 = 0.1$, obtemos $\mathbb{E}(X) = 0.4$, ou seja, na média dos três genótipos o gene letal ocorre 0.4 vezes.

## Variância

[Variance: Wikipedia](https://en.wikipedia.org/wiki/Variance){target="_blank"}

Com o mero conhecimento de $\mathbb{E}(X)$ não temos ideia de quanto os resultados $x_1, x_2, \dots, x_m$ diferem do valor esperado.

Algumas distribuições de probabilidade estão concentradas em torno de $\mathbb{E}(X)$, outras não.

Portanto, é desejável ter uma medida de dispersão. Como na Seção 1.9, consideramos o quadrado do desvio $(x_i-\mathbb{E}(X))^2$ já que não estamos interessados no sinal de $x_i-\mathbb{E}(X)$. Para obter um desvio médio, não podemos simplesmente tomar a média aritmética ordinária de todos os $m$ termos. Temos que observar a proporção na qual cada $x_i$ ocorre.

Pela mesma razão que na fórmula da esperança formamos a média aritmética ponderada com "pesos" $p_i$. O resultado é chamado de variância de $X$ e é definido por:

$$\mathbb{V}(X)=\sum_{i=1}^{m}{p_i(x_i-\mathbb{E}(X))^2}$$

Para a maioria das pessoas, a fórmula parece menos assustadora se introduzirmos as seguintes notações padrão:

$$\mathbb{E}(X) = \mu\\
\mathbb{V}(X) = \sigma^2$$

(letras gregas mi e sigma). Com esta notação, a fórmula da variância se transforma em

$$\sigma^2=\sum_{i=1}^{m}{p_i(x_i-\mu)^2}$$

Como é frequentemente o caso, os $x_i$ não são números puros, mas expressos em uma determinada unidade, como cm, kg, seg. Então a fórmula da esperança revela que $\mathbb{E}(X)$ é expresso na mesma unidade. No entanto, $\sigma^2$ é medido no quadrado dessa unidade. Por exemplo, se a média for 85 cm, a variância pode ser de 16 cm<sup>2</sup>. Essa propriedade da variância é desvantajosa.

Portanto, tiramos a raiz quadrada da variância e obtemos:

$$\sigma=\sqrt{\sum_{i=1}^{m}{p_i(x_i-\mu)^2}}$$

Essa medida de dispersão dos valores em torno da esperança é amplamente utilizada e é conhecida como desvio-padrão de $X$.

### Exemplo 13.9.6: Valor em cm

Consideramos uma variável aleatória $X$ com o espaço de resultado {83 cm, 84 cm, 85 cm, 86 cm, 87 cm} e com probabilidades 1/11, 2/11, 5/11, 2/11, 1/11, respectivamente. Com respeito a 85 cm esta distribuição particular é simétrica. Se interpretarmos as probabilidades como massas, o centro de gravidade cai no ponto com abcissa 85 cm. O leitor pode verificar que $\mu= \mathbb{E}(X)$ é de fato 85 cm. Omitindo temporariamente "cm" obtemos a variância

$$\begin{align}
\sigma^2&=\dfrac{1}{11}(83-85)^2+\dfrac{2}{11}(84-85)^2+\dfrac{5}{11}(85-85)^2+\dfrac{2}{11}(86-85)^2+\dfrac{1}{11}(87-85)^2\\
&=\dfrac{12}{11}\;\text{cm}^2 
\end{align}$$

O desvio-padrão é $\sigma=1.044 \;\text{cm}$.

## Esperança e variância de distribuição binomial

Agora aplicamos os conceitos de esperança, variância e desvio-padrão à distribuição binomial. 

Na fórmula da esperança, os $x_i$ são os resultados $0, 1, 2, \ldots, n$ e os $p_i$ são as probabilidades binomiais dadas por $C_{n;i}p^iq^{n-i}$, $p+q=1$.

Portanto:

$$\mu=\mathbb{E}(S_n)=\sum_{i=0}^{n}{C_{n;i}p^iq^{n-i}i}=np(p+q)^{n-1}=np$$

Um cálculo semelhante, mas mais longo, que vamos pular aqui, leva à fórmula da variância:

$$\sigma^2=\mathbb{V}(S_n)=np(1-p)$$

Portanto, o desvio-padrão da distribuição binomial é $\sigma=\sqrt{np(1-p)}$.

## Exemplo 13.9.7 

Um dado honesto de seis faces é lançado 12 vezes. Se a face superior for um seis, consideramos um sucesso e denotamos esse evento por $E$.

Qualquer outro evento é uma falha e denotado por $E$. 

$$\begin{align}
p&= P(E) =\dfrac{1}{6} \\
q&=P(\bar{E})=\dfrac{5}{6}
\end{align}$$

Por $S_{12}$ denotamos o número de sucessos em $n= 12$ tentativas. 

Essa variável aleatória varia de 0 a 12. 

A esperança é $\mu = \mathbb{E}(S_{12}) = 12 \dfrac{1}{6}=2$.

```{r echo=FALSE, out.width="80%", fig.cap="Fig. 13.9. Distribuição binomial para p = 1/6, n = 12. Esperança e desvio-padrão da variável aleatória S<sub>12</sub> são mostrados."}
knitr::include_graphics("./image/Fig13.9.png")
```

O resultado corresponde à nossa intuição que sugere que em 12 tentativas devemos esperar 2 faces superiores "seis".

A variância é:

$$\sigma^2=\mathbb{V}(S_{12})=12\dfrac{1}{6}\dfrac{5}{6}=\dfrac{5}{3}$$

Disso obtemos o desvio-padrão 

$$\sigma=\sqrt{\mathbb{V}(S_{12})}=\sqrt{\dfrac{5}{3}}\approx1.29$$

A distribuição binomial deste exemplo é mostrada na Fig. 13.9.

# Distribuição de Poisson

[Poisson distribution: Wikipedia](https://en.wikipedia.org/wiki/Poisson_distribution){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`Poisson distribution`](https://www.wolframalpha.com/input?i=Poisson+distribution){target="_blank"}

Em ecologia, o padrão de distribuição de plantas ou animais da mesma espécie em uma região (campo, floresta) foi estudado muitas vezes.

Para tanto, a região é subdividida em um grande número de chamados quadrats (quadrados ou retângulos de área igual). Fig. 13.10 mostra um exemplo. Em cada quadrado único, o número de indivíduos é contado.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.10. Distribuição de plantas individuais de uma espécie em uma região."}
knitr::include_graphics("./image/Fig13.10.png")
```

Entre as 20 quadrats mostradas, existem algumas vazias e outras que contêm 1, 2, 3 ou mais indivíduos. Um levantamento completo da distribuição é dado abaixo:

| Número de indivíduos por quadrats | Número de quadrats | Total de indivíduos por tipo de quadrat |
|:-----------------------------------|:---------------------|:--------------------------------------------|
| 0                                 | 3                   | 0                                          |
| 1                                 | 6                   | 6                                          |
| 2                                 | 5                   | 10                                         |
| 3                                 | 4                   | 12                                         |
| 4                                 | 1                   | 4                                          |
| 5                                 | 0                   | 0                                          |
| 6                                 | 1                   | 6                                          |
| Total                        | 20                  | 38                                         |

Em algumas partes da região observamos uma agregação de plantas.

Outras partes mostram algum tipo de vazio. No entanto, como um todo, sentimos que os indivíduos estão dispersos aleatoriamente.

O padrão de dispersão aleatória pode ser descrito matematicamente?

Isso é realmente possível. 

Para simplificar, substituímos as plantas ou animais por bolas. 

Agora deixamos cair uma bola sobre a região de tal forma que cada quadrat tenha a mesma probabilidade $p$ de ser atingido pela bola. 

Na Fig. 13.10, $p=1/20$. Em seguida, repetimos o mesmo experimento $n$ vezes e assumimos que cada tentativa é independente de todas as tentativas anteriores. 

Seja $X$ o número de bolas que atingem um determinado quadrat. A variável aleatória $X$ pode assumir qualquer um dos inteiros $0, 1, 2, ... , n$. 

Temos que encontrar a probabilidade de que $X$ assuma um valor específico $k$. 

Escrevendo $X$ em vez de $S_n$, obtemos:

$$P(X=k)=C_{n,k}p^k q^{n-k}$$

sendo que $q = 1-p$. 

Na Fig. 13.10, temos $p = 1/20$ e $n = 38$ (número total de indivíduos).

Segue uma breve tabela de valores arredondados para três casas decimais:

| \(k\) | \(P(X = k)\) |
|:-------|:--------------|
| 0     | 0.142        |
| 1     | 0.285        |
| 2     | 0.277        |
| 3     | 0.175        |
| 4     | 0.081        |
| 5     | 0.029        |
| 6     | 0.008        |
| 7     | 0.002        |
| $\vdots$  | $\vdots$         |
| 20     | 0.000 |


```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`binomial distribution n=38 p=1/20`](https://www.wolframalpha.com/input?i=binomial+distribution+n%3D38+p%3D1%2F20){target="_blank"}

Portanto, esperamos:

* $20 \times 0.142 = 2.84$ (3) quadrats vazios,
* $20 \times 0.285 = 5.70$ (5) quadrats com um ponto,
* $20 \times 0.277 = 5.54$ (5) quadrats com dois pontos,
* $20 \times 0.175 = 3.50$ (4) quadrats com três pontos,
* $20 \times 0.081 = 1.62$ (1) quadrats com quatro pontos etc.

O resultado teórico é bastante compatível com as contagens da Fig. 13.10.

Apesar do bom acordo entre teoria e experiência, esta fórmula da distribuição binomial não é a resposta final para nosso problema. 

O número total $n$ de pontos geralmente é muito grande e seu valor exato é irrelevante. 

Por outro lado, a probabilidade $p$ de que um determinado quadrat dentre um grande número de quadrats seja atingido por um ponto é muito pequena.

Novamente, é irrelevante saber o valor exato de $p$. Finalmente, o cálculo dos termos binomiais é bastante trabalhoso.

Por todas essas razões, é apropriado perguntar pela distribuição limite que obtemos quando $n$ tende ao infinito e $p$ a zero. 

Para iniciar o processo de limite, consideramos o valor médio ou a experança de $X$. 

Por conveniência, denotamos essa média aqui por $\lambda$ em vez de $\mathbb{E}(X)$ ou $\mu$. Assim, $\lambda=np$. 

Em nosso exemplo $n = 38$ e $p = 1/20$, portanto $\lambda = 1.9$. Na maioria das aplicações, $\lambda$ varia de 0 a cerca de 10. 

Enquanto $n$ tende ao infinito e $p$ a zero, mantemos a média $\lambda$ constante. 

Reorganizando a fórmula da binomial e substituindo $p$ por $\lambda/n$ obtemos:

$$P(X=k)=C_{n,k}p^k q^{n-k}=\dfrac{\lambda^k}{k!}\dfrac{n(n-1)\cdots (n-k+1)}{(n-\lambda)^k}\left(1-\dfrac{\lambda}{n}\right)^n$$

$$\begin{align}
\lim_{n\to \infty}{P(X=k|n)}&=\lim_{n\to \infty}{C_{n,k}p^k q^{n-k}}\\
&=\lim_{n\to \infty}{\dfrac{\lambda^k}{k!}\dfrac{n(n-1)\cdots (n-k+1)}{(n-\lambda)^k}\left(1-\dfrac{\lambda}{n}\right)^n}\\
&=\dfrac{\lambda^k}{k!}\lim_{n\to \infty}{\dfrac{n(n-1)\cdots (n-k+1)}{(n-\lambda)^k}}\lim_{n\to \infty}{\left(1-\dfrac{\lambda}{n}\right)^n}\\
&=\dfrac{\lambda^k}{k!} 1 e^{-\lambda}\\
\lim_{n\to \infty}{P(X=k|n)}&=\dfrac{\lambda^k e^{-\lambda}}{k!} 
\end{align}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[Limit[(Factorial[n]/Factorial[n - k])/((n^k) (1 - lambda/n)^k),   n -> Infinity]](https://www.wolframalpha.com/input?i=Limit%5B%28Factorial%5Bn%5D%2FFactorial%5Bn+-+k%5D%29%2F%28%28n%5Ek%29+%281+-+lambda%2Fn%29%5Ek%29%2C+++n+-%3E+Infinity%5D&assumption=%22LimitHead%22+-%3E+%7B%22Discrete%22%7D){target="_blank"}

[Limit[(1- lambda/n)^n,   n -> Infinity]](https://www.wolframalpha.com/input?i=Limit%5B%281-+lambda%2Fn%29%5En%2C+++n+-%3E+Infinity%5D&assumption=%22LimitHead%22+-%3E+%7B%22Discrete%22%7D){target="_blank"}

Com essas probabilidades definimos uma nova variável aleatória $X$:

$$P(X=k)=\dfrac{\lambda^k e^{-\lambda}}{k!} $$

Aqui o espaço resultante consiste em todos os inteiros $k = 0, 1,2,3, \ldots$. O espaço amostral é, portanto, infinito.

A fórmula estabelece a famosa distribuição de Poisson.

Dizer que os objetos são aleatoriamente dispersos ou distribuídos aleatoriamente em uma região é dizer que eles seguem uma distribuição de Poisson.

Não é trabalhoso calcular os termos na fórmula, já que $k$ raramente excede 10 ou 20. 

Em nosso exemplo numérico, temos $\lambda=1.9$. Por este valor o
probabilidades são:

| \(k\) | \(P(X = k)\) |
|:-------|:--------------|
| 0     | 0.150        |
| 1     | 0.284        |
| 2     | 0.270        |
| 3     | 0.171        |
| 4     | 0.081        |
| 5     | 0.031        |
| 6     | 0.010        |
| 7     | 0.002        |
| \(\vdots\) | \(\vdots\) |

Essas probabilidades diferem pouco dos valores correspondentes da distribuição binomial. 

Uma apresentação gráfica de nossa distribuição de Poisson é dada na Fig. 13.11.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.11. Gráfico de barras da distribuição de Poisson com média 1.9."}
knitr::include_graphics("./image/Fig13.11.png")
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`Poisson distribution` ](https://www.wolframalpha.com/input?i=Poisson+distribution+){target="_blank"}

[`Poisson distribution mean = 1.9`](https://www.wolframalpha.com/input?i=Poisson+distribution+mean+%3D+1.9){target="_blank"}

Como já sabemos, a média da distribuição de Poisson é

$$\mathbb{E}(X)=\lambda$$

É fácil encontrar a variância. Na fórmula da variância da distribuição binomial, substituímos $p$ por $\lambda/n$ e então deixamos $n$ tender ao infinito. Então:

$$npq = n \dfrac{\lambda}{n}\left(1-\dfrac{\lambda}{n}\right)= \lambda \left(1-\dfrac{\lambda}{n}\right)\to\lambda$$

Então:

$$\sigma^2=\mathbb{V}(X)=\lambda$$

Observe que a média e a variância de uma distribuição de Poisson têm o mesmo valor numérico. O desvio-padrão é $\sigma=\sqrt{\lambda}$. Na Fig. 13.11, o desvio-padrão é $\sqrt{1.9}=1.38$.

[`Univariate Distribution Relationships` ](https://www.math.wm.edu/~leemis/chart/UDR/UDR.html){target="_blank"}

## Distribuição de Poisson em R

* Distribuição de probabilidade de variável discreta infinita $X$
* A distribuição de Poisson surge de um experimento de contagem de $X$ ocorrências de um evento de interesse por determinado período de tempo, área ou volume.
* $X \sim Poisson(\lambda)$: variável aleatória $X$ representando o número de ocorrências do evento com  distribuição de Poisson de taxa média de ocorrências $\lambda$
* $X=0,1,2,...$
* Média de $X$: $\lambda$
* Desvio-padrão de $X$: $\sqrt{\lambda}$
* Suposições
  * Modela evento raro;
  * As condições permanecem estáveis no decorrer do tempo ou espaço, i.e., a taxa média de ocorrências, $\lambda$, é constante;
  * Intervalos de tempo ou espaço disjuntos são independentes, i.e., a informação sobre o número de ocorrências num intervalo nada revela sobre o número de ocorrências em outro intervalo;
  * A probabilidade de ocorrência de um evento em um certo intervalo é a mesma para todos os demais intervalos de tempo;
  * A probabilidade de ocorrência dos eventos é proporcional ao tamanho do intervalo;
  * Em uma porção infinitesimal do intervalo, a probabilidade de mais de uma ocorrência do evento é desprezível.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
k <- 0:10
plot(k,dpois(x=k,lambda=2),
     type='h',
     main='Distribuicao de Poisson(lambda=2)',
     ylab='Probabilidade',
     xlab ='x',
     lwd=3)
```

### Exemplo: Número de atendimentos completos em pronto-atendimento 

O número de pacientes que têm atendimento completo num pronto-socorro de uma pequena cidade durante uma madrugada, $X$, tem distribuição de Poisson com taxa média $\lambda=3$.

Portanto, 

* $X \sim Poisson(3)$
* Média de $X$: $3$
* Desvio-padrão de $X$: $\sqrt{3} \approx 1.732$

Se forem consideradas duas madrugadas, a taxa média de atendimentos completos é $6 = 2 \times 3$. 

Num mês espera-se que a taxa média de atendimentos completos durante a madrugada seja $90 = 30 \times 3$.

> Siqueira & Tibúrcio, 2011, p. 201-2

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
lambda <- 3
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+5*dp)
cat("X ~ Poisson(lambda =", lambda,")\n")
cat("Media de X =",m,"\n")
cat("Desvio-padrao de X =",dp,"\n\n")
probs <- dpois(x=X,lambda=lambda)
probsacum <- ppois(q=X,lambda=lambda)
print(tbl <- round(data.frame(X,probs,probsacum),6))
cat("\nP(X=0) + P(X=1) + ... + P(X=10) =",sum(probs),"\n")
# lower.tail: logical; if TRUE (default), probabilities are P[X <= x], 
#                      otherwise, P[X > x]
px01 <- ppois(q=1,lambda=lambda,lower.tail=TRUE)
cat("P(X=0) + P(X=1) =", px01, "\n")
px23inf <- ppois(q=1,lambda=lambda,lower.tail=FALSE)
cat("P(X=2) + P(X=3) + ... =", px23inf, "\n")
plot(X,probs,type="h",
     xlim=c(-1,m+5*dp),ylim=c(0,1.1*max(probs)), 
     main=paste("Distribuicao de Poisson(",lambda,")"),
     lwd=1,col="black",ylab="Probabilidade")
points(X,probs,pch=16,cex=1,col="black")
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
lambda <- 3
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+5*dp)
cat("X ~ Poisson(lambda =", lambda,")\n")
cat("Media de X =",m,"\n")
cat("Desvio-padrao de X =",dp,"\n\n")
probs <- dpois(x=X,lambda=lambda)
probsacum <- ppois(q=X,lambda=lambda)
print(tbl <- round(data.frame(X,probs,probsacum),6))
cat("\nP(X=0) + P(X=1) + ... + P(X=10) =",sum(probs),"\n")
# lower.tail: logical; if TRUE (default), probabilities are P[X <= x], 
#                      otherwise, P[X > x]
px01 <- ppois(q=1,lambda=lambda,lower.tail=TRUE)
cat("P(X=0) + P(X=1) =", px01, "\n")
px23inf <- ppois(q=1,lambda=lambda,lower.tail=FALSE)
cat("P(X=2) + P(X=3) + ... =", px23inf, "\n")
plot(X,probs,type="h",
     xlim=c(-1,m+5*dp),ylim=c(0,1.1*max(probs)), 
     main=paste("Distribuicao de Poisson(",lambda,")"),
     lwd=1,col="black",ylab="Probabilidade")
points(X,probs,pch=16,cex=1,col="black")
```

#### Questão 1

Qual é a probabilidade que nenhum paciente tenha atendimento completo durante uma madrugada no pronto-atendimento dessa pequena cidade?

R.: $P(X=0) = 0.05$ 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
dpois(x=0, lambda=3)
```

#### Questão 2

Qual é a probabilidade que pelo menos um paciente tenha atendimento completo durante uma madrugada no pronto-atendimento dessa pequena cidade?

R.: $1 – P(X=0) = 1 – 0.05 = 0.95$ 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
1 - dpois(x=0, lambda=3)
```

#### Questão 3

Qual é a probabilidade que mais de dez pacientes tenham atendimento completo durante uma madrugada no pronto-atendimento dessa pequena cidade?

R.: $P(X>10) = 0.0003$ 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
ppois(q=10, lambda=3, lower.tail=FALSE)
# OU
1 - ppois(q=10, lambda=3)
```

### Exemplo: Número de consultas médicas anual de plano de saúde 

> Siqueira & Tibúrcio, 2011, p. 202

O número de consultas médicas anual de um associado de um plano de saúde é finito.

No entanto, uma aproximação pode ser feita supondo que o número de consultas anual de um associado pode ser infinito.

Num plano de saúde com 5694 filiados, ao fim de um ano foram realizadas 13098 consultas, conforme tabela a seguir:

Número de consultas anuais de um associado	| Frequência
:------------------- | :----------
0	                  | 589
1	                  | 1274
2	                  | 1542
3	                  | 1144
4                   |	663
5	                  | 304
6	                  | 126
7	                  | 39
8	                  | 10
9	                  | 3

$$\begin{align}
\lambda&=\dfrac{0\times589+1\times1274+\cdots+9\times3}{589+1274+\cdots+3}\\
\lambda&=2.3 \;\text{consultas por associado por ano}
\end{align}$$

O valor 2.3 é o número médio de consultas anual de um associado típico deste plano de saúde.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
lambda <- 2.3
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+5*dp)
cat("X ~ Poisson(lambda =", lambda,")\n")
cat("Media de X =",m,"\n")
cat("Desvio-padrao de X =",dp,"\n\n")
probs <- dpois(x=X,lambda=lambda)
probsacum <- ppois(q=X,lambda=lambda)
print(tbl <- round(data.frame(X,probs,probsacum),6))
cat("\nP(X=0) + P(X=1) + ... + P(X=10) =",sum(probs),"\n")
# lower.tail: logical; if TRUE (default), probabilities are P[X <= x], 
#                      otherwise, P[X > x]
px01 <- ppois(q=1,lambda=lambda,lower.tail=TRUE)
cat("P(X=0) + P(X=1) =", px01, "\n")
px23inf <- ppois(q=1,lambda=lambda,lower.tail=FALSE)
cat("P(X=2) + P(X=3) + ... =", px23inf, "\n")
plot(X,probs,type="h",
     xlim=c(-1,(m+5*dp)),ylim=c(0,1.1*max(probs)), 
     main=paste("Distribuicao de Poisson(",lambda,")"),
     lwd=1,col="black",ylab="Probabilidade")
points(X,probs,pch=16,cex=1,col="black")
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
lambda <- 2.3
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+5*dp)
cat("X ~ Poisson(lambda =", lambda,")\n")
cat("Media de X =",m,"\n")
cat("Desvio-padrao de X =",dp,"\n\n")
probs <- dpois(x=X,lambda=lambda)
probsacum <- ppois(q=X,lambda=lambda)
print(tbl <- round(data.frame(X,probs,probsacum),6))
cat("\nP(X=0) + P(X=1) + ... + P(X=10) =",sum(probs),"\n")
# lower.tail: logical; if TRUE (default), probabilities are P[X <= x], 
#                      otherwise, P[X > x]
px01 <- ppois(q=1,lambda=lambda,lower.tail=TRUE)
cat("P(X=0) + P(X=1) =", px01, "\n")
px23inf <- ppois(q=1,lambda=lambda,lower.tail=FALSE)
cat("P(X=2) + P(X=3) + ... =", px23inf, "\n")
plot(X,probs,type="h",
     xlim=c(-1,(m+5*dp)),ylim=c(0,1.1*max(probs)), 
     main=paste("Distribuicao de Poisson(",lambda,")"),
     lwd=1,col="black",ylab="Probabilidade")
points(X,probs,pch=16,cex=1,col="black")
```

## Aproximação da binomial pela Poisson

$X \sim binomial(n,p)$

Se $n > 20$ e $p<0.05$, então a binomial pode ser aproximada por Poisson$(np)$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
n <- 10
p <- 0.1
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n >= 20 && p <= 0.05)
{cat("Binomial(",n,", ",p,") semelhante a Poisson(",m,")\n\n", sep="")} else
{cat("Binomial(",n,", ",p,") diferente de Poisson(",m,")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probPois <- dpois(x=X,lambda=m)
print(round((tbl <- data.frame(X,probbinom,probPois)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,"): circulo\n\n"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
plot(X,probPois,type="h",
     main=paste0("Distribuicao de Poisson(",m,"): quadrado"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probPois,pch=12,cex=1,col="black")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
n <- 10
p <- 0.1
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n >= 20 && p <= 0.05)
{cat("Binomial(",n,", ",p,") semelhante a Poisson(",m,")\n\n", sep="")} else
{cat("Binomial(",n,", ",p,") diferente de Poisson(",m,")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probPois <- dpois(x=X,lambda=m)
print(round((tbl <- data.frame(X,probbinom,probPois)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,"): circulo\n\n"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
plot(X,probPois,type="h",
     main=paste0("Distribuicao de Poisson(",m,"): quadrado"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probPois,pch=12,cex=1,col="black")
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
n <- 30
p <- 0.03
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n >= 20 && p <= 0.05)
{cat("Binomial(",n,", ",p,") semelhante a Poisson(",m,")\n\n", sep="")} else
{cat("Binomial(",n,", ",p,") diferente de Poisson(",m,")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probPois <- dpois(x=X,lambda=m)
print(round((tbl <- data.frame(X,probbinom,probPois)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,"): circulo\n\n"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
plot(X,probPois,type="h",
     main=paste0("Distribuicao de Poisson(",m,"): quadrado"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probPois,pch=12,cex=1,col="black")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
n <- 30
p <- 0.03
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n >= 20 && p <= 0.05)
{cat("Binomial(",n,", ",p,") semelhante a Poisson(",m,")\n\n", sep="")} else
{cat("Binomial(",n,", ",p,") diferente de Poisson(",m,")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probPois <- dpois(x=X,lambda=m)
print(round((tbl <- data.frame(X,probbinom,probPois)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,"): circulo\n\n"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
plot(X,probPois,type="h",
     main=paste0("Distribuicao de Poisson(",m,"): quadrado"),
     xlab="X", ylab="probabilidade",
     xlim=c(-1,m+5*dp), ylim=c(0,1.1*max(c(max(probbinom),max(probPois)))),
     lwd=1,col="black")
points(X,probPois,pch=12,cex=1,col="black")
```

### Exemplo: Anestésico

Uma em cada mil pessoas que utilizam determinado anestésico sofre uma reação negativa (choque).

#### Questão 1

Num total de 500 cirurgias em que se empregou esse anestésico, a probabilidade de que uma pessoa sofra a reação negativa é:

R.: $0.3$

Como $n = 500 > 20$ e $p = 1/1000 \le 0.05$, então a $binomial(500,1/1000)$ é aproximadamente igual à Poisson$(500\times(1/1000))$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
dbinom(x=1, size=500, prob=1/1000)
dpois(x=1,lambda=500*(1/1000))
```

> Arango, 2012, p. 187

> ARANGO, HG (2012) _Bioestatística_. 3a ed. RJ: Guanabara Koogan.

#### Questão 2

Num total de 500 cirurgias em que se empregou esse anestésico a probabilidade de que nenhuma pessoa sofra a reação negativa é:

R.: $0.61$

Como $n = 500 > 20$ e $p = 1/1000 \le 0.05$, então a $binomial(500,1/1000)$ é aproximadamente igual à Poisson$(500\times(1/1000))$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
dbinom(x=0, size=500, prob=1/1000)
dpois(x=0,lambda=500*(1/1000))
```

#### Questão 3

Num total de 500 cirurgias em que se empregou esse anestésico a probabilidade de que mais de uma pessoa sofra a reação negativa é:

R.: $0.09$

Como $n = 500 > 20$ e $p = 1/1000 \le 0.05$, então a $binomial(500,1/1000)$ é aproximadamente igual à Poisson$(500\times(1/1000))$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
1-pbinom(q=1, size=500, prob=1/1000)
1-ppois(q=1, lambda=500*(1/1000))
```

## Exemplo 13.10.1: Distribuição binomial negativa em Ecologia

* `stats::NegBinomial`

* [Negative binomial distribution: Wikipedia](https://en.wikipedia.org/wiki/Negative_binomial_distribution){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

* [`negative binomial distribution`](https://www.wolframalpha.com/input?i=negative+binomial+distribution){target="_blank"}

A distribuição de plantas e animais em uma região raramente é uma distribuição de Poisson. Por alguma razão, os indivíduos podem ser agregados em grupos. Então, contando a frequência dentro dos quadrats, obtemos muitos quadrats vazios e muitos quadrats com altas frequências. Isso aumenta a variância. A igualdade $\sigma^2=\lambda$ não é mais válida. Em vez disso, encontramos $\sigma^2>\lambda$.

O contrário também pode ocorrer: considere o caso em que há aproximadamente a mesma distância de um indivíduo para seu vizinho.

A distribuição é quase uniforme. Exemplos são as árvores em um pomar ou cabelos na pele. Por quadrat contamos quase o mesmo número de indivíduos. A variação é relativamente pequena. Portanto, $\sigma^2<\lambda$.

A distribuição binomial negativa pode ser uma solução adequada para modelar a agregação de indivíduos em ecologia, como descrito no exemplo. Vamos entender por quê e como ela se aplica.

### Contexto e Problema

1. **Distribuição Poissoniana**: 
   - Supõe eventos raros e independentes, com a variância (\(\sigma^2\)) igual à média (\(\lambda\)).
   - \( \sigma^2 = \lambda \)

2. **Agregação de Indivíduos**:
   - Indivíduos formam grupos, aumentando a variância.
   - \( \sigma^2 > \lambda \)

3. **Distribuição Quase Uniforme**:
   - Indivíduos são espaçados uniformemente, diminuindo a variância.
   - \( \sigma^2 < \lambda \)

### Solução com Distribuição Binomial Negativa

#### Distribuição Binomial Negativa

A distribuição binomial negativa é utilizada para modelar o número de falhas até um número fixo de sucessos em ensaios de Bernoulli. É particularmente útil quando os dados apresentam superdispersão, isto é, quando a variância é maior que a média (\(\sigma^2 > \lambda\)).

#### Aplicação na Ecologia

- **Superdispersão**: Quando os indivíduos estão agrupados, a contagem de indivíduos em diferentes quadrats tende a mostrar uma variância maior que a média. A distribuição binomial negativa captura essa característica, permitindo modelar a superdispersão.
- **Probabilidade de Contagem**: A distribuição binomial negativa pode ser parametrizada para ajustar a probabilidade de contagens em quadrats com frequências muito altas ou vazias.

#### Fórmulas

A distribuição binomial negativa com parâmetros \(r\) (número de sucessos) e \(p\) (probabilidade de sucesso) tem a seguinte função de probabilidade de massa (pmf):

\[ P(X = k) = \binom{k + r - 1}{k} (1 - p)^r p^k \]

Para um modelo de contagem, a parametrização pode ser feita em termos da média (\(\mu\)) e dispersão (\(k\)):

\[ \mu = \dfrac{r(1-p)}{p} \]
\[ \sigma^2 = \mu + \dfrac{\mu^2}{k} \]

Onde \(k\) é um parâmetro de dispersão.

### Conclusão

A distribuição binomial negativa é apropriada para modelar a situação onde a variação da contagem de indivíduos em quadrats é maior que a média, devido à agregação de indivíduos. Portanto, para dados ecológicos com superdispersão (\(\sigma^2 > \lambda\)), a distribuição binomial negativa é uma solução mais adequada que a distribuição de Poisson.

### Exemplo de Aplicação

Se você estiver contando o número de plantas em quadrats e observar que a variação é significativamente maior que a média, a distribuição binomial negativa pode fornecer um ajuste melhor para os seus dados. Esse modelo permitiria capturar a maior variabilidade observada e oferecer previsões mais precisas para a distribuição das plantas.

### Quasi-Negative Binomial Distribution

* Shoukri & Aleid (2020)

As distribuições de Poisson e Binomial Negativa são comumente usadas para modelar dados de contagem. A distribuição de Poisson é caracterizada pela igualdade entre média e variância, enquanto a Binomial Negativa possui uma variância maior que a média, sendo ambas apropriadas para modelar dados de contagem com superdispersão.

Neste artigo, está sendo estudada uma nova distribuição de probabilidade com dois parâmetros, chamada Distribuição Quase-Binomial Negativa (QNBD, do inglês *Quasi-Negative Binomial Distribution*), que generaliza a bem conhecida distribuição binomial negativa. Este modelo mostra-se bastante flexível para a análise de dados de contagem.

## Exemplo 13.10.2: Contagem de bactérias e sangue

[Hemocytometer: Wikipedia](https://en.wikipedia.org/wiki/Hemocytometer){target="_blank"}

Em uma pequena lâmina com uma grade quadrada, um líquido contendo células individuais é espalhado de forma homogênea. Grande cuidado é tomado para que o líquido tenha uma espessura constante.

Ao microscópio contamos o número de quadrados sem células, com uma célula, com duas células etc. A distribuição observada deve estar próxima de uma distribuição de Poisson. Este é frequentemente o caso. Grandes desvios podem ocorrer devido a flutuações aleatórias ou, mais provavelmente, devido a um fator que causa aglomerados de células.

Existem também instrumentos para contagem automática de células. O líquido passa por um tubo de vidro estreito. Cada célula que passa causa uma mudança na transparência. Um olho eletrônico é capaz de registrar as células com alta velocidade, mas com precisão restrita.

Quando as contagens de quantidades iguais de líquido são comparadas, a distribuição deve novamente se assemelhar a uma distribuição de Poisson.

Para realizar o teste da hipótese nula de distribuição de Poisson, há a função do R `vcd::goodfit`.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
set.seed(123)
p <- rpois(n=1e4, lambda=5)
b <- rbinom(n=1e5, size=10, prob=0.5)

gfp <- vcd::goodfit(p, type="poisson")
plot(gfp, main="Count data vs Poisson distribution")
summary(gfp)

gfb <- vcd::goodfit(p, type="binomial", 
                    par=list(prob=0.5, size=10))
summary(gfb)

gfb <- vcd::goodfit(b, type="binomial",
                    par=list(prob=0.5, size=10))
plot(gfb, main="Count data vs binomial distribution")
summary(gfb)

gfp <- vcd::goodfit(b, type="poisson")
summary(gfp)
```

## Exemplo 13.10.3: Mutações

Um grande número de placas de ágar é tratado com um antibiótico. As bactérias que estão espalhadas pelas placas não podem se multiplicar, exceto aqueles raros mutantes que são resistentes a antibióticos. Eles formam colônias. Se as condições experimentais puderem ser mantidas razoavelmente constantes para todas as placas de ágar, a contagem de colônias deve resultar aproximadamente em uma distribuição de Poisson.

## Exemplo 13.10.4: Decaimento radioativo

Em uma substância radioativa, a desintegração dos núcleos ocorre espontaneamente, ou seja, a desintegração não é causada por fatores fora do núcleo. O número de pulsos em um contador Geiger durante intervalos de tempo de comprimento fixo são registrados.

Existem intervalos sem pulso, com um pulso, com dois pulsos etc.

A hipótese de uma distribuição de Poisson é bem suportada por fatos experimentais.

Nesta aplicação, os intervalos de tempo desempenham o mesmo papel que os quadrados em ecologia ou as placas de ágar em bacteriologia. A distribuição de Poisson é aplicável a intervalos de tempo, bem como a seções de espaço.

## Exemplo 13.10.5 Vida cotidiana

Quando uma tempestade de chuva começa, as primeiras gotas em uma estrada pavimentada são distribuídas de acordo com a lei de Poisson.

O mesmo vale para os erros de impressão de um livro, o número de cartas ou telefonemas que recebemos durante a semana. Em um hospital, o número de nascimentos ou mortes por dia, em um cruzamento de rua, o número mensal de acidentes de trânsito, na indústria, o número de itens defeituosos produzidos por hora, todos seguem mais ou menos a lei de Poisson.

# Distribuição contínua

## Exemplo 13.11.1:  &alpha;-globulina

O conteúdo de &alpha;-globulina $x$ no plasma sanguíneo de um grande número de adultos humanos saudáveis é medido. Cada medição é expressa em gramas por 100 mililitros. Estamos interessados na frequência relativa de ocorrência. Para uma pesquisa aproximada, podemos subdividir as medições em três grupos de largura 0.18 g/100 ml com pontos médios em 0.60, 0.78, 0.96 g/100 ml. O histograma (a) da Fig. 13.12 descreve o resultado. A área de cada barra é numericamente igual à frequência relativa correspondente. A área total do histograma é 1 ou 100%.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.12. A distribuição de &alpha;-globulina no plasma sanguíneo de humanos adultos. Para obter maior precisão, o número de grupos é aumentado. No limite a distribuição é contínua."}
knitr::include_graphics("./image/Fig13.12.png")
```

Para informações mais precisas, podemos usar um número maior de grupos. O histograma (b) da Fig. 13.12 mostra grupos de largura 0.06 g/100 ml com pontos médios em 0.60, 0.66, ..., 1.02 g/100 ml. Novamente a área de cada barra é numericamente igual à frequência relativa correspondente.

As medições reais podem ser feitas com tanta precisão que uma distinção ainda mais precisa é possível, conforme demonstrado pelo histograma (c) da Fig. 13.12.

Devido à imperfeição de nossos instrumentos, no entanto, a precisão não pode ser aumentada indefinidamente. Para a apresentação de dados empíricos sempre seremos forçados a usar um número finito de grupos. A distribuição da frequência relativa é necessariamente discreta.

Por outro lado, para uma distribuição teórica, hesitamos em usar uma subdivisão bastante arbitrária em grupos. Sentimos que nossa quantidade $x$ varia continuamente. Em outras palavras: consideramos $x$ como uma variável aleatória contínua e a denotamos por $X$. Ao mesmo tempo, substituímos frequências relativas por probabilidades. Não temos mais um histograma com barras simples. Em vez disso, desenhamos uma curva suave cujas ordenadas indicam a densidade de probabilidade em cada ponto do eixo $x$ (Fig. 13.12d). (Para a noção de densidade, veja o final da Seção 9.1.) Considere a curva como o gráfico de uma certa função $y = f(x)$. Então $f(x)$ é chamada de função de densidade de probabilidade. Uma distribuição que tem uma função de densidade é dita ser contínua.

Qual é a relação entre a densidade de probabilidade e a própria probabilidade? 

A transição de uma distribuição discreta para uma distribuição contínua nos dá uma pista. 

Fixamos um intervalo $[a, b]$, sendo $b>a$, no eixo $x$ e pedimos a frequência relativa de todas as medições que se enquadram nesse intervalo. Como vemos da Fig. 13.12c temos que somar a frequência relativa de todos os grupos que caem no intervalo, ou seja, 5.2% + 3.6% + 2.8% = 11.6%. Em outras palavras: Devemos encontrar a área total de todas as barras do histograma acima de $[a, b]$. De forma bastante correspondente, obtemos a probabilidade de que a variável $X$ cai no intervalo $[a, b]$ calculando a área entre $[a, b]$ e a curva (Fig. 13.8d). Portanto, 

$$P(a \le X \le b)=\int_{a}^{b}{f(x)dx}$$

isto é, a probabilidade do evento composto $X \in [a, b]$ é a integral da função densidade no intervalo $[a, b]$.

Por que não determinamos primeiro a probabilidade de um evento simples?

Seja $a$ um número fixo. Então $X = a$ seria um evento simples. Então:

$$P(X = a) = \int_{a}^{a}{f(x)dx} = 0$$

uma vez que a área sobre um ponto do eixo $x$ é zero. O resultado pode parecer surpreendente. O evento $X = a$ não é de forma alguma impossível. No entanto, sua probabilidade desaparece. No entanto, não há contradição. Seja $a = 2/3 = 0.\bar{6}$ um número preciso e não arredondado. É extremamente improvável que uma quantidade assuma esse valor específico. Além disso, existem infinitos outros números na vizinhança de $a$, de modo que a probabilidade de um evento simples de uma distribuição contínua deve desaparecer.

Na fórmula $P(a \le X \le b)=\int_{a}^{b}{f(x)dx}$ expressamos uma probabilidade em termos da função de densidade de probabilidade. 

O inverso também é possível, ou seja, podemos derivar a função de densidade de probabilidade a partir do conhecimento das probabilidades? 
A resposta é sim. 

Como um passo preliminar nessa direção, introduzimos uma nova função.

Para simplificar, suponha que a probabilidade total 1 esteja contida em um intervalo finito, digamos de $A$ a $B$ no eixo $x$ (Fig. 13.13). Isso significa que: 

$$\int_{A}^{B}{f(x)dx}=1$$

Consideramos a probabilidade de que a variável aleatória $X$ caia em um intervalo $[A, x]$ onde o limite superior $x$ é variável. Esta probabilidade é:

$$F(x)=P(X \le x) = \int_{A}^{x}{f(t)dt}$$

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.13. A função de distribuição de uma distribuição de probabilidade contínua."}
knitr::include_graphics("./image/Fig13.13.png")
```

É um tanto confuso que $x$ apareça como a variável de integração e, ao mesmo tempo, como a variável limite superior da integral. Portanto, mudamos a notação da variável de integração e escolhemos $t$ em vez de $x$. A probabilidade $P(X \le x)$ é uma função de $x$ e será denotada por $F(x)$. 

A nova função $F(x)$ é chamada de função de distribuição. 

Tem as seguintes propriedades:

a. $F(A) = 0$ 
a. $F(B) = 1$ 
a. $F(x)$ cresce monotonicamente de $A$ para $B$.

Sabemos que integração e diferenciação são operações inversas. 

Portanto, implica que: 

$$F^{\prime}(x) = f(x)$$

ou seja, a função de densidade de probabilidade é a derivada da função de distribuição. Assim, a função de densidade de probabilidade é a derivada da probabilidade associada ao intervalo $[A, x]$.

O conhecimento da função de distribuição $F(x)$ é bastante útil em todas as aplicações da teoria da probabilidade e da estatística. 

Por exemplo, se $[a, b]$ denota um intervalo do eixo $x$ que pertence a $[A, B]$, então obtemos:

$$\begin{align}
P(X \le a) &= F(a)\\
P(X \le b) &= F(b)\\
P(a < X \le b) &= P(X \le b) - P(X \le a)\\
P(a < X \le b) &= F(b) - F(a)
\end{align}$$

Como para uma distribuição contínua $P(X=a)=0$ e $P(X=b)=0$, também podemos escrever $P(a<X<b)=F(b) - F(a)$ ou $P(a \le X \le b) = F(b) - F(a)$.

O resultado é representado na Fig. 13.14.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.14. Cálculo de P(a < X < b)."}
knitr::include_graphics("./image/Fig13.14.png")
```

## Exemplo 13.11.2: Distribuição uniforme contínua

[Continuous uniform distribution: Wikipedia](https://en.wikipedia.org/wiki/Continuous_uniform_distribution){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`uniform distribution`](https://www.wolframalpha.com/input?i=uniform+distribution){target="_blank"}

Em um estudo de comportamento animal, as aves foram soltas uma de cada vez em circunstâncias que dificultaram muito a orientação. Esperava-se que os pássaros escolhessem direções aleatórias. O que queremos dizer com "aleatório" nesta conexão?

As direções podem ser definidas pelo azimute $\alpha$, ou seja, pelo ângulo entre o norte e a direção medida no sentido horário. A direção é dita aleatória se cada azimute de 0° a 360° tem a mesma chance de sendo escolhido, ou mais precisamente, se cada azimute tem a mesma densidade de probabilidade. A função de densidade de probabilidade $f(\alpha)$ é, portanto, uma constante no intervalo [0°, 360°]. Como a área entre o intervalo [0°, 360°] e o gráfico de $f(\alpha)$ deve ser 1, o valor constante de $f(\alpha)$ é 1/360 (Fig. 13.15).

$$\begin{align}
f(\alpha)&=\dfrac{1}{360}\\
\alpha&\in [0^\circ,360^\circ]
\end{align}$$

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.15. A distribuição uniforme definida no intervalo [0°, 360°]."}
knitr::include_graphics("./image/Fig13.15.png")
```

A função de distribuição da uniforme contínua é:

$$F(\alpha)=\int_{0}^{\alpha}{\dfrac{1}{360}d\alpha}=\dfrac{\alpha}{360}\\
0^\circ\le \alpha \le 360^\circ$$

O exemplo pode ser generalizado para qualquer intervalo finito $[A, B]$ do eixo $x$. Uma distribuição com densidade de probabilidade constante é chamada de distribuição uniforme contínua.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`uniform distribution min=0 max=360`](https://www.wolframalpha.com/input?i=uniform+distribution+min%3D0+max%3D360){target="_blank"}

## Exemplo 13.11.3: Distribuição exponencial: Cintilações da radioatividade 

* [Exponential distribution: Wikipedia](https://en.wikipedia.org/wiki/Exponential_distribution){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

* [`exponential distribution`](https://www.wolframalpha.com/input?i=exponential+distribution){target="_blank"}

Já sabemos que na radioatividade as cintilações seguem uma distribuição de Poisson (ver Exemplo 13.10.4).

Os instantes de tempo em que os núcleos decaem são denotados por $t_1, t_2 , \ldots$ e plotados como pontos em um eixo de tempo na Fig. 13.16. 

Além disso, os intervalos de tempo entre valores $t$ consecutivos são denotados por $\Delta t_1, \Delta t_2, ...$.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.16. Intervalos de tempo entre decaimentos nucleicos subsequentes. A distribuição de probabilidade desses intervalos é exponencial."}
knitr::include_graphics("./image/Fig13.16.png")
```

Podemos tratar $\Delta t$ como uma variável aleatória e introduzir uma nova e mais simples notação $X=\Delta t$.

Essa variável aleatória é tipicamente contínua, pois $X$ pode assumir qualquer valor real de zero até o infinito.

Estamos interessados na distribuição de $X$, especialmente na função de densidade de probabilidade $f(x)$. Um argumento teórico que não é apresentado aqui revela que: 

$$f(x)=\dfrac{1}{\mu}\exp\left(-\dfrac{1}{\mu}x\right)$$

sendo que $\mu = \mathbb{E}(X)$ é a duração média entre instantes de tempo consecutivos de núcleos em decomposição. 

Uma distribuição com densidade $f(x)$ é chamada de distribuição exponencial. 

Para o valor particular $\mu = 2.5$ seg, é representado na Fig. 13.17.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.17. Densidade de probabilidade da distribuição exponencial com expectativa &mu;=2.5."}
knitr::include_graphics("./image/Fig13.17.png")
```

[`exponential distribution lambda = 1/2.5: Desmos`](https://www.desmos.com/calculator/r0kzbndour){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`exponential distribution lambda = 1/2.5`](https://www.wolframalpha.com/input?i=exponential+distribution+lambda+%3D+1%2F2.5){target="_blank"}

Obtemos a seguinte fórmula para a função de distribuição da exponencial:

$$F(x)=\int_{0}^{x}{f(t)dt}=\int_{0}^{x}{\dfrac{1}{\mu}e^{-t/\mu}dt}=-e^{-t/\mu}\LARGE|\large_0^x$$

ou

$$F(x)=1-\exp\left(-\dfrac{1}{\mu}x\right)\\
x\ge 0$$

Podemos verificar facilmente que $F(0) = 0$ e $\lim_{x \to \infty}{F(x)}  = 1$.

## Exemplo 13.11.4: Distribuição normal padrão

[Standard Normal distribution: Wikipedia](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution){target="_blank"}

Muitas distribuições são em forma de sino e simétricas em relação à média. 

Um modelo teórico frequentemente usado para distribuição normal padrão é definido pela função de densidade:

$$\phi(x)=\dfrac{1}{\sqrt{2\pi}}\exp\left(-\dfrac{1}{2}x^2\right)\\
x\in \mathbb{R}$$

Observe que o expoente $-x^2/2$ é negativo e que $x$ é elevado ao quadrado. O coeficiente da função exponencial é $1/\sqrt{2\pi} \approx 0.4$. 

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.18. A distribuição normal com média 0 e desvio-padrão 1."}
knitr::include_graphics("./image/Fig13.18.png")
```

1) A probabilidade total é igual a 1:

$$\int_{-\infty}^{\infty}{\phi(t)dt}=\dfrac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}{\exp\left(-\dfrac{1}{2}t^2\right)dt}=1$$

2) A média ou a esperança da distribuição é igual a 0, i.e., $\mu=\mathbb{E}(X)=0$.

3) O desvio-padrão é igual a 1, i.e., $\sigma=\sqrt{\mathbb{V}(X)}=1$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`normal distribution mean=0 standard deviation=1`](https://www.wolframalpha.com/input?i=normal+distribution+mean%3D0+standard+deviation%3D1){target="_blank"}

Essa distribuição é chamada de distribuição normal ou Gaussiana com média 0 e desvio-padrão 1. Um gráfico da função de densidade é mostrado na Fig. 13.18. A função de densidade atinge apenas um máximo. Dizemos que a distribuição tem apenas uma moda ou que é unimodal.

Como a distribuição normal varia de $-\infty$ a $+\infty$, não parece ser uma distribuição adequada para aplicações nas ciências naturais. Nenhuma quantidade real pode atingir um valor infinito. No entanto, devemos julgar a distribuição normal de um ponto de vista diferente.

Devido à função exponencial na função de densidade $f(x)$ diminui muito rapidamente à medida que $|x|$ aumenta. A probabilidade de um valor fora do intervalo $[ -3, +3]$ é 0.0027, e de um valor fora de $[ -4, +4]$ apenas 0.00004. Tal evento é improvável.

A função de distribuição normal padrão é denotada por $\Phi(x)$:

$$\Phi(x)=\int_{-\infty}^{x}{\dfrac{1}{\sqrt{2\pi}}\exp\left(-\dfrac{1}{2}t^2\right)dt}\\
x\in \mathbb{R}$$

```{r echo=FALSE, out.width="90%", fig.cap="Distribuição normal padrão."}
knitr::include_graphics("./image/z.png")
```

[`Normal Padrão: Desmos`](https://www.desmos.com/calculator/zyqntguhwr){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`cumulative normal distribution mean=0 standard deviation=1`](https://www.wolframalpha.com/input?i=cumulative+normal+distribution+mean%3D0+standard+deviation%3D1){target="_blank"}

<!-- [`1/2 (1 + erf(1/sqrt(2)))` ](https://www.wolframalpha.com/input?i=1%2F2+%281+%2B+erf%281%2Fsqrt%282%29%29%29+){target="_blank"} -->

<!-- [`1/2 (1 + erf(2/sqrt(2)))` ](https://www.wolframalpha.com/input?i=1%2F2+%281+%2B+erf%282%2Fsqrt%282%29%29%29+){target="_blank"} -->

<!-- [`1/2 (1 + erf(3/sqrt(2)))` ](https://www.wolframalpha.com/input?i=1%2F2+%281+%2B+erf%283%2Fsqrt%282%29%29%29+){target="_blank"} -->

[`1/2 (1 + erf(x/sqrt(2))) = 0.975`](https://www.wolframalpha.com/input?i=1%2F2+%281+%2B+erf%28x%2Fsqrt%282%29%29%29+%3D+0.975){target="_blank"}

[`1/2 (1 + erf(x/sqrt(2))) = 0.75`](https://www.wolframalpha.com/input?i=1%2F2+%281+%2B+erf%28x%2Fsqrt%282%29%29%29+%3D+0.75){target="_blank"}

## Exemplo 13.11.5: Distribuição normal 

[Normal distribution: Wikipedia](https://en.wikipedia.org/wiki/Normal_distribution){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`normal distribution`](https://www.wolframalpha.com/input?i=normal+distribution){target="_blank"}

O gráfico na Fig. 13.18 pode ser "espremido" ou "esticado" para obter um desvio-padrão diferente de 1. Também pode ser deslocado ao longo do eixo $x$ para fornecer um valor médio $\mu$ que difere de 0. 

A nova função de densidade é:

$$\begin{align}
\phi(x)&=\dfrac{1}{\sigma\sqrt{2\pi}}\exp\left(-\dfrac{1}{2}\left(\dfrac{x-\mu}{\sigma}\right)^2\right)\\
x&\in \mathbb{R}\\
\mu&\in \mathbb{R}\\
\sigma&\in \mathbb{R_+^*}
\end{align}$$

$$\begin{align}
\phi(x)&=\dfrac{1}{\sigma\sqrt{2\pi}}\exp\left(-\dfrac{1}{2}z^2\right)\\
z&=\dfrac{x-\mu}{\sigma}\in \mathbb{R}\\
x&\in \mathbb{R}\\
\mu&\in \mathbb{R}\\
\sigma&\in \mathbb{R_+^*}
\end{align}$$

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.19. Gráficos da distribuição normal com médias &mu;<sub>1</sub>, &mu;<sub>2</sub> e desvios-padrão &sigma;<sub>1</sub>, &sigma;<sub>2</sub>, respectivamente. Geometricamente, o desvio-padrão é a distância horizontal da média a um dos pontos de inflexão."}
knitr::include_graphics("./image/Fig13.19.png")
```

Mais uma vez, omitimos a demonstração matemática, pois ela exigiria ferramentas que vão além do escopo deste livro. A função de densidade é plotada na Fig. 13.19 para dois valores diferentes da média e dois valores do desvio-padrão. Os dois gráficos têm alturas diferentes, pois a área entre o eixo $x$ e cada gráfico deve ser unitária.

[`Normal: Desmos`](https://www.desmos.com/calculator/nl6svoyhqj){target="_blank"}

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`normal distribution: Wolfram Cloud`](https://www.wolframcloud.com/obj/siqueira0/Published/BiNorm.nb){target="_blank"}

[`normal distribution mean=170 standard deviation=10`](https://www.wolframalpha.com/input?i=normal+distribution+mean%3D170+standard+deviation%3D10){target="_blank"}

[`cumulative normal distribution mean=170 standard deviation=10`](https://www.wolframalpha.com/input?i=cumulative+normal+distribution+mean%3D170+standard+deviation%3D10){target="_blank"}

Numerosas quantidades nas ciências naturais parecem ser normalmente distribuídas, e muitos procedimentos estatísticos são baseados na suposição de uma distribuição normal subjacente. 

Surge a pergunta: 

A ocorrência frequente da distribuição normal é um fato puramente empírico ou tem uma base teórica?

Para responder a esta pergunta, consideremos primeiro um exemplo: a estatura de pessoas adultas. É sabido que a estatura corporal é essencialmente determinada por fatores genéticos. Há boas razões para supor que vários genes localizados em diferentes _loci_ contribuem para a estatura do corpo.

Alguns deles são relacionados ao sexo. Cada gene depende da sorte.

Antes e depois do nascimento, muitos outros fatores também contribuem para a estatura do corpo.

Esses fatores são nutrição, ambiente, estado de saúde, trabalho e exercício. Alguns dos muitos fatores genéticos e não genéticos tendem a aumentar, alguns a diminuir a altura. A maioria dos fatores agem de forma independente. Agora formamos um modelo matemático para a estatura de pessoas adultas. Denote a estatura por $H$ e as contribuições de vários fatores para $H$ por $X_1, X_2, \ldots$. Então:

$$H=\sum_{i=1}^{n}{X_i}$$

A estatura $H$ assim como os componentes $X_i$ são variáveis aleatórias.

Não conhecemos as distribuições de $X_1, X_2, \ldots$ em detalhes. Alguns dos componentes podem assumir apenas valores positivos ou apenas negativos, outros podem ser capazes de ambos os sinais. Apenas sabemos por experiência que todos esses componentes são razoavelmente limitado em tamanho. 

Podemos pensar que na ausência de qualquer conhecimento preciso muito pouco pode ser dito sobre a distribuição de $H$. Surpreendentemente, este não é o caso. Matemáticos são capazes de provar que a distribuição de $H$ é aproximadamente normal.

A este respeito, o chamado teorema do limite central (TLC) afirma:

Seja $X_1, X_2, \ldots$ uma sequência infinita de variáveis aleatórias.

Assuma que:

a. $X_1, X_2, \ldots$ são mutuamente independentes,
a. cada $X_i$ assume apenas valores de um intervalo finito $[-A, A]$ sendo que $A$ denota uma constante,
a. a soma das variâncias $\sum_{i=1}^{n}{\mathbb{V}(X_i)}=\sum_{i=1}^{n}{\sigma_i^2}$ tende para infinito se $n\to\infty$,

A distribuição da soma parcial $S_n=\sum_{i=1}^{n}{X_i}$ tende para a distribuição normal se $n\to\infty$.

Vamos considerar este teorema básico e suas consequências em detalhes.

A suposição (a) nem sempre é satisfeita nas aplicações. No exemplo da estatura corporal, pode haver alguma interação entre fatores genéticos ou entre fatores ambientais. 

Com relação à suposição (b), não há dificuldade alguma, pois não há restrição imposta ao tamanho de $A$ e todas as quantidades nas ciências naturais são necessariamente finitas. 

A suposição (c) implica que existem infinitos componentes cujas variâncias não são muito pequenas. Essa suposição quase nunca é violada nas ciências naturais. 

Uma restrição é imposta pela fórmula da soma parcial: Os componentes $X_i$ são considerados aditivos.

Essa suposição não precisa ser satisfeita. Podemos, por exemplo, pensar em componentes que devem ser multiplicados.

[Galton board: Wikipedia](https://en.wikipedia.org/wiki/Galton_board){target="_blank"}

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.20. Caixa de Galton. Através de um funil, pequenas bolas (por exemplo, tiro) entram em uma placa que é inclinada para a horizontal. Ao descer, as bolas batem nos pregos que são colocados no tabuleiro em várias fileiras. Cada bola é desviada para a direita ou para a esquerda sempre que colide com um prego ou outra bola. Ao pé do tabuleiro existem muitos compartimentos igualmente espaçados que recolhem as bolas. Os compartimentos próximos ao centro recebem mais bolas. Para os lados, a frequência diminui. A distribuição se assemelha muito a uma distribuição normal. A razão é o efeito conjunto de um grande número de desvios aleatórios independentes que X<sub>i</sub> impôs às bolas. A distribuição de uma única variável aleatória X<sub>i</sub> não é conhecida."}
knitr::include_graphics("./image/Fig13.20.png")
```

Observe que os componentes podem ter distribuições discretas ou contínuas. Alguns dos componentes podem ter distribuições simétricas, outros assimétricas, alguns unimodais ou multimodais.

Quaisquer que sejam as distribuições dos componentes, o resultado é sempre o mesmo: com o aumento de $n$, a distribuição de $S_n$ se aproxima de uma distribuição normal. 

O Teorema do Limite Central (TLC) foi provado pela primeira vez pelo matemático russo Aleksandr Lyapunov (ou Liapunov) em 1901.

```{r echo=FALSE, out.width="70%", fig.cap="Fig. 13.21. Distribuição binomial com probabilidade de sucesso p = 0.3 e número de tentativas n = 20. A abcissa k denota o número de sucessos. À medida que n aumenta, a distribuição binomial se aproxima de uma distribuição normal (de Moivre)."}
knitr::include_graphics("./image/Fig13.21.png")
```

A frequente ocorrência de distribuições que se assemelham à distribuição normal é atribuída ao teorema do limite central. Um experimento modelo para esse teorema é a caixa de Galton, conforme mostrado na Fig. 13.20.

Finalmente, vamos aplicar o teorema do limite central à fórmula da binomial para o número de sucessos em $n$ tentativas de Bernoulli. As variáveis aleatórias $X_1, X_2, \ldots, X_n$. assumem apenas os valores 0 e 1 e são mutuamente independentes. Todas as suposições do teorema do limite central são satisfeitas.

Assim, a distribuição de $S_n$, conhecida como distribuição binomial, tende à distribuição normal à medida que $n$ tende ao infinito. Este resultado particular foi encontrado por de Moivre em 1733. Fig. 13.21 descreve uma distribuição binomial com probabilidade de sucesso $p = 0.3$ e $n = 20$ tentativas. Vemos que a distribuição é ligeiramente assimétrica, mas com o aumento do número de tentativas a assimetria desaparecerá. 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`normal distribution mean=0.3 20 standard deviation=(0.3 0.7 20)^(1/2)`](https://www.wolframalpha.com/input?i=normal+distribution+mean%3D0.3+20+standard+deviation%3D%280.3+0.7+20%29%5E%281%2F2%29){target="_blank"}

[`normal distribution mean=0.3 20 standard deviation=(0.3 0.7 20)^(1/2)`](binomial distribution n=20 p=0.3){target="_blank"}

[`Solve[D[(1/(sigma sqrt(2 pi))) exp(-(1/2) ((x - mu)/sigma)^2), x]==0,x]`](https://www.wolframalpha.com/input?i=Solve%5BD%5B%281%2F%28sigma+sqrt%282+pi%29%29%29+exp%28-%281%2F2%29+%28%28x+-+mu%29%2Fsigma%29%5E2%29%2C+x%5D%3D%3D0%2Cx%5D){target="_blank"}

[`Solve[D[(1/(sigma sqrt(2 pi))) exp(-(1/2) ((x - mu)/sigma)^2), {x,2}]==0,x]`](https://www.wolframalpha.com/input?i=Solve%5BD%5B%281%2F%28sigma+sqrt%282+pi%29%29%29+exp%28-%281%2F2%29+%28%28x+-+mu%29%2Fsigma%29%5E2%29%2C+%7Bx%2C2%7D%5D%3D%3D0%2Cx%5D){target="_blank"}

[`limit (1/(sigma sqrt(2 pi))) exp(-(1/2) ((x - mu)/sigma)^2) as x -> infinity`](hhttps://www.wolframalpha.com/input?i=limit+%281%2F%28sigma+sqrt%282+pi%29%29%29+exp%28-%281%2F2%29+%28%28x+-+mu%29%2Fsigma%29%5E2%29+as+x+-%3E+infinity){target="_blank"}

## Distribuição normal (gaussiana) em R

* Distribuição de probabilidade de variável contínua ilimitada bilateralmente $X$
* $X \sim normal(\mu,\sigma)$: variável aleatória $X$ representa valores contínuos reais
* $X\in\Re$
* Média de $X$: $\mu$
* Desvio-padrão de $X$: $\sigma$
* O desvio-padrão, $\sigma$, é a distância entre a média, $\mu$, e o ponto de inflexão da curva da normal
* A curva da normal é unimodal
* A curva da normal é simétrica 

```{r fig.align="center", out.width = '80%', out.height = '70%', echo=FALSE}
knitr::include_graphics("./image/normalpadrao.png")
```

### Variável com distribuição normal

* Variável bruta ou com transformação potência de Tukey (e.g., logaritmo, raiz quadrada)

* Somatométrica
  * Massa corporal total 
  * Comprimento total/ estatura
  * Comprimento de garra, presa, unha, pelo
  * Medidas fisiológicas: pressão sanguínea de humanos adultos
* Psicológica
  * Escore de teste cognitivo: QI
* Física
  * A soma de pequenos erros independentes de mensuração 

```{r fig.align="center", out.width = '70%', out.height = '40%', echo=FALSE, fig.cap="Figura 1. Os gráficos de linha mostram a distribuição de frequência ponderada suavizada, a mediana e o percentil 90 das pressões sistólicas para as populações de 18 a 29 anos e de 60 a 74 anos de idade nos Estados Unidos, de 1960 a 1991. (Fonte: Centros de Controle e Prevenção de Doenças, Centro Nacional de Estatísticas de Saúde.); https://www.ahajournals.org/doi/10.1161/01.HYP.26.1.60"}
knitr::include_graphics("./image/PA.png")
```

```{r fig.align="center", out.width = '70%', out.height = '50%', echo=FALSE}
knitr::include_graphics("./image/Lee2015.png")
```

```{r fig.align="center", out.width = '90%', out.height = '90%', echo=FALSE}
knitr::include_graphics("./image/Lee2015distrib.png")
```

#### Exemplo: Pressão arterial sistólica de jovens saudáveis 

A pressão arterial sistólica (PAS) medida em milímetro de mercúrio (mmHg) em jovens gozando de boa saúde tem distribuição $normal(120,10)$.

> Siqueira & Tibúrcio, 2011, p. 2010-1

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
m <- 120
dp <- 10
cat("X ~ Normal(",m,";",dp,")\n\n", sep="")
cat("P(X < ",m-3*dp,") = ",pnorm(q=m-3*dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m-2*dp,") = ",pnorm(q=m-2*dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m-dp,") = ",pnorm(q=m-dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m,") = ",pnorm(q=m,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m+dp,") = ",pnorm(q=m+dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m+2*dp,") = ",pnorm(q=m+2*dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m+3*dp,") = ",pnorm(q=m+3*dp,mean=m,sd=dp),"\n", sep="")
cat("\nP(",m-dp," < X < ",m+dp,") = ",
    pnorm(q=m+dp,mean=m,sd=dp)-pnorm(q=m-dp,mean=m,sd=dp),"\n", sep="")
cat("P(",m-2*dp," < X < ",m+2*dp,") = ",
    pnorm(q=m+2*dp,mean=m,sd=dp)-pnorm(q=m-2*dp,mean=m,sd=dp),"\n", sep="")
qnorm(p=0.975,mean=0,sd=1)
cat("P(",m-1.96*dp," < X < ",m+1.96*dp,") = ",
    pnorm(q=m+1.96*dp,mean=m,sd=dp)-pnorm(q=m-1.96*dp,mean=m,sd=dp),"\n", sep="")
cat("P(",m-3*dp," < X < ",m+3*dp,") = ",
    pnorm(q=m+3*dp,mean=m,sd=dp)-pnorm(q=m-3*dp,mean=m,sd=dp),"\n", sep="")
qnorm(p=0.75,mean=0,sd=1)
cat("P(",m-0.675*dp," < X < ",m+0.675*dp,") = ",
    pnorm(q=m+0.675*dp,mean=m,sd=dp)-pnorm(q=m-0.675*dp,mean=m,sd=dp),"\n", sep="")
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn),xlab="PAS(mmHg)",ylab="densidade",
     main=paste0("Distribuicao normal(",m,";",dp,")"))
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
m <- 120
dp <- 10
cat("X ~ Normal(",m,";",dp,")\n\n", sep="")
cat("P(X < ",m-3*dp,") = ",pnorm(q=m-3*dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m-2*dp,") = ",pnorm(q=m-2*dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m-dp,") = ",pnorm(q=m-dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m,") = ",pnorm(q=m,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m+dp,") = ",pnorm(q=m+dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m+2*dp,") = ",pnorm(q=m+2*dp,mean=m,sd=dp),"\n", sep="")
cat("P(X < ",m+3*dp,") = ",pnorm(q=m+3*dp,mean=m,sd=dp),"\n", sep="")
cat("\nP(",m-dp," < X < ",m+dp,") = ",
    pnorm(q=m+dp,mean=m,sd=dp)-pnorm(q=m-dp,mean=m,sd=dp),"\n", sep="")
cat("P(",m-2*dp," < X < ",m+2*dp,") = ",
    pnorm(q=m+2*dp,mean=m,sd=dp)-pnorm(q=m-2*dp,mean=m,sd=dp),"\n", sep="")
qnorm(p=0.975,mean=0,sd=1)
cat("P(",m-1.96*dp," < X < ",m+1.96*dp,") = ",
    pnorm(q=m+1.96*dp,mean=m,sd=dp)-pnorm(q=m-1.96*dp,mean=m,sd=dp),"\n", sep="")
cat("P(",m-3*dp," < X < ",m+3*dp,") = ",
    pnorm(q=m+3*dp,mean=m,sd=dp)-pnorm(q=m-3*dp,mean=m,sd=dp),"\n", sep="")
qnorm(p=0.75,mean=0,sd=1)
cat("P(",m-0.675*dp," < X < ",m+0.675*dp,") = ",
    pnorm(q=m+0.675*dp,mean=m,sd=dp)-pnorm(q=m-0.675*dp,mean=m,sd=dp),"\n", sep="")
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn),xlab="PAS(mmHg)",ylab="densidade",
     main=paste0("Distribuicao normal(",m,";",dp,")"))
```

##### Questão 1

Qual é a probabilidade que nenhum paciente tenha atendimento completo durante uma madrugada no pronto-atendimento dessa pequena cidade?

R.: $P(X > 140) = 0.023$ 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
print(p <- pnorm(q=140, mean=120, sd=10, lower.tail=FALSE))
# OU
1-pnorm(q=140, mean=120, sd=10)
DescTools::PlotProbDist(breaks=c(80,140,160), 
                        function(x) dnorm(x, mean=120, sd=10), 
                        blab=c(140), 
                        alab=c("",round(p,3)),
                        xlim=c(80,160),
                        main="normal(120,10)",
                        col=c("black", "black"), 
                        density=c(0,20))
```

##### Questão 2

Qual é a probabilidade dos valores de PAS de jovens sadios estarem entre 100 e 140 mmHg?

R.: $P(100 < X < 140) = 0.9545$ 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
print(p <- pnorm(q=140, mean=120, sd=10)-pnorm(q=100, mean=120, sd=10))
DescTools::PlotProbDist(breaks=c(80,100,140,160), 
                        function(x) dnorm(x, mean=120, sd=10), 
                        blab=c(100,140), 
                        alab=c("",round(p,4),""),
                        xlim=c(80,160),
                        main="normal(120,10)",
                        col=c("black", "black"), 
                        density=c(0,20,0))
```

##### Questão 3

Quais são os limites de um intervalo simétrico em relação à média que engloba 95% dos valores de PAS de jovens sadios?

R.: $P(100.4 < X < 139.6) = 0.95$ 

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=TRUE}
qnorm(p=0.025, mean=120, sd=10)
qnorm(p=0.975, mean=120, sd=10)
q1 <- round(qnorm(p=0.025, mean=120, sd=10),2)
q2 <- round(qnorm(p=0.975, mean=120, sd=10),2)
DescTools::PlotProbDist(breaks=c(80,q1,q2,160), 
                        function(x) dnorm(x, mean=120, sd=10), 
                        blab=c(q1,q2), 
                        alab=c("",0.95,""),
                        xlim=c(80,160),
                        main="normal(120,10)",
                        col=c("black", "black"), 
                        density=c(0,20,0))
```

### Aproximação da binomial pela normal

* $X \sim binomial(n, p)$
* Se 
  * $np > 5$ e 
  * $n(1-p) > 5$ e 
  * ${\frac{1}{\sqrt{n}} \left|\sqrt{\frac{1-p}{p}}-\sqrt{\frac{p}{1-p}}\right|} < 0.3$, 
* Então a $binomial(n, p)$ pode ser aproximada pela 
$normal\left(np, \sqrt{np(1-p)}\right)$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
n <- 6
p <- 0.7
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n*p > 5 && n*(1-p) > 5 && abs((1/sqrt(n))*(sqrt((1-p)/p)-sqrt(p/(1-p)))) < 0.3)
{cat("\nBinomial(",n,", ",p,") semelhante a Normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("\nBinomial(",n,", ",p,") diferente da Normal(",m,", ",round(dp,2),")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probbinom,probnorm)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,")\n\n"),
     xlab="X", ylab="densidade & probabilidade",
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))))
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
n <- 6
p <- 0.7
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n*p > 5 && n*(1-p) > 5 && abs((1/sqrt(n))*(sqrt((1-p)/p)-sqrt(p/(1-p)))) < 0.3)
{cat("\nBinomial(",n,", ",p,") semelhante a Normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("\nBinomial(",n,", ",p,") diferente da Normal(",m,", ",round(dp,2),")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probbinom,probnorm)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,")\n\n"),
     xlab="X", ylab="densidade & probabilidade",
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))))
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
n <- 30
p <- 0.7
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n*p > 5 && n*(1-p) > 5 && abs((1/sqrt(n))*(sqrt((1-p)/p)-sqrt(p/(1-p)))) < 0.3)
{cat("\nBinomial(",n,", ",p,") semelhante a Normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("\nBinomial(",n,", ",p,") diferente da Normal(",m,", ",round(dp,2),")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probbinom,probnorm)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,")\n\n"),
     xlab="X", ylab="densidade & probabilidade",
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))))
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
n <- 30
p <- 0.7
cat("X ~ binomial(",n,", ",p,")\n", sep="")
m <- n*p
dp <- sqrt(n*p*(1-p))
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n", sep="")
X <- 0:n
if (n*p > 5 && n*(1-p) > 5 && abs((1/sqrt(n))*(sqrt((1-p)/p)-sqrt(p/(1-p)))) < 0.3)
{cat("\nBinomial(",n,", ",p,") semelhante a Normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("\nBinomial(",n,", ",p,") diferente da Normal(",m,", ",round(dp,2),")\n\n", sep="")}
probbinom <- dbinom(x=X,size=n,prob=p)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probbinom,probnorm)),6))
plot(X,probbinom,type="h",
     main=paste0("Distribuicao binomial(",n,", ",p,")\n\n"),
     xlab="X", ylab="densidade & probabilidade",
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))),
     lwd=1,col="black")
points(X,probbinom,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,n+1), ylim=c(0,1.1*max(c(max(probbinom),max(probnorm)))))
```

### Aproximação da Poisson pela normal

$X \sim Poisson(\lambda)$

Se $\lambda > 10$, então a Poisson pode ser aproximada pela 
$normal\left(\lambda, \sqrt{\lambda}\right)$.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
lambda <- 5
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+3*dp)
cat("X ~ Poisson(lambda = ", lambda,")\n", sep="")
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n\n", sep="")
if (lambda > 10)
{cat("Poisson(",lambda,") semelhante a normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("Poisson(",lambda,") diferente da normal(",m,", ",round(dp,2),")\n\n", sep="")}
probPois <- dpois(x=X,lambda=lambda)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probPois,probnorm)),6))
plot(X,probPois,type="h",
     xlim=c(-1,m+3*dp),ylim=c(0,1.1*max(probPois)), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao de Poisson(",lambda,")\n\n"),
     lwd=1,col="black")
points(X,probPois,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,m+3*dp), ylim=c(0,1.1*max(probPois)))
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
lambda <- 5
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+3*dp)
cat("X ~ Poisson(lambda = ", lambda,")\n", sep="")
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n\n", sep="")
if (lambda > 10)
{cat("Poisson(",lambda,") semelhante a normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("Poisson(",lambda,") diferente da normal(",m,", ",round(dp,2),")\n\n", sep="")}
probPois <- dpois(x=X,lambda=lambda)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probPois,probnorm)),6))
plot(X,probPois,type="h",
     xlim=c(-1,m+3*dp),ylim=c(0,1.1*max(probPois)), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao de Poisson(",lambda,")\n\n"),
     lwd=1,col="black")
points(X,probPois,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,m+3*dp), ylim=c(0,1.1*max(probPois)))
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE}
lambda <- 50
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+3*dp)
cat("X ~ Poisson(lambda = ", lambda,")\n", sep="")
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n\n", sep="")
if (lambda > 10)
{cat("Poisson(",lambda,") semelhante a normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("Poisson(",lambda,") diferente da normal(",m,", ",round(dp,2),")\n\n", sep="")}
probPois <- dpois(x=X,lambda=lambda)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probPois,probnorm)),6))
plot(X,probPois,type="h",
     xlim=c(-1,m+3*dp),ylim=c(0,1.1*max(probPois)), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao de Poisson(",lambda,")\n\n"),
     lwd=1,col="black")
points(X,probPois,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,m+3*dp), ylim=c(0,1.1*max(probPois)))
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r eval=FALSE, echo=TRUE, warning=FALSE}
lambda <- 50
m <- lambda
dp <- sqrt(lambda)
X <- 0:(m+3*dp)
cat("X ~ Poisson(lambda = ", lambda,")\n", sep="")
cat("Media de X = ",m,"\n", sep="")
cat("Desvio-padrao de X = ",round(dp,2),"\n\n", sep="")
if (lambda > 10)
{cat("Poisson(",lambda,") semelhante a normal(",m,", ",round(dp,2),")\n\n", sep="")} else
{cat("Poisson(",lambda,") diferente da normal(",m,", ",round(dp,2),")\n\n", sep="")}
probPois <- dpois(x=X,lambda=lambda)
probnorm <- dnorm(x=X,mean=m,sd=dp)
print(round((tbl <- data.frame(X,probPois,probnorm)),6))
plot(X,probPois,type="h",
     xlim=c(-1,m+3*dp),ylim=c(0,1.1*max(probPois)), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao de Poisson(",lambda,")\n\n"),
     lwd=1,col="black")
points(X,probPois,pch=16,cex=1,col="black")
par(new=TRUE)
rn <- rnorm(n=1e6,mean=m,sd=dp)
plot(density(rn), 
     xlab="X", ylab="densidade & probabilidade",
     main=paste0("Distribuicao normal(",m,", ",round(dp,2),")"),
     xlim=c(-1,m+3*dp), ylim=c(0,1.1*max(probPois)))
```

# Padronização

* [Standard score: Wikipedia](https://en.wikipedia.org/wiki/Standard_score){target="_blank"}

A padronização das observações de uma variável aleatória é um processo estatístico que transforma a variável original em uma nova variável com média zero e desvio-padrão igual a um. Essa técnica é frequentemente usada em estatística e análise de dados para tornar as variáveis comparáveis e facilitar a interpretação dos resultados.

A padronização é realizada subtraindo-se a média da variável original e dividindo-a pelo desvio-padrão. Matematicamente, a fórmula para a padronização de uma observação da variável aleatória $X$ é dada por:

$$z_i=\dfrac{x_i-\bar{x}}{s}$$

Sendo que:

* $i=1,2,\ldots,n$
* $z_i$: nova observação padronizada
* $x_i$: observação da variável original
* $\bar{x}$: média amostral da variável original
* $s$: desvio-padrão amostral da variável original

Ao padronizar uma variável aleatória, ela passa a ter uma distribuição com média zero e desvio-padrão igual a um. 

A padronização também permite interpretar os valores da nova variável em termos de desvios-padrão em relação à média. Por exemplo, um valor padronizado de 1 significa que ele está um desvio-padrão acima da média, enquanto um valor padronizado de -2 indica que ele está dois desvios-padrão abaixo da média. Isso facilita a comparação e a compreensão dos dados.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r}
# Dados de exemplo
dados <- c(10, 15, 8, 12, 20, 5)

# Função para padronização de variável
padronizar <- function(x) {
  media <- mean(x)
  desvio_padrao <- sd(x)
  resultado <- (x - media) / desvio_padrao
  return(resultado)
}

# Chamada da função de padronização
dados_padronizados <- padronizar(dados)

# Imprimindo os resultados
print("Dados originais:")
print(dados)
print("Dados padronizados:")
print(dados_padronizados)

scale(dados)
```

# Momento estatístico central de variável aleatória

O momento estatístico em torno da esperança ou apenas momento central de ordem $k$ da variável aleatória $X$ é:

$$\begin{align}
\mu_k&=\mathbb{E}\left((X-\mu)^k\right)=\int_{-\infty}^{\infty}{(x-\mu)^kdF(x)} \\
\mu&=\mathbb{E}(X)=\int_{-\infty}^{\infty}{xdF(x)}\\
k&=1,2,\ldots
\end{align}$$

Portanto:

$$\begin{align}
\mu_1&=\mathbb{E}(X-\mu)=0 \\
\mu_2&=\mathbb{E}\left((X-\mu)^2\right)=\mathbb{E}(X^2)-\mu^2=\sigma^2\\
\mu_3&=\mathbb{E}\left((X-\mu)^3\right)=\mathbb{E}(X^3)-3\mu\mathbb{E}(X^2)+2\mu^3\\
\mu_4&=\mathbb{E}\left((X-\mu)^4\right)=\mathbb{E}(X^4)-4\mu\mathbb{E}(X^3)+6\mu^2\mathbb{E}(X^2)-3\mu^4\\
\end{align}$$

__Desvio-padrão__

$$\sigma=\sqrt{\mathbb{E}(X-\mu)^2}$$

__Índice de segurança (_safety index_)__ (Beck & Melchers, 2018, p. 17)

$$\beta_0=\dfrac{\mu}{\sigma}$$

__Coeficiente de variação de Pearson__

$$\kappa=\dfrac{1}{\beta_0}=\dfrac{\sigma}{\mu}$$

Os terceiro e quarto momentos centrados, respectivamente índices de assimetria e curtose, da variável aleatória $X$ são denotados por:

$$\begin{align}
\beta_1&=\dfrac{\mu_3}{\sigma^3}=\dfrac{\mu_3}{\left(\sigma^2\right)^{3/2}}\\
\beta_2&=\dfrac{\mu_4}{\sigma^4}=\dfrac{\mu_4}{\left(\sigma^2\right)^{2}}\\
\end{align}$$

Se $\beta_1=0$, a distribuição é simétrica. Se $\beta_1>0$, a distribuição é positivamente assimétrica. Se $\beta_1<0$, a distribuição é negativamente assimétrica.

Se a distribuição é simétrica, a curtose pode ser analisada. Para distribuição simétrica, se $\beta_2=3$, a distribuição é mesocúrtica (normal); se $\beta_2>3$, a distribuição é leptocúrtica; se $\beta_2<3$, a distribuição é platicúrtica. 

# Por que esperança e variância de variável aleatória?

Seja $Y = f(X_1, X_2, \ldots, X_k)$, $k\in \mathbb{N}^*$, uma função de variáveis aleatórias. Portanto, $Y$ também é uma variável aleatória. 

Se $k=1$, $Y$ tem esperança e variância, exceto a distribuição de Cauchy (t de Student com um grau de liberdade) que não tem esperança e variância e t de Student com dois graus de liberdade que tem esperança, mas não tem variância.

Se $k>1$ e as variáveis da função são independentes, então é possível, em geral, determinar esperança e variância exatas por meio das transformadas de Mellin e de Laplace mesmo desconhecendo a distribuição de $Y$, mas conhecendo as distribuições de $X_1, X_2, \ldots, X_k$. 

A esperança e variância aproximadas podem ser obtidas por expansão de Taylor.

Se $k>1$ e as variáveis da função são dependentes, então não é possível determinar esperança e variância exatas por meio das transformadas de Mellin e de Laplace. Para algumas funções específicas de variáveis aleatórias dependentes é possível a determinação exata da esperança e da variância. Se as variáveis aleatórias são dependentes e as correlações são conhecidas, a esperança e variância aproximadas podem ser obtidas por expansão de Taylor.

A obtenção da esperança e variância de função de variáveis aleatórias é importante para a determinação do [coeficiente de variação de Pearson](https://pt.wikipedia.org/wiki/Coeficiente_de_varia%C3%A7%C3%A3o) (desvio-padrão/esperança) e para o intervalo de predição por meio da [desigualdade de Chebychev](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality), [desigualdade de Selberg](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality) e desigualdade de Gauss-Camp-Meidell.

## Desigualdade de Chebychev

A desigualdade de Chebychev fornece um limite para a probabilidade de uma variável aleatória \( X \) se desviar da sua média \( \mu \) por mais de \( k \) vezes o desvio-padrão \( \sigma \). Isso é útil porque a desigualdade é válida para qualquer distribuição de probabilidade com variância finita, independentemente da forma dessa distribuição.

__Forma da Desigualdade__

Para uma variável aleatória \( X \) com média \( \mu \) e variância \( \sigma^2 \), a desigualdade de Chebychev pode ser expressa da seguinte forma:

\[ P(|X - \mu| \geq k\sigma) \leq \dfrac{1}{k^2} \]

Podemos reescrever essa expressão para evitar o uso do módulo, indicando um intervalo de predição simétrico em torno da média:

\[ P(\mu - k\sigma \leq X \leq \mu + k\sigma) \geq 1 - \dfrac{1}{k^2} \]

__Interpretação__

Esta forma indica que a probabilidade de \( X \) estar dentro do intervalo \(\mu \pm k\sigma\) é, no mínimo, \(1 - \frac{1}{k^2}\). Em outras palavras, pelo menos \(1 - \frac{1}{k^2}\) da distribuição da variável \( X \) está contida nesse intervalo.

__Exemplos__

Para valores específicos de \( k \):

- **Para \( k = 2 \)**:
  \[ P(\mu - 2\sigma \leq X \leq \mu + 2\sigma) \geq 1 - \frac{1}{4} = 0.75 \]
  Ou seja, pelo menos 75% dos valores de \( X \) estão dentro de \( 2 \) desvios-padrão da média.

- **Para \( k = 3 \)**:
  \[ P(\mu - 3\sigma \leq X \leq \mu + 3\sigma) \geq 1 - \frac{1}{9} \approx 0.89 \]
  Pelo menos 89% dos valores de \( X \) estão dentro de \( 3 \) desvios-padrão da média.

__Utilidade__

A desigualdade de Chebychev é particularmente útil porque:

1. **Independência da Distribuição**: Ela não assume que a distribuição de \( X \) segue qualquer forma específica, como a normal. Isso a torna aplicável em uma ampla gama de situações.
2. **Estimativa Conservadora**: Fornece uma estimativa conservadora que pode ser utilizada mesmo quando pouco se sabe sobre a forma da distribuição.
3. **Aplicação Geral**: Pode ser aplicada em diversas áreas, como controle de qualidade, análise de risco e qualquer situação onde a variância é conhecida e se deseja estimar a concentração dos valores em torno da média.

| \( k \)        | Mínima porcentagem dentro de \( k \) desvios-padrão da média |
|:----------------|:--------------------------------------------------|
| 1              | 0%                                               |
| \( \sqrt{2} \) | 50%                                              |
| 1.5            | 55.56%                                           |
| 2              | 75%                                              |
| \( 2\sqrt{2} \)| 87.5%                                            |
| 3              | 88.89%                                           |
| 4              | 93.75%                                           |
| 5              | 96%                                              |
| 6              | 97.22%                                           |
| 7              | 97.96%                                           |
| 8              | 98.44%                                           |
| 9              | 98.77%                                           |
| 10             | 99%                                              |

## Desigualdade de Selberg

A desigualdade de Selberg é uma generalização da desigualdade de Chebyshev para intervalos arbitrários. Suponha que \( X \) seja uma variável aleatória com média \( \mu \) e variância \( \sigma^2 \). A desigualdade de Selberg estabelece que, se $ \beta \geq \alpha \geq 0 $, então:

$$ \Pr(X \in [\mu - \alpha, \mu + \beta]) \geq \begin{cases} 
\dfrac{\alpha^2}{\alpha^2 + \sigma^2} & \text{se } \alpha(\beta - \alpha) \geq 2\sigma^2 \\
\dfrac{4\alpha\beta - 4\sigma^2}{(\alpha + \beta)^2} & \text{se } 2\alpha\beta \geq 2\sigma^2 \geq \alpha(\beta - \alpha) \\
0 & \text{se } \sigma^2 \geq \alpha\beta 
\end{cases} $$

Quando \( \alpha = \beta \), esta desigualdade se reduz à desigualdade de Chebyshev. Esses limites são conhecidos por serem os melhores possíveis.

A desigualdade de Selberg fornece uma maneira de estimar a probabilidade de uma variável aleatória \( X \) cair dentro de um intervalo específico e _assimétrico_ em torno de sua média, levando em consideração a variância da variável. Vamos aplicar essa desigualdade em um exemplo prático em biologia.

### Exemplo Prático em Biologia

Suponha que estamos estudando o comprimento das asas de uma espécie de pássaro. Vamos dizer que o comprimento das asas, \( X \), é uma variável aleatória com média \( \mu = 15 \) cm e variância \( \sigma^2 = 4 \) cm².

Queremos saber a probabilidade de que o comprimento das asas esteja dentro do intervalo [13 cm, 18 cm]. Aqui, temos:

- \(\mu = 15\)
- \(\alpha = \mu - 13 = 15 - 13 = 2\)
- \(\beta = 18 - \mu = 18 - 15 = 3\)
- \(\sigma^2 = 4\)

Vamos verificar as condições para aplicar a desigualdade de Selberg:

1. \(\alpha(\beta - \alpha) = 2(3 - 2) = 2\)
2. \(2\sigma^2 = 2 \times 4 = 8\)
3. \(2\alpha\beta = 2 \times 2 \times 3 = 12\)
4. \(\alpha\beta = 2 \times 3 = 6\)

Agora, aplicamos a desigualdade de Selberg:

1. \(\alpha(\beta - \alpha) = 2\) e \(2\sigma^2 = 8\). Portanto, a condição \( \alpha(\beta - \alpha) \geq 2\sigma^2 \) não é satisfeita.
2. \(2\alpha\beta = 12 \geq 8 \geq 2\), e a condição \(2\alpha\beta \geq 2\sigma^2 \geq \alpha(\beta - \alpha)\) é satisfeita.

Portanto, usamos a segunda fórmula da desigualdade de Selberg:

\[ \Pr(X \in [13, 18]) \geq \dfrac{4\alpha\beta - 4\sigma^2}{(\alpha + \beta)^2} \]

Substituindo os valores:

\[ \Pr(X \in [13, 18]) \geq \dfrac{4 \times 2 \times 3 - 4 \times 4}{(2 + 3)^2} \]
\[ \Pr(X \in [13, 18]) \geq \dfrac{24 - 16}{25} \]
\[ \Pr(X \in [13, 18]) \geq \dfrac{8}{25} \]
\[ \Pr(X \in [13, 18]) \geq 0.32 \]

Assim, a probabilidade de que o comprimento das asas dos pássaros esteja dentro do intervalo de 13 cm a 18 cm é pelo menos 32%.

### Interpretação

Essa aplicação da desigualdade de Selberg em biologia nos permite estimar a probabilidade de uma característica biológica (como o comprimento das asas de pássaros) cair dentro de um intervalo específico, considerando a variância da medida. 

## Desigualdade de Gauss-Camp-Meidell

A desigualdade é uma variação da desigualdade de Chebyshev que pode ser usada para distribuição unimodal assimétrica de variável intervalar.

### Desigualdade de Gauss-Camp-Meidell

Para uma distribuição unimodal assimétrica, a desigualdade de Gauss-Camp-Meidell estabelece que um intervalo simétrico centrado na média, com semi-amplitude $k\sigma$, sendo que $k$ é dado por:

$$k=\gamma + \dfrac{2}{3} \sqrt{\frac{1 - \gamma^2}{1 - \pi}} $$

contém pelo menos \( \pi \)% dos valores de uma variável intervalar unimodal assimétrica. 

A fórmula do intervalo de predição é:

$$ IP_\pi(X) = \left[ \mu \pm k\sigma \right] $$

onde:

- \( \sigma \): desvio-padrão
- \( \mu \): média
- $\beta_1=\dfrac{\mu_3}{\sigma^{3/2}}$: 
indice de assimetria
- $\beta_2=\dfrac{\mu_4}{\sigma^{4}}$: índice de curtose
- $\zeta=\mu-\dfrac{\sigma}{2}\dfrac{\beta_1\left(\beta_2+3\right)}{5\beta_2-6\beta_1^2-9}$: moda (Park & Sharp-Bete, 1990, p. 421)
- $\gamma = \dfrac{|\mu-\zeta|}{\sigma}$: coeficiente de assimetria
- \( \pi \) é a porcentagem mínima de valores contidos no intervalo.

#### Exemplo

Suponhamos que estamos estudando a massa corporal total de uma espécie de peixe em um lago.

Dados do Exemplo:

- Média (\(\mu\)): 26 gramas
- Variância (\(\sigma^2\)): 88 gramas²
- Terceiro momento central (\(\mu_3\)): 131 gramas³
- Quarto momento central (\(\mu_4\)): 15520 gramas⁴

Cálculos Iniciais:

O desvio-padrão (\(\sigma\)) é a raiz quadrada da variância:
\[ \sigma = \sqrt{88} \approx 9.38 \]

Índices de Assimetria e Curtose:

Índice de Assimetria (\(\beta_1\)):

\[ \beta_1 = \dfrac{\mu_3}{\sigma^3} = \dfrac{131}{9.38^3} \]
Calculando \(9.38^3\):

\[ 9.38^3 \approx 9.38 \times 9.38 \times 9.38 \approx 825.68 \]

Portanto,
\[ \beta_1 = \dfrac{131}{825.68} \approx 0.159 \]

Índice de Curtose (\(\beta_2\)):

\[ \beta_2 = \dfrac{\mu_4}{\sigma^4} = \dfrac{15520}{9.38^4} \]

Calculando \(9.38^4\):

\[ 9.38^4 \approx 9.38 \times 9.38 \times 9.38 \times 9.38 \approx 7745.76 \]

Portanto,
\[ \beta_2 = \dfrac{15520}{7745.76} \approx 2.00 \]

Cálculo da Moda (\(\zeta\)):

\[ \zeta = \mu - \dfrac{\sigma}{2} \dfrac{\beta_1 (\beta_2 + 3)}{5 \beta_2 - 6 \beta_1^2 - 9} \]

Substituindo os valores:

\[ \zeta = 26 - \dfrac{9.38}{2} \dfrac{0.159 (2.00 + 3)}{5 \times 2.00 - 6 \times 0.159^2 - 9} \]

Calculamos o numerador:

\[ \dfrac{9.38}{2} \times 0.159 \times 5 \approx 4.69 \times 0.795 = 3.73 \]

Agora, o denominador:

\[ 5 \times 2.00 - 6 \times 0.159^2 - 9 = 10 - 6 \times 0.0253 - 9 = 10 - 0.1518 - 9 = 0.8482 \]

Então, a moda é:

\[ \zeta = 26 - \dfrac{3.73}{0.8482} \approx 26 - 4.40 \approx 21.60 \]

Portanto, a moda (\(\zeta\)) é aproximadamente 21.60.

Cálculo do Coeficiente de Assimetria (\(\gamma\)):

\[ \gamma = \dfrac{|\mu - \zeta|}{\sigma} = \dfrac{|26 - 21.60|}{9.38} = \dfrac{4.40}{9.38} \approx 0.469 \]

Cálculo de \( k \) para \( \pi = 0.75 \):

\[ k = \gamma + \dfrac{2}{3} \sqrt{\dfrac{1 - \gamma^2}{1 - \pi}} \]

Substituindo os valores:

\[ k = 0.469 + \dfrac{2}{3} \sqrt{\dfrac{1 - 0.469^2}{1 - 0.75}} \]

Primeiro, calculamos a parte interna da raiz:

\[ \dfrac{1 - 0.469^2}{1 - 0.75} = \dfrac{1 - 0.220}{0.25} = \dfrac{0.780}{0.25} = 3.12 \]

Então:

\[ \sqrt{3.12} \approx 1.77 \]

Agora substituímos de volta na fórmula:

\[ k = 0.469 + \dfrac{2}{3} \times 1.77 \approx 0.469 + 1.18 \approx 1.65 \]

Intervalo de Predição (IP) para 75%:

\[ IP_{75}(X) = \left[ \mu \pm k\sigma \right] = \left[ 26 \pm 1.65 \times 9.38 \right] \]

Portanto, o intervalo é:

\[ \left[ 26 - 15.47, 26 + 15.47 \right] \]

\[ \left[ 10.53, 41.47 \right] \]

Comparação com a Desigualdade de Chebyshev:

Para a desigualdade de Chebyshev, com \( p = 75\% \) (\( p = 0.75 \)):

\[ 1 - \dfrac{1}{k^2} = 0.75 \]
\[ \dfrac{1}{k^2} = 0.25 \]
\[ k^2 = 4 \]
\[ k = 2 \]

O intervalo de Chebyshev é:

\[ IP_{75}(X) = \left[ \mu \pm 2\sigma \right] = \left[ 26 \pm 2 \times 9.38 \right] \]

Portanto, o intervalo é:

\[ \left[ 26 - 18.76, 26 + 18.76 \right] \]
\[ \left[ 7.24, 44.76 \right] \]

Conclusão:

* Intervalo de Gauss-Camp-Meidell para 75% dos valores: [10.53, 41.47]

* Intervalo de Chebyshev para 75% dos valores: [7.24, 44.76]

O intervalo de Gauss-Camp-Meidell é mais estreito do que o intervalo de Chebyshev, refletindo uma estimativa mais precisa para distribuições unimodais assimétricas. Este exemplo biológico demonstra como a desigualdade de Gauss-Camp-Meidell pode fornecer intervalos de predição mais precisos para dados de peso de peixes, considerando a assimetria da distribuição.

# Aproximação da média e variância por distribuição de PERT

Uma variável intervalar $X\in[a,b]$ tem distribuição unimodal.

Os parâmetros $a$ e $b$ podem ser interpretados como os estimativas pessimista e otimista, respectivamente.

O paramêtro $m$ é a estimativa mais provável.

Supondo que distribuição unimodal com domínio limitado tem desvio-padrão igual a 1/6 de sua amplitude, então, conforme Park & Sharp-Bete (1990, p. 423):

$$\begin{align}
\mu&\approx\dfrac{b+4m+a}{6}\\
\sigma^2&\approx\left(\dfrac{b-a}{6}\right)^2
\end{align}$$

# Momentos de combinação linear

* [Expected value: Wikipedia](https://en.wikipedia.org/wiki/Expected_value){target="_blank"}
* [Variance: Wikipedia](https://en.wikipedia.org/wiki/Variance){target="_blank"}
* [Covariance: Wikipedia](https://en.wikipedia.org/wiki/Covariance){target="_blank"}
* [Skewness: Wikipedia](https://en.wikipedia.org/wiki/Skewness){target="_blank"}
* [Kurtosis: Wikipedia](https://en.wikipedia.org/wiki/Kurtosis){target="_blank"}

Para calcular a esperança (ou valor esperado) de uma combinação linear de variáveis aleatórias dependentes, usamos a propriedade de linearidade da esperança matemática. Suponha que temos uma combinação linear de duas variáveis aleatórias \(X_1\) e \(X_2\):

\[
Y = a_1X_1 + a_2X_2
\]

onde \(a_1\) e \(a_2\) são constantes.

A esperança de \(Y\) é dada por:

\[
\mathbb{E}[Y] = \mathbb{E}[a_1X_1 + a_2X_2]
\]

Usando a propriedade de linearidade da esperança, temos:

\[
\mathbb{E}[Y] = a_1\mathbb{E}[X_1] + a_2\mathbb{E}[X_2]
\]

Em termos de \(\mu\), onde \(\mu_{X_1} = \mathbb{E}[X_1]\) e \(\mu_{X_2} = \mathbb{E}[X_2]\), a expressão fica:

\[
\mu_Y = a_1\mu_{X_1} + a_2\mu_{X_2}
\]

Portanto, a esperança de uma combinação linear de variáveis aleatórias é a combinação linear das esperanças dessas variáveis, independentemente de serem dependentes ou independentes.

Uma medida de associação (dependência) linear entre duas variáveis aleatórias é a covariância:

$$\begin{align}
\sigma_{12}&=\mathbb{C}\left(X_1,X_2\right)\in \mathbb{R}\\
&=\mathbb{E}\left(\left(X_1-\mathbb{E}\left(X_1\right)\right)\left(X_2-\mathbb{E}\left(X_2\right)\right)\right)\\
\sigma_{12}&=\mathbb{E}\left(X_1X_2\right)-\mathbb{E}\left(X_1\right)\mathbb{E}\left(X_2\right)\\
\sigma_{12}&=\mathbb{E}\left(X_1X_2\right)-\mu_1\mu_2\\
\end{align}$$

Note que a unidade de medida da covariância é o produto das unidades de medidas das duas variáveis aleatórias.

1. **Definição**:
   \[
   \mathbb{C}(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]
   \]
   Onde \(\mathbb{E}\) denota a esperança matemática.

2. **Comutatividade**:
   \[
   \mathbb{C}(X, Y) = \mathbb{C}(Y, X)
   \]

3. **Linearidade**:
   Se \(a\) e \(b\) são constantes, então:
   \[
   \mathbb{C}(aX + b, Y) = a \mathbb{C}(X, Y)
   \]
   \[
   \mathbb{C}(X, aY + b) = a \mathbb{C}(X, Y)
   \]

4. **Soma de variáveis**:
   \[
   \mathbb{C}(X + Z, Y) = \mathbb{C}(X, Y) + \mathbb{C}(Z, Y)
   \]
   \[
   \mathbb{C}(X, Y + W) = \mathbb{C}(X, Y) + \mathbb{C}(X, W)
   \]

5. **Independência**:
   Se \(X\) e \(Y\) são independentes, então:
   \[
   \mathbb{C}(X, Y) = 0
   \]
   No entanto, a covariância nula não implica independência, exceto no caso de variáveis aleatórias normais.

6. **Relação com a variância**:
   $$
   \mathbb{V}(X) = \sigma_X^2=\mathbb{C}(X, X)
   $$
   
7. **Relação de duas combinações lineares**:  

Para encontrar a covariância entre duas combinações lineares de variáveis aleatórias, considere duas combinações lineares \( Y_1 \) e \( Y_2 \) definidas como:

\[
Y_1 = a_1X_1 + a_2X_2 + \cdots + a_nX_n
\]
\[
Y_2 = b_1X_1 + b_2X_2 + \cdots + b_nX_n
\]

Onde \( a_i \) e \( b_i \) são constantes e \( X_i \) são variáveis aleatórias.

A covariância entre \( Y_1 \) e \( Y_2 \) é dada por:

\[
\mathbb{C}(Y_1, Y_2) = \mathbb{C}\left(\sum_{i=1}^{n} a_i X_i, \sum_{j=1}^{n} b_j X_j \right)
\]

Usando a linearidade da covariância, isso se expande para:

\[
\mathbb{C}(Y_1, Y_2) = \sum_{i=1}^{n} \sum_{j=1}^{n} a_i b_j \mathbb{C}(X_i, X_j)
\]

Isso significa que a covariância entre duas combinações lineares é a soma ponderada das covariâncias entre as variáveis individuais, onde os pesos são os produtos das constantes associadas às variáveis em cada combinação linear.

__Exemplo__

Considere as combinações lineares \( Y_1 = a_1X_1 + a_2X_2 \) e \( Y_2 = b_1X_1 + b_2X_2 \).

A covariância entre \( Y_1 \) e \( Y_2 \) é:

\[
\mathbb{C}(Y_1, Y_2) = \mathbb{C}(a_1X_1 + a_2X_2, b_1X_1 + b_2X_2)
\]

Aplicando a linearidade:

\[
\mathbb{C}(Y_1, Y_2) = a_1b_1\mathbb{C}(X_1, X_1) + a_1b_2\mathbb{C}(X_1, X_2) + a_2b_1\mathbb{C}(X_2, X_1) + a_2b_2\mathbb{C}(X_2, X_2)
\]

Como \(\mathbb{C}(X_i, X_j) = \mathbb{C}(X_j, X_i)\):

\[
\mathbb{C}(Y_1, Y_2) = a_1b_1\mathbb{V}(X_1) + a_1b_2\mathbb{C}(X_1, X_2) + a_2b_1\mathbb{C}(X_2, X_1) + a_2b_2\mathbb{V}(X_2)
\]

Portanto:

\[
\mathbb{C}(Y_1, Y_2) = a_1b_1\mathbb{V}(X_1) + (a_1b_2 + a_2b_1)\mathbb{C}(X_1, X_2) + a_2b_2\mathbb{V}(X_2)
\]

A correlação de Pearson é:

   $$\begin{align}
   \rho &= \dfrac{\mathbb{C}(X_1, X_2)}{\sqrt{\mathbb{V}(X_1) \mathbb{V}(X_2)}}\\
   \rho &= \dfrac{\sigma_{12}}{\sigma_1\sigma_2}
   \end{align}$$
   
**Amplitude**:
   \[
   -1 \leq \rho \leq 1
   \]
   A correlação de Pearson varia entre -1 e 1. Um valor de 1 indica uma relação linear positiva perfeita, -1 indica uma relação linear negativa perfeita e 0 indica nenhuma relação linear.

**Sinal**:
   O sinal de \(\rho\) indica a direção da relação. Um valor positivo indica que, à medida que uma variável aumenta, a outra também tende a aumentar. Um valor negativo indica que, à medida que uma variável aumenta, a outra tende a diminuir.

**Invariância a mudanças de escala e localização**:
   A correlação de Pearson não é afetada por mudanças lineares nas variáveis. Se \(a\), \(b\), $c$ e $d$ são constantes tais que $ac>0$, então:
   \[
   \rho_{aX_1 + b, cX_2 + d} = \rho_{X_1, X_2}
   \]
Se $ac<0$, então:
   \[
   \rho_{aX_1 + b, cX_2 + d} = -\rho_{X_1, X_2}
   \]   
   
**Simetria**:
   \[
   \rho_{X_1, X_2} = \rho_{X_2, X_1}
   \]

**Coeficiente de Determinação**:
   O quadrado do coeficiente de correlação, \(\rho^2\), é conhecido como o coeficiente de determinação e representa a proporção da variância em uma das variáveis que é explicada pela variância na outra variável.

**Independência**:
   Se \(X_1\) e \(X_2\) são independentes, então:
   \[
   \rho = 0
   \]
   No entanto, \(\rho = 0\) não implica necessariamente independência.
   
**Correlação de duas combinações lineares**:
   
Fórmula da correlação \(\rho_{Y_1, Y_2}\):

1. **Covariância de \(Y_1\) e \(Y_2\)**:
   \[
   \mathbb{C}(Y_1, Y_2) = a_1a_2\sigma_{X_1}^2 + (a_1b_2 + b_1a_2)\rho \sigma_{X_1}\sigma_{X_2} + b_1b_2\sigma_{X_2}^2
   \]

2. **Variância de \(Y_1\)**:
   \[
   \sigma_{Y_1}^2 = a_1^2\sigma_{X_1}^2 + 2a_1b_1\rho \sigma_{X_1}\sigma_{X_2} + b_1^2\sigma_{X_2}^2
   \]

3. **Variância de \(Y_2\)**:
   \[
   \sigma_{Y_2}^2 = a_2^2\sigma_{X_1}^2 + 2a_2b_2\rho \sigma_{X_1}\sigma_{X_2} + b_2^2\sigma_{X_2}^2
   \]

Agora, substituímos essas expressões na fórmula da correlação:

\[
\rho_{Y_1, Y_2} = \frac{a_1a_2\sigma_{X_1}^2 + (a_1b_2 + b_1a_2)\rho \sigma_{X_1}\sigma_{X_2} + b_1b_2\sigma_{X_2}^2}{\sqrt{(a_1^2\sigma_{X_1}^2 + 2a_1b_1\rho \sigma_{X_1}\sigma_{X_2} + b_1^2\sigma_{X_2}^2)(a_2^2\sigma_{X_1}^2 + 2a_2b_2\rho \sigma_{X_1}\sigma_{X_2} + b_2^2\sigma_{X_2}^2)}}
\]

Essa fórmula fornece a correlação \(\rho_{Y_1, Y_2}\) em termos das correlações \(\rho\), desvios-padrão \(\sigma_{X_1}\) e \(\sigma_{X_2}\), e os coeficientes das combinações lineares \(a_1\), \(a_2\), \(b_1\) e \(b_2\).

A variância de \(Y\) é dada por:

\[
\mathbb{V}(Y) = \mathbb{V}(a_1X_1 + a_2X_2)
\]

Usando a propriedade da variância e covariância, temos:

\[
\mathbb{V}(Y) = a_1^2 \mathbb{V}(X_1) + a_2^2 \mathbb{V}(X_2) + 2a_1a_2 \mathbb{C}(X_1, X_2)
\]

Em termos de \(\sigma\) e \(\rho\), onde \(\sigma_{X_1}^2 = \mathbb{V}(X_1)\), \(\sigma_{X_2}^2 = \mathbb{V}(X_2)\), e \(\rho = \frac{\mathbb{C}(X_1, X_2)}{\sigma_{X_1}\sigma_{X_2}}\), a expressão pode ser escrita como:

$$
\sigma_Y^2 = a_1^2 \sigma_{1}^2 + a_2^2 \sigma_{2}^2 + 2a_1a_2 \rho \sigma_{1} \sigma_{2}
$$

Se $X_1$ e $X_2$ são independentes, então $\rho=0$. Desta forma, a variância de $Y$ é:

$$
\sigma_Y^2 = a_1^2 \sigma_{1}^2 + a_2^2 \sigma_{2}^2 
$$


Suponha que temos uma combinação linear de \(m\) variáveis aleatórias  \(X_1, X_2, \ldots, X_m\):

\[ Y = a_1X_1 + a_2X_2 + \cdots + a_mX_m \]

Para variáveis aleatórias independentes:

$$
\sigma_Y^2 = \sum_{i=1}^m a_i^2 \sigma_{i}^2
$$

Para variáveis aleatórias dependentes, a fórmula inclui as correlações entre todas as variáveis:

$$
\sigma_Y^2 = \sum_{i=1}^m a_i^2 \sigma_{i}^2 + 2 \sum_{i =1}^{m-1} \sum_{j=i+1}^m a_i a_j \rho_{ij}\sigma_{i}\sigma_{j}
$$ 


Para variáveis aleatórias independentes::

$$
\sigma_Y^2 = \sum_{i=1}^m a_i^2 \sigma_{i}^2
$$ 

Conforme Hool & Maghsoodloo (1988), os terceiro e quarto momentos centrais (assimetria e curtose), respectivamente, de \(Y\) para variáveis independentes são:

$$\mu_{3,Y}= \sum_{i=1}^m a_i^3  \mu_{3i}$$

Demonstração:

1. Definimos \( \mu_{3,Y} = \mathbb{E} \left\{ \left( \sum_{i=1}^m a_i (X_i - \mu_i) \right)^3 \right\} \).

2. Expandimos o cubo da soma:

$$\mu_{3,Y} = \mathbb{E} \left\{ \sum_{i=1}^m a_i^3 (X_i - \mu_i)^3 + \\
3 \sum_{i=1}^{m-2} \sum_{j=i+1}^{m-1} \sum_{k=j+1}^m a_i^2 a_j (X_i - \mu_i)^2 (X_j - \mu_j) + \\
6 \sum_{i=1}^{m-2} \sum_{j=i+1}^{m-1} \sum_{k=j+1}^m a_i a_j a_k (X_i - \mu_i) (X_j - \mu_j) (X_k - \mu_k) \right\}$$

3. Aproveitamos a independência das \( X_i \), que faz com que os termos cruzados com três variáveis diferentes desapareçam quando tomamos a esperança:

\[ \mu_{3,Y} = \sum_{i=1}^m a_i^3 \mathbb{E}(X_i - \mu_i)^3. \]

4. Utilizando a definição de \( \mathbb{E}(X_i - \mu_i)^3 = \mu_{3,i} \), temos:
\[ \mu_{3,Y} = \sum_{i=1}^m a_i^3 \mu_{3,i}. \]

$$\Diamond$$

$$\mu_{4,Y} = \sum_{i=1}^m a_i^4 \mu_{4i} + 6 \sum_{i=1}^{m-1} \sum_{j=i+1}^{m}{a_i^2 a_j^2 \sigma_{i}^2 \sigma_{j}^2}$$

Demonstração:

Dado \(Y = \sum_{i=1}^m a_i X_i\), queremos calcular:

\[
\mu_{4,Y} = \mathbb{E}\left[\left(Y - \mathbb{E}[Y]\right)^4\right]
\]

Como \(\mathbb{E}[Y] = \sum_{i=1}^m a_i \mathbb{E}[X_i] = \sum_{i=1}^m a_i \mu_i\), podemos escrever:

\[
Y - \mathbb{E}[Y] = \sum_{i=1}^m a_i (X_i - \mu_i)
\]

Então, o quarto momento centralizado é:

\[
\mu_{4,Y} = \mathbb{E}\left[\left(\sum_{i=1}^m a_i (X_i - \mu_i)\right)^4\right]
\]

Expandindo o termo:

\[
\mu_{4,Y} = \mathbb{E}\left[\left(\sum_{i=1}^m a_i (X_i - \mu_i)\right)^4\right]
\]

Podemos expandir isso utilizando a fórmula binomial generalizada:

$$
\left(\sum_{i=1}^m a_i (X_i - \mu_i)\right)^4 = \sum_{i=1}^m a_i^4 (X_i - \mu_i)^4 + \\
4 \sum_{i=1}^m \sum_{j \neq i} a_i^3 a_j (X_i - \mu_i)^3 (X_j - \mu_j) + \\
6 \sum_{i=1}^m \sum_{j \neq i} a_i^2 a_j^2 (X_i - \mu_i)^2 (X_j - \mu_j)^2 +\\
 12 \sum_{i=1}^m \sum_{j \neq i} \sum_{k \neq i, j} a_i^2 a_j a_k (X_i - \mu_i)^2 (X_j - \mu_j)(X_k - \mu_k) + \\
 24 \sum_{i=1}^m \sum_{j \neq i} \sum_{k \neq i, j} \sum_{l \neq i, j, k} a_i a_j a_k a_l (X_i - \mu_i)(X_j - \mu_j)(X_k - \mu_k)(X_l - \mu_l)
$$

1. **Primeiro Termo**:

\[
\mathbb{E}\left[\sum_{i=1}^m a_i^4 (X_i - \mu_i)^4\right] = \sum_{i=1}^m a_i^4 \mathbb{E}[(X_i - \mu_i)^4] = \sum_{i=1}^m a_i^4 \mu_{4,i}
\]

2. **Segundo Termo**:

Para \(i \neq j\),

\[
\mathbb{E}\left[4 \sum_{i=1}^m \sum_{j \neq i} a_i^3 a_j (X_i - \mu_i)^3 (X_j - \mu_j)\right] = 4 \sum_{i=1}^m \sum_{j \neq i} a_i^3 a_j \mathbb{E}[(X_i - \mu_i)^3] \mathbb{E}[(X_j - \mu_j)]
\]

Como \(X_i\) e \(X_j\) são independentes e a esperança de \((X_j - \mu_j)\) é zero, esse termo se anula.

3. **Terceiro Termo**:

Para \(i \neq j\),

\[
\mathbb{E}\left[6 \sum_{i=1}^m \sum_{j \neq i} a_i^2 a_j^2 (X_i - \mu_i)^2 (X_j - \mu_j)^2\right] = 6 \sum_{i=1}^m \sum_{j \neq i} a_i^2 a_j^2 \mathbb{E}[(X_i - \mu_i)^2] \mathbb{E}[(X_j - \mu_j)^2]
\]

Como \(X_i\) e \(X_j\) são independentes:

\[
\mathbb{E}[(X_i - \mu_i)^2] = \sigma_{X_i}^2 \quad \text{e} \quad \mathbb{E}[(X_j - \mu_j)^2] = \sigma_{X_j}^2
\]

Então:

\[
6 \sum_{i=1}^m \sum_{j \neq i} a_i^2 a_j^2 \sigma_{X_i}^2 \sigma_{X_j}^2
\]

4. **Quarto e Quinto Termos**:

Esses termos se anulam devido à independência das variáveis e a soma dos momentos ímpares sendo zero.

Assim, a fórmula final para o quarto momento centralizado \(\mu_{4,Y}\) é:

$$
\mu_{4,Y} = \sum_{i=1}^m a_i^4 \mu_{4,i} + 6 \sum_{i=1}^{m-1} \sum_{j = i+1}^{m} a_i^2 a_j^2 \sigma_i^2 \sigma_j^2
$$

$$\Diamond$$

Propriedades da variável aleatória diferença \(Y = X_1 - X_2\), onde \(X_1\) e \(X_2\) são variáveis aleatórias dependentes:

1. Esperança (Média):

\[ \mu_Y = \mu_{X_1} - \mu_{X_2} \]

2. Variância:

\[ \sigma_Y^2 = \sigma_{X_1}^2 + \sigma_{X_2}^2 - 2 \mathbb{C}(X_1, X_2) \]

ou, em termos de correlação:

\[ \sigma_Y^2 = \sigma_{X_1}^2 + \sigma_{X_2}^2 - 2 \rho \sigma_{X_1} \sigma_{X_2} \]

# Método aproximativo de Taylor de Esperança e Variância

O texto está baseado principalmente no capítulo 4 de Rice (2007).

O artigo de Ku (1966) é uma referência importante para este tópico.

Em muitas aplicações, apenas os dois primeiros momentos de uma variável aleatória, e não toda a distribuição de probabilidade, são conhecidos, e mesmo esses podem ser conhecidos apenas aproximadamente. Veremos no Capítulo 5 que observações independentes repetidas de uma variável aleatória permitem estimativas confiáveis de sua média e variância. Suponha que conhecemos a esperança e a variância de uma variável aleatória \(X\), mas não toda a distribuição, e que estamos interessados na média e variância de \(Y = g(X)\) para alguma função fixa \(g\). Por exemplo, podemos medir \(X\) e determinar sua média e variância, mas realmente estamos interessados em \(Y\), que está relacionado a \(X\) de uma forma conhecida. Podemos querer saber \(\mathbb{V}(Y)\), pelo menos aproximadamente, para avaliar a precisão do processo de medição indireta. A partir dos resultados dados neste capítulo, não podemos, em geral, encontrar \(\mathbb{E}(Y) = \mu_Y\) e \(\mathbb{V}(Y) = \sigma_Y^2\) a partir de \(\mathbb{E}(X) = \mu_X\) e \(\mathbb{V}(X) = \sigma_X^2\), a menos que a função \(g\) seja linear. No entanto, se \(g\) for quase linear em uma faixa na qual \(X\) tem alta probabilidade, ela pode ser aproximada por uma função linear e momentos aproximados de \(Y\) podem ser encontrados.

Ao proceder como descrito, seguimos uma abordagem frequentemente adotada em matemática aplicada: quando confrontados com um problema não linear que não podemos resolver, linearizamos. Em probabilidade e estatística, esse método é chamado de propagação de erro, ou método \(\delta\). A linearização é realizada por meio de uma expansão em série de Taylor de \(g\) em torno de \(\mu_X\). De primeira ordem,

$$ Y = g(X) \approx g(\mu_X) + (X - \mu_X)g{\prime}(\mu_X) $$

Expressamos \(Y\) como aproximadamente igual a uma função linear de \(X\). Lembrando que se \(U = a + bV\), então \(\mu_U = a + b\mu_V\) e \(\sigma_U^2 = b^2\sigma_V^2\), encontramos

$$\begin{align}
\mu_Y &\approx g(\mu_X) \\
\sigma_Y^2 &\approx \sigma_X^2\left[g{\prime}(\mu_X)\right]^2
\end{align}$$

Sabemos que, em geral, \(\mu_Y = g(\mu_X)\), conforme dado pela aproximação. Na verdade, podemos realizar a expansão em série de Taylor até a segunda ordem para obter uma aproximação melhor de \(\mu_Y\):

$$Y = g(X) \approx g(\mu_X) + (X - \mu_X)g{\prime}(\mu_X) + \dfrac{1}{2}(X - \mu_X)^2g{\prime\prime}(\mu_X) $$

Tomando a esperança do lado direito, temos, já que \(\mathbb{E}(X - \mu_X) = 0\),

$$\mu_Y \approx g(\mu_X) + \dfrac{1}{2}\sigma_X^2 g{\prime\prime}(\mu_X)$$

A qualidade dessas aproximações depende de quão não linear é \(g\) na vizinhança de \(\mu_X\) e do tamanho de \(\sigma_X\). Pela desigualdade de Chebyshev, sabemos que \(X\) é improvável estar muitos desvios-padrão distante de \(\mu_X\); se \(g\) puder ser razoavelmente bem aproximada nessa faixa por uma função linear, as aproximações para os momentos também serão razoáveis.

__Tabela de [Propagation of uncertainty: Wikipedia](https://en.m.wikipedia.org/wiki/Propagation_of_uncertainty)__

| Função                           | Variância                                                                                    |
|:---------------------------------|:--------------------------------------------------------------------------------------------|
| \( f = aA \)                     | \( \sigma_f^2 = a^2 \sigma_A^2 \)                                                            |
| \( f = A + B \)                  | \( \sigma_f^2 = \sigma_A^2 + \sigma_B^2 + 2\sigma_{AB} \)                                   |
| \( f = A - B \)                  | \( \sigma_f^2 = \sigma_A^2 + \sigma_B^2 - 2\sigma_{AB} \)                                   |
| \( f = aA + bB \)                | \( \sigma_f^2 = a^2 \sigma_A^2 + b^2 \sigma_B^2 + 2ab \sigma_{AB} \)                        |
| \( f = aA - bB \)                | \( \sigma_f^2 = a^2 \sigma_A^2 + b^2 \sigma_B^2 - 2ab \sigma_{AB} \)                        |
| \( f = AB \)                     | \( \sigma_f^2 \approx f^2 \left[ \left( \dfrac{\sigma_A}{A} \right)^2 + \left( \dfrac{\sigma_B}{B} \right)^2 + 2 \dfrac{\sigma_{AB}}{AB} \right] \) |
| \( f = \dfrac{A}{B} \)           | \( \sigma_f^2 \approx f^2 \left[ \left( \dfrac{\sigma_A}{A} \right)^2 + \left( \dfrac{\sigma_B}{B} \right)^2 - 2 \dfrac{\sigma_{AB}}{AB} \right] \) |
| \( f = \dfrac{A}{A+B} \)         | \( \sigma_f^2 \approx \dfrac{f^2}{(A+B)^2} \left( \dfrac{B^2}{A^2} \sigma_A^2 + \sigma_B^2 - 2 \dfrac{B}{A} \sigma_{AB} \right) \) |
| \( f = aA^b \)                   | \( \sigma_f^2 \approx f^2 \left( \dfrac{b \sigma_A}{A} \right)^2 \)                                                               |
| \( f = a \ln(bA) \)              | \( \sigma_f^2 \approx \left( a \dfrac{\sigma_A}{A} \right)^2 \)                                                                   |
| \( f = a \log_{10}(bA) \)        | \( \sigma_f^2 \approx \left( a \dfrac{\sigma_A}{A \ln(10)} \right)^2 \)                                                           |
| \( f = ae^{bA} \)                | \( \sigma_f^2 \approx f^2 (b \sigma_A)^2 \)                                                                                       |
| \( f = a^{bA} \)                 | \( \sigma_f^2 \approx f^2 (b \ln(a) \sigma_A)^2 \)                                                                                |
| \( f = a \sin(bA) \)             | \( \sigma_f^2 \approx (ab \cos(bA) \sigma_A)^2 \)                                                                                 |
| \( f = a \cos(bA) \)             | \( \sigma_f^2 \approx (ab \sin(bA) \sigma_A)^2 \)                                                                                 |
| \( f = a \tan(bA) \)             | \( \sigma_f^2 \approx (ab \sec^2(bA) \sigma_A)^2 \)                                                                               |
| \( f = A^B \)                    | \( \sigma_f^2 \approx f^2 \left[ \left( \dfrac{B \sigma_A}{A} \right)^2 + (\ln(A) \sigma_B)^2 + 2 \dfrac{B \ln(A) \sigma_{AB}}{A} \right] \) |
| \( f = \sqrt{aA^2 \pm bB^2} \)   | \( \sigma_f^2 \approx \left( \dfrac{A}{f} \right)^2 a^2 \sigma_A^2 + \left( \dfrac{B}{f} \right)^2 b^2 \sigma_B^2 \pm 2ab \dfrac{AB \sigma_{AB}}{f^2} \) |

A teoria de propagação de erro propõe a criação de intervalo de predição da função aproximada pelos dois primeiros momentos por meio da [desigualdade de Chebychev: Wikipedia](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality).

## Exemplo A: Função inversa

A relação entre tensão, corrente e resistência é \(V = IR\). Suponha que a tensão seja mantida constante em um valor \(V_0\) através de um meio cuja resistência flutua aleatoriamente devido, por exemplo, a flutuações aleatórias em nível molecular. A corrente, portanto, também varia aleatoriamente. Suponha que possa ser determinado experimentalmente que a corrente tem média \(\mu_I = 0\) e variância \(\sigma_I^2\). Desejamos encontrar a média e a variância da resistência \(R\), e como não conhecemos a distribuição de \(I\), devemos recorrer a uma aproximação. Temos:

$$\begin{align}
R = g(I) &= \dfrac{V_0}{I} \\
g{\prime}(\mu_I) &= -\dfrac{V_0}{\mu_I^2} \\
g{\prime\prime}(\mu_I) &= 2\dfrac{V_0}{\mu_I^3} 
\end{align}$$

Assim,

$$\begin{align}
\mu_R &\approx \dfrac{V_0}{\mu_I}\left(1+\dfrac{\sigma_I^2}{\mu_I^2}\right)\\
\mu_R &\approx \dfrac{V_0}{\mu_I}\left(1+\kappa_I^2\right)\\
\\
\sigma_R^2 &\approx V_0^2\dfrac{\sigma_I^2}{\mu_I^4}  \\
\sigma_R^2 &\approx \left(\dfrac{V_0}{\mu_I}\right)^2\kappa_I^2 
\end{align}$$

Sendo que o coeficiente de variação de Pearson de $I$ é $\kappa_I=\dfrac{\sigma_I}{\mu_I}$.

Portanto, o coeficiente de variação de Pearson de $R$ é 

$$\kappa_R=\dfrac{\sigma_R}{\mu_R}=\dfrac{\dfrac{V_0}{\mu_I}\kappa_I}{\dfrac{V_0}{\mu_I}\left(1+\kappa_I^2\right)}=\dfrac{\kappa_I}{1+\kappa_I^2}$$.

Vemos que a variabilidade de \(R\) depende tanto do nível médio de \(I\) quanto da variância de \(I\). Isso faz sentido, pois se \(I\) é bastante pequeno, pequenas variações em \(I\) resultarão em grandes variações em \(R = \dfrac{V_0}{I}\), enquanto se \(I\) é grande, pequenas variações não afetarão \(R\) tanto. O fator de correção de segunda ordem para \(\mu_R\) também depende de \(\mu_I\) e é grande se \(\mu_I\) for pequeno. Na verdade, quando \(I\) está próximo de zero, a função \(g(I) = \dfrac{V_0}{I}\) é bastante não linear, e a linearização não é uma boa aproximação.

## Exemplo B: Precisão da aproximação

Este exemplo examina a precisão das aproximações usando um caso de teste simples. Escolhemos a função \( g(x) = \sqrt{x} \) e consideramos dois casos: \( X \) uniforme em \([0, 1]\) e \( X \) uniforme em \([1, 2]\).  Seja \( Y = \sqrt{X} \); como \( X \) é uniforme em \([0, 1]\),

$$ \mathbb{E}(Y) = \int_{0}^{1} \sqrt{x} \, dx = \dfrac{2}{3} $$

$$ \mathbb{E}\left(Y^2\right) = \int_{0}^{1} x \, dx = \dfrac{1}{2} $$

Portanto, \(\sigma_Y^2 = \dfrac{1}{2} - \left( \dfrac{2}{3} \right)^2 = \dfrac{1}{18}\) e \(\sigma_Y \approx 0.236\). Estes resultados são exatos.

Note que A função \( g(x) = \sqrt{x} \) é mais linear no intervalo \([1, 2]\) do que no intervalo \([0, 1]\).

```{r}
curve(sqrt(x), 0, 3)
curve(x*1, 0, 3, lty=2, add=TRUE)
abline(v=1, lty=3)
abline(v=2, lty=3)
```

Usando o método de aproximação, primeiro calculamos

$$\begin{align}
g{\prime}(x) &= \dfrac{1}{2} x^{-1/2} \\
g{\prime\prime}(x) &= -\dfrac{1}{4} x^{-3/2}
\end{align}$$

Como \( X \) é uniforme em \([0, 1]\), \(\mu_X = \dfrac{1}{2}\), e avaliando as derivadas neste valor, temos

$$\begin{align}
g{\prime}(\mu_X) &= \dfrac{\sqrt{2}}{2} \\
g{\prime\prime}(\mu_X) &= -\dfrac{\sqrt{2}}{2}
\end{align}$$

Sabemos que \(\sigma_X^2 = \dfrac{1}{12}\) para uma variável aleatória uniforme em \([0, 1]\), então as aproximações são

$$\begin{align} \mu_Y &\approx \dfrac{1}{2} - \dfrac{1}{2} \times \dfrac{\sqrt{2}}{2} \times \dfrac{1}{12} = 0.678 \\
\sigma_Y^2 &\approx \left( \dfrac{\sqrt{2}}{2} \right)^2 \times \dfrac{1}{12} = 0.042 \\
\sigma_Y &\approx 0.204
\end{align}$$

A aproximação da média é 0.678 e, comparada ao valor real de 0.667, está errada em cerca de 1.6%. A aproximação do desvio-padrão é 0.204 e, comparada ao valor real de 0.236, está errada em 13%.

Agora, consideremos o caso em que \( X \) é uniforme em \([1, 2]\). Procedendo como antes, encontramos que \( y = \sqrt{x} \) tem média 1.219. A variância e o desvio-padrão são 0.0142 e 0.119, respectivamente. Para comparar estas aproximações, notamos que \(\mu_X = \dfrac{3}{2}\) e \(\sigma^2_X = \dfrac{1}{12}\) (a variável aleatória uniforme em \([1, 2]\) pode ser obtida adicionando a constante 1 a uma variável aleatória uniforme em \([0, 1]\); compare com o Teorema A na Seção 4.2). Encontramos

$$\begin{align}
g{\prime}(\mu_X) &= 0.408 \\
g{\prime\prime}(\mu_X) &= -0.136 
\end{align}$$

Então, as aproximações são

$$\begin{align} 
\mu_Y &\approx \dfrac{3}{2} - \dfrac{1}{2} \times 0.136 \times \frac{1}{12} = 1.219 \\
\sigma^2_Y &\approx \left( 0.408 \right)^2 \times \dfrac{1}{12} = 0.0138\\
\sigma_Y &\approx 0.118 \\
\end{align}$$

Esses valores estão muito mais próximos dos valores reais do que as aproximações para o primeiro caso.

$$\Diamond$$

Suponha que temos \( Z = g(X, Y) \), uma função de duas variáveis. Podemos, novamente, realizar expansões em série de Taylor para aproximar a média e a variância de \( Z \). Na primeira ordem, deixando \(\mu\) denotar o ponto \((\mu_X, \mu_Y)\),

$$ Z = g(X, Y) \approx g(\mu) + (X - \mu_X) \dfrac{\partial g(\mu)}{\partial x} + (Y - \mu_Y) \dfrac{\partial g(\mu)}{\partial y} $$

A notação \(\dfrac{\partial g(\mu)}{\partial x}\) significa que a derivada é avaliada no ponto \(\mu\). Aqui, \( Z \) é expressa como aproximadamente igual a uma função linear de \( X \) e \( Y \), e a média e a variância dessa função linear são facilmente calculadas como

$$ \mu_Z \approx g(\mu) $$

e

$$ \sigma_Z^2 \approx \sigma_X^2 \left( \dfrac{\partial g(\mu)}{\partial x} \right)^2 + \sigma_Y^2 \left( \dfrac{\partial g(\mu)}{\partial y} \right)^2 + 2\sigma_{XY} \dfrac{\partial g(\mu)}{\partial x} \dfrac{\partial g(\mu)}{\partial y} $$

(Para este último cálculo, veja o Corolário A na Seção 4.3). Assim como no caso de uma variável, uma expansão de segunda ordem pode ser usada para obter uma estimativa melhor de \(\mu_Z\):

$$ Z = g(X, Y) \approx g(\mu) + (X - \mu_X) \dfrac{\partial g(\mu)}{\partial x} + (Y - \mu_Y) \dfrac{\partial g(\mu)}{\partial y} +\\ \dfrac{1}{2} (X - \mu_X)^2 \dfrac{\partial^2 g(\mu)}{\partial x^2} + \dfrac{1}{2} (Y - \mu_Y)^2 \dfrac{\partial^2 g(\mu)}{\partial y^2} + \\(X - \mu_X)(Y - \mu_Y) \dfrac{\partial^2 g(\mu)}{\partial x \partial y} $$

Tomando as esperanças termo a termo no lado direito, obtemos

$$ \mu_Z \approx g(\mu) + \dfrac{1}{2} \sigma_X^2 \dfrac{\partial^2 g(\mu)}{\partial x^2} + \dfrac{1}{2} \sigma_Y^2 \dfrac{\partial^2 g(\mu)}{\partial y^2} + \sigma_{XY} \dfrac{\partial^2 g(\mu)}{\partial x \partial y} $$

O caso geral de uma função de \( n \) variáveis pode ser trabalhado de maneira similar; os conceitos básicos são ilustrados pelo caso de duas variáveis.

## Exemplo C: Esperança e Variância de Razão

* [Ratio distribution: Wikipedia](https://en.wikipedia.org/wiki/Ratio_distribution)

**Distribuição Estatística de Índices Biomédicos**

"Há muito tempo é prática na medicina clínica usar valores médios de certas razões selecionadas, como o IMC para avaliar a gordura corporal, LDL/HDL (lipoproteína de baixa e alta densidade) para avaliar o risco cardiovascular, A/G (albumina e globulina) para avaliar a função hepática, BUN (nitrogênio ureico no sangue)/creatinina para avaliar a função renal, entre outros. No entanto, conforme enfatizado pelos autores em relação às inferências estatísticas [12] [11], os valores médios sozinhos podem ser pouco informativos e até mesmo enganosos. O que realmente é necessário para uma interpretação válida e aplicação prática de um índice biomédico é a sua distribuição estatística. Ao conhecer a distribuição de uma variável aleatória, um analista pode determinar com confiança quantificável todas as estatísticas populacionais (para comparação com estatísticas amostrais empíricas), tais como momentos (média, variância, assimetria, curtose, etc.), cumulantes, percentis (como mediana, quartis, etc.) e, especialmente no caso de razões biomédicas, os valores de corte que determinam os graus de saúde e risco."

> Silverman & Lipscombe, 2022, p. 326

Vamos considerar o caso onde \( Z = \dfrac{Y}{X} \), que surge frequentemente na prática. Por exemplo, um químico pode medir as concentrações de duas substâncias, ambas com algum erro de medição indicado por seus desvios-padrão, e depois relatar as concentrações relativas na forma de uma razão. Qual é o desvio-padrão aproximado da razão \( Z \)?

Usando o método de propagação de erro derivado acima, para \( g(x, y) = \dfrac{y}{x} \), temos:

$$\begin{align} 
\dfrac{\partial g}{\partial x} &= -\dfrac{y}{x^2} \\
\dfrac{\partial g}{\partial y} &= \dfrac{1}{x} \\
\dfrac{\partial^2 g}{\partial x^2} &= \dfrac{2y}{x^3} \\
\dfrac{\partial^2 g}{\partial y^2} &= 0 \\
\dfrac{\partial^2 g}{\partial x \partial y} &= -\dfrac{1}{x^2} 
\end{align}$$ 

Avaliando essas derivadas em \((\mu_X, \mu_Y)\) e usando o resultado precedente, encontramos, se \(\mu_X \neq 0\),

$$\begin{align}
\mu_Z &\approx \dfrac{\mu_Y}{\mu_X} + \sigma_X^2\dfrac{ \mu_Y}{\mu_X^3} - \dfrac{\sigma_{XY}}{\mu_X^2} \\
\mu_Z &\approx \dfrac{\mu_Y}{\mu_X}\left(1+\kappa_X^2-\rho\kappa_Y\kappa_X\right)
\end{align}$$

Sendo que os coeficientes de variação de Pearson são $\kappa_X=\dfrac{\sigma_X}{\mu_X}$ e $\kappa_Y=\dfrac{\sigma_Y}{\mu_Y}$.

Dessa equação, vemos que a diferença entre \(\mu_Z\) e \(\dfrac{\mu_Y}{\mu_X}\) depende de vários fatores. Se \(\sigma_X\) e \(\sigma_Y\) são pequenos — ou seja, se as duas concentrações são medidas com bastante precisão — a diferença é pequena. Se \(\mu_X\) é pequeno, a diferença é relativamente grande. Finalmente, a correlação entre \(X\) e \(Y\) afeta a diferença.

Agora, consideremos a variância. Novamente usando o resultado precedente e avaliando as derivadas parciais em \((\mu_X, \mu_Y)\), encontramos

$$\begin{align}
\sigma_Z^2 &\approx \dfrac{\sigma_X^2 \mu_Y^2}{\mu_X^4} + \dfrac{\sigma_Y^2}{\mu_X^2} - 2 \dfrac{\sigma_{XY} \mu_Y}{\mu_X^3} \\
\sigma_Z^2 &\approx \left(\dfrac{\mu_Y}{\mu_X}\right)^2\left(\kappa_Y^2+\kappa_X^2-2\rho\kappa_Y\kappa_X\right)
\end{align}$$

Sendo que os coeficientes de variação de Pearson de $X$ e $Y$ são $\kappa_X=\dfrac{\sigma_X}{\mu_X}$ e $\kappa_Y=\dfrac{\sigma_Y}{\mu_Y}$.

Portanto, o coeficiente de variação de Pearson de $Z$ é:

$$\kappa_Z=\dfrac{\sqrt{\kappa_Y^2+\kappa_X^2-2\rho\kappa_Y\kappa_X}}{1+\kappa_X^2-\rho\kappa_Y\kappa_X}$$

Dessa equação, vemos que a razão é bastante variável quando \(\mu_X\) é pequeno, paralelamente aos resultados do Exemplo A, e que a correlação entre \(X\) e \(Y\), se do mesmo sinal que \(\dfrac{\mu_Y}{\mu_X}\), diminui a \(\sigma_Z^2\).

## Exemplo D: Esperança e Variância Aproximadas de \( \text{BMI} \)

* [Body mass index: Wikipedia](https://en.wikipedia.org/wiki/Body_mass_index)

* [Anthropometric Survey of US Army Personnel (ANSUR I) data, 1988](https://www.openlab.psu.edu/ansur/)

**ANSUR I**

"Os dados da Pesquisa Antropométrica do Pessoal do Exército dos EUA (ANSUR I) foram publicados em 1988. Até 2012, foram o conjunto de dados mais abrangente disponível publicamente sobre tamanho e forma do corpo. Eles incluem mais de 140 medidas para quase 4000 militares adultos dos EUA (1774 homens e 2208 mulheres). Devido à qualidade dos dados – e ao preço extremamente baixo de "gratuito" – são amplamente utilizados em todo o mundo na prática de design. No entanto, eles não são representativos de nenhuma população-alvo de usuários, incluindo os militares dos EUA. Como resultado, projetos e padrões baseados nesses dados não acomodarão a população de usuários da maneira pretendida. Dito isso, eles são um recurso útil para estudar antropometria e construir modelos de síntese de dados."

* [Anthropometric Survey of US Army Personnel (ANSUR 2 or ANSUR II) data, 2012](https://www.openlab.psu.edu/ansur2/)

**ANSUR II**

"Os dados da Pesquisa Antropométrica do Pessoal do Exército dos EUA (ANSUR 2 ou ANSUR II) foram publicados internamente em 2012 e disponibilizados publicamente em 2017. Eles substituíram o ANSUR I como o conjunto de dados mais abrangente disponível publicamente sobre tamanho e forma do corpo. Eles incluem 93 medidas para mais de 6.000 militares adultos dos EUA (4.082 homens e 1.986 mulheres). Em contraste com os dados do ANSUR I, a nova amostra inclui reservistas. Apesar da presença de reservistas na amostra, ela ainda não é uma aproximação da população civil dos EUA. Consequentemente, embora haja informações úteis aqui, projetos e padrões baseados nesses dados não acomodarão a maioria das populações de usuários da maneira pretendida."

Para calcular a esperança $(\mu_{\text{BMI}}$) e a variância (\(\sigma_{\text{BMI}}^2\)) do Índice de Massa Corporal (\(\text{BMI}\)), onde \(\text{BMI} = \dfrac{W}{H^2}\), sendo \( W \) a massa corporal total e \( H \) a estatura, podemos usar o método aproximativo.

Suponha que \( W \) e \( H \) tenham médias \(\mu_W\) e \(\mu_H\) e variâncias \(\sigma_W^2\) e \(\sigma_H^2\). A correlação entre \( W \) e \( H \) é \(\rho\). A covariância \(\sigma_{WH}\) pode ser expressa em termos das variâncias e da correlação como:

$$ \sigma_{WH} = \rho \sigma_W \sigma_H $$

### Esperança de \( \text{BMI} \)

Para \( g(W, H) = \dfrac{W}{H^2} \):

\[ \mu_{\text{BMI}} \approx g(\mu_W, \mu_H) + \dfrac{1}{2} \left( \sigma_W^2 \dfrac{\partial^2 g}{\partial W^2} + \sigma_H^2 \dfrac{\partial^2 g}{\partial H^2} + 2 \sigma_{WH} \dfrac{\partial^2 g}{\partial W \partial H} \right) \]

Primeiro, calculamos as derivadas parciais:

$$\begin{align}
\dfrac{\partial g}{\partial W} &= \dfrac{1}{H^2} \\
\dfrac{\partial g}{\partial H} &= -\dfrac{2W}{H^3} \\
\dfrac{\partial^2 g}{\partial W^2} &= 0 \\
\dfrac{\partial^2 g}{\partial H^2} &= \dfrac{6W}{H^4} \\
\dfrac{\partial^2 g}{\partial W \partial H} &= -\dfrac{2}{H^3}
\end{align}$$

Avaliando essas derivadas nas médias \(\mu_W\) e \(\mu_H\), obtemos:

$$ \mu_{\text{BMI}} \approx \dfrac{\mu_W}{\mu_H^2} + \dfrac{1}{2} \left( \sigma_W^2 \cdot 0 + \sigma_H^2 \cdot \dfrac{6 \mu_W}{\mu_H^4} + 2 \sigma_{WH} \cdot \left(-\dfrac{2}{\mu_H^3}\right) \right) $$

Portanto,

$$ \mu_{\text{BMI}} \approx \dfrac{\mu_W}{\mu_H^2} + \dfrac{3 \sigma_H^2 \mu_W}{\mu_H^4} - \dfrac{4 \sigma_{WH}}{\mu_H^3} $$

Substituindo \(\sigma_{WH} = \rho \sigma_W \sigma_H\), temos:

$$\begin{align}
\mu_{\text{BMI}} &\approx \dfrac{\mu_W}{\mu_H^2} + \dfrac{3 \sigma_H^2 \mu_W}{\mu_H^4} - \dfrac{4 \rho \sigma_W \sigma_H}{\mu_H^3} \\
\mu_{\text{BMI}} &\approx \dfrac{\mu_W}{\mu_H^2}\left(1+3\kappa_H^2-4\rho\kappa_W\kappa_H\right)
\end{align}$$

Sendo que os coeficientes de variação de Pearson de $W$ e $H$ são $\kappa_W=\dfrac{\sigma_W}{\mu_W}$ e $\kappa_H=\dfrac{\sigma_H}{\mu_H}$.

### Variância de \( \text{BMI} \)

A variância é aproximada por:

$$ \sigma_{\text{BMI}}^2 \approx \left( \dfrac{\partial g}{\partial W} \right)^2 \sigma_W^2 + \left( \dfrac{\partial g}{\partial H} \right)^2 \sigma_H^2 + 2 \dfrac{\partial g}{\partial W} \dfrac{\partial g}{\partial H} \sigma_{WH}$$

Substituindo as derivadas parciais:

$$\begin{align}
\left( \dfrac{\partial g}{\partial W} \right)^2 &= \left( \dfrac{1}{\mu_H^2} \right)^2 = \dfrac{1}{\mu_H^4} \\
\left( \dfrac{\partial g}{\partial H} \right)^2 &= \left( -\dfrac{2 \mu_W}{\mu_H^3} \right)^2 = \dfrac{4 \mu_W^2}{\mu_H^6} \\
2 \dfrac{\partial g}{\partial W} \dfrac{\partial g}{\partial H} &= 2 \left( \dfrac{1}{\mu_H^2} \right) \left( -\dfrac{2 \mu_W}{\mu_H^3} \right) = -\dfrac{4 \mu_W}{\mu_H^5}
\end{align}$$

Portanto,

$$ \sigma_{\text{BMI}}^2 \approx \dfrac{\sigma_W^2}{\mu_H^4} + \dfrac{4 \mu_W^2 \sigma_H^2}{\mu_H^6} - \dfrac{4 \mu_W \sigma_{WH}}{\mu_H^5} $$

Substituindo \(\sigma_{WH} = \rho \sigma_W \sigma_H\), temos:

$$\begin{align} 
\sigma_{\text{BMI}}^2 &\approx \dfrac{\sigma_W^2}{\mu_H^4} + \dfrac{4 \mu_W^2 \sigma_H^2}{\mu_H^6} - \dfrac{4 \mu_W \rho \sigma_W \sigma_H}{\mu_H^5}\\
\sigma_{\text{BMI}}^2 &\approx \left(\dfrac{\mu_W}{\mu_H^2}\right)^2\left(\kappa_W^2+4\kappa_H^2-4\rho\kappa_W\kappa_H\right)
\end{align}$$

Sendo que os coeficientes de variação de Pearson são $\kappa_W=\dfrac{\sigma_W}{\mu_W}$ e $\kappa_H=\dfrac{\sigma_H}{\mu_H}$.

Portanto, o coeficiente de variação de Pearson de $\text{BMI}$ é:

$$\kappa_\text{BMI}=\dfrac{\sqrt{\kappa_W^2+4\kappa_H^2-4\rho\kappa_W\kappa_H}}{1+3\kappa_H^2-4\rho\kappa_W\kappa_H}$$

A distribuição exata lognormal de BMI pode ser encontrada em Silverman & Lipscombe (2022).

* [Log-normal distribution: Wikipedia](https://en.wikipedia.org/wiki/Log-normal_distribution)

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r}
# Set parameters for the lognormal distribution
meanlog <- 2  # mean of the log of the distribution
sdlog <- 1    # standard deviation of the log of the distribution

# Density function
x <- seq(0, 50, by = 0.01)
density <- dlnorm(x, meanlog, sdlog)
plot(x, density, type = "l", 
     main = "Lognormal Density", xlab = "x", 
     ylab = "Density")
abline(h=0,lty=2)

# Distribution function
cumulative <- plnorm(x, meanlog, sdlog)
plot(x, cumulative, type = "l", 
     main = "Lognormal Distribution Function", xlab = "x", 
     ylab = "Cumulative Probability")
```

A plausibilidade da lognormal para modelar variáveis biológicas foi defendida em Limpert et al. (2001).

"Em muitos laboratórios, a variabilidade do ELISA e de outros métodos de ensaio químico que produzem valores do tipo contínuo é resumida não pelo desvio-padrão (DP), mas pelo coeficiente de variação (CV), que é definido como o DP dividido pela média, sendo o resultado frequentemente relatado como uma porcentagem. O principal apelo do CV é que os DPs de tais ensaios geralmente aumentam ou diminuem proporcionalmente à medida que a média aumenta ou diminui, de modo que a divisão pela média a remove como um fator na variabilidade. O CV é, portanto, uma padronização do DP que permite a comparação das estimativas de variabilidade independentemente da magnitude da concentração do analito, pelo menos durante a maior parte da faixa de trabalho do ensaio." 

> REED, GF et al., 2002, p. 1235

Silverman (2023) mostrou que a distribuição lognormal é a mais provável para a infectividade do SARS-Cov-2 (COVID) após o início dos sintomas.

Conforme van Belle (2008, p. 88-90), em "5.1 begin with the lognormal distribution in environmental studies": 

"_Introdução_

Até agora, tem-se feito amplo uso do modelo estatístico normal. A distribuição normal foi a base para a maioria dos cálculos de tamanho de amostra e modelos de regressão. Em estudos ambientais, a distribuição lognormal é uma distribuição chave, e este capítulo começa com ela. Uma razão é que os dados ambientais tendem a ser assimétricos à direita, limitados à esquerda por zero, e os limites inferior e superior são de especial interesse científico e regulatório. Outras abordagens para lidar com dados assimétricos também serão discutidas.

_Regra Prática_

Pense em lognormal para dados de medição em estudos ambientais.

_Ilustração_

Os níveis de pesticidas no sangue de crianças que vivem perto de pomares tendem a ser distribuídos lognormalmente. Os níveis tendem a ser baixos e limitados à esquerda por zero, com as complicações adicionais de que os níveis de interesse científico estão próximos do nível de detecção e as medições são bastante caras.

Tabela 5.1 Caracterizações da Distribuição Lognormal e Conexão com a Distribuição Normal

| Escala Lognormal ($X$) | Escala Original ($Y$) |
|:------------------------|:-----------------------|
| **Média**              | $\mu_x$               | $\mu_y = e^{\mu_x + \sigma_x^2 / 2}$ |
| **Mediana**            | $\tilde{\mu}_x = \mu_x$ | $\tilde{\mu}_y = e^{\mu_x}$          |
| **Média/Mediana**      | $\mu_x / \tilde{\mu}_x = 1$ | $\mu_y / \tilde{\mu}_y = e^{\sigma_x^2 / 2}$ |
| **Variância**          | $\sigma_x^2$          | $\sigma_y^2 = \left[ e^{\sigma_x^2} - 1 \right] \left[ e^{2\mu_x + \sigma_x^2} \right]$ |
| **$CV^2$**             | $CV_x^2 = \sigma_x^2 / (\mu_x)^2$ | $CV_y^2 = e^{\sigma_x^2} - 1$         |

_Base da Regra_

Ott (1995a) fornece a base para esta regra. "Uma concentração que passa por uma série de diluições aleatórias independentes tende a ser distribuída lognormalmente." Ott lista três condições para que um processo aleatório forme um processo lognormal. Como muitos desses processos estão associados ao tempo, as condições serão declaradas em termos de tempo.

1. O valor da variável de resultado em um momento específico pode ser expresso como uma proporção linear de seu valor em um momento anterior.
2. Proporções lineares sucessivas são estatisticamente independentes.
3. O processo está em andamento há muito tempo.

_Discussão e Extensões_

Concentrações de ar e água em estudos ambientais são o resultado de processos de diluição independentes, o que equivale a multiplicar variáveis aleatórias independentes em vez de somá-las. A distribuição lognormal pode ser vista como a distribuição do teorema do limite central para esses tipos de variáveis ambientais. Uma discussão extensa pode ser encontrada em Ott (1995a), Capítulo 9.

A distribuição lognormal pode ser descrita da seguinte forma: Seja $X = \ln(Y) \sim N(\mu, \sigma^2)$, então $Y = e^X$ é lognormal. A Tabela 5.1 lista algumas propriedades das duas distribuições. Várias regras úteis podem ser derivadas desta tabela.

A razão entre a média e a mediana na escala $Y$ fornece um teste simples de lognormalidade, uma vez que o logaritmo dessa razão deve ser aproximadamente a variância na escala logarítmica:
$$
\ln\left(\dfrac{\mu_y}{\tilde{\mu}_y}\right) \approx \sigma_x^2
$$

Para um pequeno coeficiente de variação na escala $Y$, é aproximadamente verdadeiro que:
$$
\sigma_x \approx CV_y. \tag{5.2}
$$

Para valores muito grandes de $CV_y^2$ (digamos, maiores que 10),
$$
\sigma_x^2 \approx \ln(CV_y^2). \tag{5.3}
$$

Existe uma relação interessante entre a média, a mediana e o coeficiente de variação na escala $Y$:
$$
CV_y^2 = \left( \dfrac{\mu_y}{\tilde{\mu}_y} \right)^2 - 1. \tag{5.4}
$$

Essa relação pode ser usada como uma verificação simples da adequação da transformação logarítmica, uma vez que todas as três quantidades podem ser estimadas a partir dos dados.

Foi apontado (veja Zhou e Gao, 1997, para algumas referências) que um intervalo de confiança para $\mu_x$ não produz um intervalo de confiança para $\mu_y$ exponenciando os limites em $\mu_x$, mas produz um intervalo de confiança para a mediana da distribuição lognormal, $e^{\mu_x}$. Um intervalo de confiança para $\mu_y$ precisa incorporar uma estimativa intervalar para $\sigma_x^2$. Uma estimativa proposta por Cox (em Land, 1972) para $\mu_x + \dfrac{\sigma_x^2}{2}$ é:
$$
\bar{X} + \dfrac{s_x^2}{2} \pm z_{1 - \alpha / 2} \sqrt{\dfrac{s_x^2}{n} + \dfrac{s_x^4}{2(n - 1)}}, \tag{5.5}
$$
onde $\bar{X}$, $s_x^2$ e $n$ são a média, a variância e o tamanho da amostra das observações na escala logarítmica. O intervalo de confiança para $\mu_y$ é então obtido exponenciando essa estimativa. Zhou e Gao (1997) mostram que esse estimador possui propriedades razoavelmente boas. Eles também recomendam a consideração de uma estimativa bootstrap de $\mu_y$, que possui propriedades superiores para tamanhos de amostra pequenos.

A distribuição lognormal é uma distribuição muito versátil e a natural a ser considerada quando as observações são limitadas por zero. Quando uma variável é limitada (abaixo ou acima), a variância geralmente está relacionada à média. Do ponto de vista inferencial, isso é indesejável porque a estimativa e a precisão da estimativa são geralmente consideradas separadamente, envolvendo processos estatísticos um tanto diferentes."

A razão entre log-normais independentes ou correlacionadas é log-normal. (https://en.wikipedia.org/wiki/Ratio_distribution)

Sob a suposição de que massa corporal total e estatura são correlacionadas e têm distribuição lognormal, a distribuição de $\text{BMI}$ é lognormal com os parâmetros $\mu_Z$ e $\sigma^2_Z$:

$$\begin{align}
\ln (\text{BMI})=\ln(Z)&=\ln\left(\dfrac{W}{H^2}\right) \sim \text{normal}\left(\mu_{\ln(Z)}, \sigma_{\ln(Z)}^2\right) \\ \\ 
\mu_{\ln(Z)} &= \ln \left( \dfrac{\mu_W}{\mu_H^2} \right) \\
\rho_{\ln(Z)} &= \text{correl} ( \ln(W), \ln(H) ) \\
\sigma_{\ln(Z)}^2 &= \sigma_{\ln(W)}^2 + 4\sigma_{\ln(H)}^2 - 4\rho_{\ln(Z)} \sigma_{\ln(W)} \sigma_{\ln(H)} \\ \\
f(z) &= \dfrac{1}{z\sigma_{\ln(Z)}\sqrt{2\pi}} 
\exp \left(-\dfrac{1}{2}\left(\dfrac{\ln(z) - \mu_{\ln(Z)}}{\sigma_{\ln(Z)}}\right)^2 \right), \; z>0
\end{align}$$

Portanto:

$$\begin{align}
\text{BMI}=Z&=\dfrac{W}{H^2} \sim \text{lognormal}\left(\mu_Z, \sigma_Z^2\right) \\ \\
\mu_Z &= \dfrac{\mu_W}{\mu_H^2} \exp\left(\dfrac{1}{2}\sigma^2_{\ln (Z)}\right)\\
\sigma^2_Z &= \left(\dfrac{\mu_W}{\mu_H^2}\right)^2 \left(\exp\left(2\sigma^2_{\ln (Z)}\right)-\exp\left(\sigma^2_{\ln (Z)}\right)\right) \\ \\\\
\kappa_Z&=\dfrac{\sqrt{\exp\left(2\sigma^2_{\ln (Z)}\right)-\exp\left(\sigma^2_{\ln (Z)}\right)}}{\exp\left(\dfrac{1}{2}\sigma^2_{\ln (Z)}\right)} \end{align}$$

Essa fórmula representa a função de densidade de probabilidade exata (PDF) para o $\text{BMI}$ de uma população com massa corporal total e estatura correlacionadas.

```{r echo=FALSE, out.width="80%", fig.cap=""}
knitr::include_graphics("./image/Table2_BMI.png")
```

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r}
# ANSUR II
muW_F <- 67.7582
muH_F <- 1.6285
Zmu_F <- muW_F/(muH_F)^2
mulnZ_F <- log(Zmu_F)
slnW2_F <- 0.1604
slnH2_F <- 0.0394
rln_F <- 0.5387
slnZ2_F <- slnW2_F + 4*slnH2_F - 4*rln_F*slnW2_F*slnH2_F
muZ_F <- Zmu_F*exp(0.5*(slnZ2_F)^2)
sZ_F <- Zmu_F*sqrt(exp(2*slnZ2_F)-exp(slnZ2_F))
cvZ_F <- sZ_F/muZ_F
cat("CV Female = ", cvZ_F, sep="")

curve((1/(x*slnZ2_F*sqrt(2*pi)))*exp((-0.5)*((log(x)-mulnZ_F)/slnZ2_F)^2),
      10, 60,
      xlab="BMI (kg/m^2)", ylab="density", 
      main="Female (blue) & Male (black) BMI\nlognormal distribution",
      col="blue")

muW_M <- 85.52
muH_M <- 1.76
Zmu_M <- muW_M/(muH_M)^2
mulnZ_M <- log(Zmu_M)
slnW2_M <- 0.1654
slnH2_M <- 0.0390
rln_M <- 0.4716
slnZ2_M <- slnW2_M + 4*slnH2_M - 4*rln_M*slnW2_M*slnH2_M
muZ_M <- Zmu_M*exp(0.5*(slnZ2_M)^2)
sZ_M <- Zmu_M*sqrt(exp(2*slnZ2_M)-exp(slnZ2_M))
cvZ_M <- sZ_M/muZ_M
cat("\nCV Male = ", cvZ_M, "\n", sep="")

curve((1/(x*slnZ2_M*sqrt(2*pi)))*exp((-0.5)*((log(x)-mulnZ_M)/slnZ2_M)^2),
      10, 60,
      add=TRUE)
abline(h=0, lty=2)
abline(v=35, lty=2)
abline(v=18.5, lty=2)

# Calculate the areas under the curves

# Female under 18.5 (magreza)
area_F_magreza <- plnorm(18.5, meanlog = mulnZ_F, sdlog = slnZ2_F)
# Female above 35 (obesidade)
area_F_obesidade <- 1 - plnorm(35, meanlog = mulnZ_F, sdlog = slnZ2_F)

# Male under 18.5 (magreza)
area_M_magreza <- plnorm(18.5, meanlog = mulnZ_M, sdlog = slnZ2_M)
# Male above 35 (obesidade)
area_M_obesidade <- 1 - plnorm(35, meanlog = mulnZ_M, sdlog = slnZ2_M)

print(paste("Área da curva feminina abaixo de 18.5 (magreza):", 
            round(area_F_magreza,4)))
print(paste("Área da curva feminina acima de 35 (obesidade):", 
            round(area_F_obesidade,4)))
print(paste("Área da curva masculina abaixo de 18.5 (magreza):", 
            round(area_M_magreza,4)))
print(paste("Área da curva masculina acima de 35 (obesidade):", 
            round(area_M_obesidade,4)))
print(paste("Área da curva feminina acima de 18.5 (magreza) e abaixo de 35 (obesidade):", 
            round(1-area_F_magreza-area_F_obesidade,4)))
print(paste("Área da curva masculina acima de 18.5 (magreza) e abaixo de 35 (obesidade):", 
            round(1-area_M_magreza-area_M_obesidade,4)))
```

# Esperança e variância do produto de duas varíaveis aleatórias

* [Distribution of the product of two random variables: Wikipedia](https://en.wikipedia.org/wiki/Distribution_of_the_product_of_two_random_variables){target="_blank"}

A esperança, variância, índices de assimetria e curtose e coeficiente de variação de Pearson do produto de duas variáveis dependentes, $W=X_1X_2$, são obtidos a seguir.

Note que a esperança do produto, $\mathbb{E}\left(X_1X_2\right)$, é uma das parcelas da covariância das duas variáveis:

$$\begin{align}
\mathbb{C}\left(X_1,X_2\right)&=\mathbb{E}\left(X_1X_2\right)-\mathbb{E}\left(X_1\right)\mathbb{E}\left(X_2\right) \\
\sigma_{12}&=\mathbb{E}\left(X_1X_2\right)-\mu_1\mu_2 \\\\
\mathbb{E}\left(X_1X_2\right)&=\mu_1\mu_2 + \sigma_{12} \\
\mu_{1.2}&=\mu_1\mu_2 + \rho\sigma_{1}\sigma_{2} \\
\mu_{1.2}&=\mu_1\mu_2\left(1+\rho\kappa_{1}\kappa_{2}\right) \\
\end{align}$$

Sendo que $\kappa=\sigma/\mu$ é o coeficiente de variação de Pearson.

A variância do produto é, conforme Bohrnstedt & Goldberger (1969):

$$\begin{align}
\mathbb{V}(X_1 X_2)&=\mathbb{E}\left(\left(X_1X_2-\mathbb{E}\left(X_1X_2\right)\right)^2\right) \\
&=\mathbb{E}\left(\left(X_1 X_2\right)^2\right) - \left(\mathbb{E}\left(X_1 X_2\right)\right)^2 \\
 &= \left(\sigma_1 \sigma_2\right)^2 + \mu_2^2\sigma_1^2 + \mu_1^2\sigma_2^2+ 2\mu_1\mu_2\rho\sigma_1\sigma_2-\left(\rho\sigma_1\sigma_2\right)^2+2\mu_1\mathbb{C}\left(X_1,X_2^2\right) + 2\mu_2\mathbb{C}\left(X_1^2,X_2\right)\\
 & \approx \left(\sigma_1 \sigma_2\right)^2 +\mu_2^2\sigma_1^2 + \mu_1^2\sigma_2^2+ 2\mu_1\mu_2\rho\sigma_1\sigma_2\\
\sigma^2_{1.2} & \approx \left(\mu_1\mu_2\right)^2\left(\left(\kappa_1\kappa_2\right)^2+\kappa_1^2 + \kappa_2^2+ 2\rho\kappa_1\kappa_2\right)
\end{align}$$

Se as variáveis são independentes, então:

$$\begin{align}
\mathbb{V}(X_1 X_2) &= \left(\sigma_1 \sigma_2\right)^2 + \mu_2^2\sigma_1^2 + \mu_1^2\sigma_2^2\\
\sigma^2_{1.2} & = \left(\mu_1\mu_2\right)^2\left(\left(\kappa_1\kappa_2\right)^2+\kappa_1^2 + \kappa_2^2\right)
\end{align}$$

Conforme Craig (1936), se duas variáveis correlacionadas têm distribuição normal, então o seu produto tem os seguintes momentos:

$$\begin{align}
\mu_{1.2}&=\mu_1\mu_2\left(1+\rho\kappa_{1}\kappa_{2}\right) \\
\sigma^2_{1.2} & = \left(\mu_1\mu_2\right)^2\left(\kappa_1^2 + \kappa_2^2+ \left(1-\rho^2\right)\left(\kappa_1\kappa_2\right)^2\right)\\
\kappa_{1.2}&=\sqrt{\kappa_1^2 + \kappa_2^2+ \left(1-\rho^2\right)\left(\kappa_1\kappa_2\right)^2}\\ \\
\beta_{1,1.2}&=\dfrac{6\left(\kappa_1\kappa_2\right)^2}{\left(\left(\kappa_1\kappa_2\right)^2+\kappa_1^2+\kappa_2^2\right)^{3/2}}\in \left[-\dfrac{2}{\sqrt{3}},\dfrac{2}{\sqrt{3}}\right]\\
\beta_{2,1.2}&=\dfrac{6\left(\kappa_1\kappa_2\right)^2\left(\left(\kappa_1\kappa_2\right)^2+2\left(\kappa_1^2+\kappa_2^2\right)\right)}{\left(\left(\kappa_1\kappa_2\right)^2+\kappa_1^2+\kappa_2^2\right)^2}\in \left[-6,6\right]\\
\dfrac{\beta_{2,1.2}}{\beta_{1,1.2}}&=\dfrac{\left(\kappa_1\kappa_2\right)^2+2\left(\kappa_1^2+\kappa_2^2\right)}{\sqrt{\left(\kappa_1\kappa_2\right)^2+\kappa_1^2+\kappa_2^2}}\in \left[-3\sqrt{3},3\sqrt{3}\right]
\end{align}$$

# Distribuição exata e aproximação da razão de duas normais dependentes

* Ratios of Normal Variables - Marsaglia - 2006
* Confidence Interval for the Ratio of Two Normal Variables - Morisugi et al. (2009)

Conforme Hinkley (1969), se duas variáveis correlacionadas têm distribuição normal, então a probabilidade de sua razão $X_1/X_2$ ser menor do que $a\in\mathbb{R}$ é:

$$\begin{align}
F\left(\dfrac{X_1}{X_2}\le a\right)&=I(b,-k;r)+I(-b,k;r)\\
b&=\dfrac{\mu_1-a\mu_2}{h}\\
k&=\dfrac{1}{\kappa_2}\\
r&=\dfrac{a\mu_2\kappa_2-\rho\kappa_1}{h}\\
h&=\sqrt{\left(\mu_1\kappa_1\right)^2 + \left(a\mu_2\kappa_2\right)^2-2a\mu_1\mu_2\rho\kappa_1\kappa_2}
\end{align}$$

Este resultado é muito difícil de implementar.

Conforme Marsaglia (2006), a razão de duas variáveis normais correlacionadas é aproximadamente normal sob certas condições. 

__Proposição__: Para quaisquer duas variáveis conjuntas normais \(X_1\) e \(X_2\) com médias \(\mu_{1}, \mu_{2}\), variâncias \(\sigma_{1}^2, \sigma_{2}^2\) e correlação \(\rho\), a distribuição de \(\dfrac{X_1}{X_2}\) é, após tradução e mudança de escala, a mesma que \(\dfrac{a + W}{b + Z}\) com \(W, Z\) independentes normais padrão e \(a, b\) constantes não-negativas. Especificamente, para uma dada razão \(\dfrac{X_1}{X_2}\), existem constantes \(r\) e \(s\) tais que

$$
r\left(\dfrac{X_1}{X_2} - s\right) = \dfrac{rX_1 - sX_2}{X_2}
$$

é distribuído como \(\dfrac{a + W}{b + Z}\) e \(\dfrac{X_1}{X_2}\) é distribuído como \(\dfrac{1}{r} \left(\dfrac{a + W}{b + Z}\right) + s\).

Assim, para escolhas adequadas das constantes \(r\) e \(s\), \(r\left(\dfrac{X_1}{X_2}\right) - rs\) se torna \(\dfrac{a + W}{b + Z}\).

Primeiro, se escolhermos \(s = \dfrac{\rho \sigma_{X_1}}{\sigma_{X_2}}\), então \(E[(X_1 - sX_2)X_2] = E[X_1 - sX_2]E[X_2]\), de modo que \(X_1 - sX_2\) e \(X_2\) serão independentes. Assim, \(\dfrac{X_1 - sX_2}{X_2} = \dfrac{X_1}{X_2} - s\) é a razão de duas variáveis normais independentes. Para colocar a razão \(\dfrac{X_1 - sX_2}{X_2} = \dfrac{X_1}{X_2} - s\) na forma requerida, precisamos apenas dividir o numerador e o denominador por seus respectivos sigmas, ou seja, multiplicar por \(r\), a razão inversa dos dois desvios-padrão.

Agora, \(X_1 - sX_2\) tem variância \(\sigma_{1}^2 (1 - \rho^2)\), e o denominador, \(X_2\), tem variância \(\sigma_{2}^2\). Assim,

$$
r = \dfrac{\sigma_{2}}{\pm \sigma_{1} \sqrt{1 - \rho^2}}, \quad s = \dfrac{\rho \sigma_{1}}{\sigma_{2}}
$$

converterá \(r \left(\dfrac{X_1}{X_2} - s\right)\) em \(\dfrac{a + W}{b + Z}\).

Mas quais são \(a\) e \(b\)? Com \(h = \pm \sigma_{1} \sqrt{1 - \rho^2}\), a média de \(\dfrac{X_1 - sX_2}{h}\) é \(\dfrac{\mu_{1} - s\mu_{2}}{h}\), e a de \(\dfrac{X_2}{\sigma_{2}}\) é \(\dfrac{\mu_{2}}{\sigma_{2}}\). Como \(\dfrac{-a + W}{-b + Z}\) tem a mesma distribuição que \(\dfrac{a + W}{b + Z}\), precisamos apenas escolher o sinal de \(h\) para que os resultantes \(a\) e \(b\) tenham o mesmo sinal:

$$
b = \dfrac{\mu_{2}}{\sigma_{2}}, \quad a = \pm \dfrac{\mu_{1} - s\mu_{2}}{h}
$$

Suponha que temos variáveis normais conjuntas $X_1$ e $X_2$ com correlação $\rho = 0.8$, médias e desvios-padrão $\mu_{1} = 30.5$, $\mu_{2} = 32$, $\sigma_{1} = 5$, $\sigma_{2} = 4$. Desenvolveremos $r$ e $s$ que fazem com que $r \left( \dfrac{X_1}{X_2} \right) - rs$ seja distribuído como $\dfrac{a + W}{b + Z}$. Em seguida, verificamos que a distribuição resultante está entre uma classe de razões de normais que são aproximadamente normais ($a < 2.5$, $b > 4$, ver Seção 4). Usando, dessa seção, aproximações para momentos que não existem, encontraremos a média e a variância para uma distribuição normal que é bastante próxima da distribuição de $\dfrac{X_1}{X_2}$, e verificaremos essa proximidade plotando as densidades verdadeira e aproximada.

Para esta razão $\dfrac{X_1}{X_2}$, temos $h = \pm \sigma_{1} \sqrt{1 - \rho^2} = \pm 3$, de modo que $r = \dfrac{4}{h}$ e $s = 1$. Então

$$
b = \dfrac{\mu_{2}}{\sigma_{2}} = \dfrac{32}{4} = 8, \quad a = \pm \dfrac{\mu_{1} - s \mu_{2}}{h} = \pm \dfrac{30.5 - 32}{3} = \pm 0.5, 
$$

escolhemos “-” para $h$ (e $r$) para fazer $a = 0.5$ ter o mesmo sinal que $b = 8$, e assim

$$ \dfrac{4}{-3} \left( \dfrac{X_1}{X_2} - 1 \right) \text{ é distribuído como } \dfrac{0.5 + W}{8 + Z} $$
e
$$ \dfrac{X_1}{X_2} \text{ é distribuído como } 1 - \dfrac{3}{4} \left( \dfrac{0.5 + W}{8 + Z} \right). $$
Uma vez que $\dfrac{0.5 + W}{8 + Z}$ cai no intervalo $a < 2.5$, $4 < b$ para o qual a razão de normais é aproximadamente normal (Seção 4), este $\dfrac{X_1}{X_2}$ deve ser quase normal. A média e a variância aproximadas de $\dfrac{0.5 + W}{8 + Z}$ são $$ \mu = \dfrac{a}{1.01b - 0.2713} = 0.06403, \quad \sigma^2 = \dfrac{a^2 + 1}{b^2 + 0.108b - 3.795} - \mu^2 = 0.01636866, $$
de modo que $\dfrac{X_1}{X_2}$ é aproximadamente normal $(0.952, 0.0959)$,
(média = $1 - \dfrac{3}{4} 0.064 = 0.952$, sigma = $ \dfrac{3}{4} 0.1279 = 0.0959$).

# Distribuição exata de de proporção entre duas variáveis aleatórias de Pareto

A distribuição exata de de proporção entre duas variáveis aleatórias,  $X/(X+Y)$, ambas com distribuição de Pareto, pode ser encontrada em Nadarajah & Kotz (2007).


# Transformada de Mellin: esperança e variância de potência de variável aleatória 

* [Kato, FH (2005) Análise de carteiras em tempo discreto: dissertação de mestrado orientada por Prof. José de Oliveira Siqueira](https://teses.usp.br/teses/disponiveis/12/12139/tde-24022005-005812/pt-br.php){target="_blank"}

* `Mellin_Laplace_Normal_LogNormal_Chi2_Uniform.nb`: funções `MellinTransform` e `LaplaceTransform` do Mathematica e WolframAlpha

Transformada de Mellin de função de variável aleatória $X$, conforme Park & Sharp-Bete, 1990, p. 394-7:

$$\begin{align}
\mathcal{M}_{X}(s) &= \int_{0}^{\infty}{x^{s-1}f(x)dx}\\
\mathcal{M}_{X}(s) &=\mathbb{E}\left(X^{s-1}\right)
\end{align}$$

$$\begin{align}
\mathcal{M}_{X}(2) &= \mathbb{E}\left(X\right)=\mu_X\\
\mathcal{M}_{X}(3) &=\mathbb{E}\left(X^{2}\right)\\
\sigma^2_X&=\mathcal{M}_{X}(3)-\left(\mathcal{M}_{X}(2)\right)^2
\end{align}$$

Sendo que $x\in \mathbb{R}_+^*$ e $s\in \mathbb{N}^*$.

A transformada de Mellin de $Y=f(X)=X^c$, $c\in \mathbb{R}$, é:

$$\begin{align}
\mathcal{M}_{Y}(s) &= \mathbb{E}\left(Y^{s-1}\right)\\
 &=\mathbb{E}\left(\left(X^c\right)^{s-1}\right)\\
 &=\mathbb{E}\left(X^{cs-c}\right)\\
\mathcal{M}_{Y}(s) &=\mathcal{M}_{X}(cs-c+1)
\end{align}$$

A transformada de Mellin de função potência de $X \sim \text{Uniforme}(a,b)$, $Y=f(X)=X^c$, $c\in \mathbb{R}_+^*$, será determinada para algumas distribuições de $X$.

## uniforme contínua

A transformada de Mellin de $X \sim \text{uniforme}(a,b)$ é:

$$\begin{align}
\mathcal{M}_{X}(s) &= \dfrac{1}{s}\dfrac{b^s-a^s}{b-a}\\
\end{align}$$

Esperança e variância:

$$\begin{align}
\mu&=\mathcal{M}_{X}(2) = \dfrac{a+b}{2}\\
\mathcal{M}_{X}(3) &= \dfrac{1}{3}\dfrac{b^3-a^3}{b-a} \\
\sigma^2&=\mathcal{M}_{X}(3) - \left(\mathcal{M}_{X}(2)\right)^2\\
\sigma^2&=\dfrac{1}{3}\dfrac{b^3-a^3}{b-a} - \left(\dfrac{a+b}{2}\right)^2\\
\sigma^2&=\dfrac{(b-a)^2}{12}
\end{align}$$

Coeficiente de variação de Pearson:

$$\begin{align}
\kappa&=\dfrac{\sqrt{\dfrac{(b-a)^2}{12}}}{\dfrac{a+b}{2}}\\
\kappa&=\dfrac{b^2-a^2}{4\sqrt{3}}
\end{align}$$

A transformada de Mellin de função potência $Y=f(X)=X^c$, $X \sim \text{uniforme}(a,b)$ e $c\in \mathbb{R}_+^*$, é:

$$\begin{align}
\mathcal{M}_{X}(s) &= \dfrac{1}{s}\dfrac{b^s-a^s}{b-a}\\ \\
\mathcal{M}_{Y}(s) &=\mathcal{M}_{X}(cs-c+1)\\
\mathcal{M}_{Y}(s) &=\dfrac{1}{cs-c+1}\dfrac{b^{cs-c+1}-a^{cs-c+1}}{b-a}
\end{align}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`MellinTransform[PDF[UniformDistribution[{a, b}], x], x, s]`](https://www.wolframalpha.com/input?i=MellinTransform%5BPDF%5BNormalDistribution%5B0%2C+1%5D%2C+x%5D%2C+x%2C+s%5D){target="_blank"}

Portanto, esperança e variância são:

$$\begin{align}
\mu_Y &= \mathcal{M}_{Y}(2)=\dfrac{1}{c+1}\dfrac{b^{c+1}-a^{c+1}}{b-a}\\
\sigma^2_Y&=\mathcal{M}_{Y}(3)-\left(\mathcal{M}_{Y}(2)\right)^2=\dfrac{1}{2c+1}\dfrac{b^{2c+1}-a^{2c+1}}{b-a}-\left(\dfrac{1}{c+1}\dfrac{b^{c+1}-a^{c+1}}{b-a}\right)^2
\end{align}$$

## normal padrão

Os dois primeiros momentos centrados na esperança de função potência de $X \sim \text{normal}(0,1)$, $Y=f(X)=X^c$, $c\in \mathbb{R}_+^*$, são:

$$\begin{align}
\mathcal{M}_{X}(s) &= \dfrac{2^{(s-3)/2}}{\sqrt{\pi}}\Gamma\left(\dfrac{s}{2}\right)\\ 
\\
\mathbb{E}\left(X^{s-1}\right)&=2 \mathcal{M}_{X}(s) = 2 \dfrac{2^{(s-3)/2}}{\sqrt{\pi}}\Gamma\left(\dfrac{s}{2}\right), \; s\; \text{par}\\
\mathbb{E}\left(X^{s-1}\right)&=0, \; s\; \text{ímpar} \\ 
\\
\mathcal{M}_{Y}^*(s)&=\mathbb{E}\left(Y^{s-1}\right)=\mathbb{E}\left(\left(X^c\right)^{s-1}\right)=2 \mathcal{M}_{X}(c(s-1)+1) \\
\mathcal{M}_{Y}^*(s)&=2 \dfrac{2^{(c(s-1)-2)/2}}{\sqrt{\pi}}\Gamma\left(\dfrac{c(s-1)+1}{2}\right)
\end{align}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`MellinTransform[PDF[NormalDistribution[0, 1], x], x, s]`](https://www.wolframalpha.com/input?i=MellinTransform%5BPDF%5BNormalDistribution%5B0%2C+1%5D%2C+x%5D%2C+x%2C+s%5D){target="_blank"}

A esperança e variância de $Y$ para $c=2$ ($Y=X^2, \; X\sim \text{normal(0,1)}$) são:

$$\mathcal{M}_Y^*(s) = 2 \dfrac{2^{(2s-4)/2}}{\sqrt{\pi}} \Gamma\left(\dfrac{2s-1}{2}\right) = \dfrac{2^{s-1}}{\sqrt{\pi}} \Gamma\left(\dfrac{2s-1}{2}\right)$$

Vamos agora calcular os momentos para \( s = 2, 3\):

Para \( s = 2 \):
   $$ \mathcal{M}_X^*(2) = 2 \frac{2^{2-2}}{\sqrt{\pi}} \Gamma\left(\frac{3}{2}\right) = 2 \frac{2^0}{\sqrt{\pi}} \Gamma\left(\frac{3}{2}\right) = 2 \frac{1}{\sqrt{\pi}} \times \frac{\sqrt{\pi}}{2} = 2 \times \frac{1}{2} = 1 $$

Para \( s = 3 \):
   $$ \mathcal{M}_X(3) = 2 \frac{2^{3-2}}{\sqrt{\pi}} \Gamma\left(\frac{5}{2}\right) = 2 \frac{2^1}{\sqrt{\pi}} \Gamma\left(\frac{5}{2}\right) = 2 \frac{2}{\sqrt{\pi}} \times \frac{3\sqrt{\pi}}{4} = \frac{12}{4} = 3 $$

$$\begin{align}
\mu_Y &= 1 \\
\sigma^2_Y&=3 - 1^2 = 2
\end{align}$$

## qui-quadrado

A transformada de Mellin de $Y\sim \chi^2_\nu$ é:

$$ \mathcal{M}_Y(s) = 2^{s-1} \dfrac{\Gamma\left(s-1 +\dfrac{ \nu}{2}\right)}{\Gamma\left(\dfrac{\nu}{2}\right)} $$

Para diferentes valores de \(\nu\):

Para \(\nu = 1\):
$$ \mathcal{M}_Y(s) = 2^{s-1} \dfrac{\Gamma\left(\dfrac{2(s-1) + 1}{2}\right)}{\Gamma\left(\dfrac{1}{2}\right)} $$
Sabemos que \(\Gamma\left(\dfrac{1}{2}\right) = \sqrt{\pi}\), então:
$$ \mathcal{M}_Y(s) = \dfrac{2^{s-1}}{\sqrt{\pi}} \Gamma\left(\dfrac{2s - 1}{2}\right) $$

Para \(\nu = 2\):
$$ \mathcal{M}_Y(s) = 2^{s-1} \dfrac{\Gamma\left(\dfrac{2(s-1) + 2}{2}\right)}{\Gamma(1)} $$
Sabemos que \(\Gamma(1) = 1\), então:
$$ \mathcal{M}_Y(s) = 2^{s-1} \Gamma(s) $$

Portanto, a transformada de Mellin de $Y\sim \chi^2_1$ é:

$$\mathcal{M}_Y(s) = \dfrac{2^{s-1}}{\sqrt{\pi}} \Gamma\left(\dfrac{2s-1}{2}\right)$$

Note que esta transformada é idêntica à transformada de Mellin da normal padrão ao quadrado.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`MellinTransform[PDF[ChiSquareDistribution[1], x], x, s]`](https://www.wolframalpha.com/input?i=MellinTransform%5BPDF%5BChiSquareDistribution%5B1%5D%2C+x%5D%2C+x%2C+s%5D){target="_blank"}

## $\text{normal}(0,\sigma^2)$

Os dois primeiros momentos centrados na esperança de função potência de $X \sim \text{normal}(0,\sigma^2)$, $Y=f(X)=X^c$, $c\in \mathbb{R}_+^*$, são:

$$\begin{align}
\mathcal{M}_{X}(s) &= \sigma^{s-1}\dfrac{2^{(s-3)/2}}{\sqrt{\pi}}\Gamma\left(\dfrac{s}{2}\right)\\ 
\\
\mathbb{E}\left(X^{s-1}\right)&=2 \mathcal{M}_{X}(s) = 2\sigma^{s-1} \dfrac{2^{(s-3)/2}}{\sqrt{\pi}}\Gamma\left(\dfrac{s}{2}\right), \; s\; \text{par}\\
\mathbb{E}\left(X^{s-1}\right)&=0, \; s\; \text{ímpar} \\ 
\\
\mathcal{M}_{Y}^*(s)&=\mathbb{E}\left(Y^{s-1}\right)=\mathbb{E}\left(\left(X^c\right)^{s-1}\right)=2 \mathcal{M}_{X}(c(s-1)+1) \\
\mathcal{M}_{Y}^*(s)&=2\sigma^{s-1} \dfrac{2^{(c(s-1)-2)/2}}{\sqrt{\pi}}\Gamma\left(\dfrac{c(s-1)+1}{2}\right)
\end{align}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`MellinTransform[PDF[NormalDistribution[0, 1], x], x, s]`](https://www.wolframalpha.com/input?i=MellinTransform%5BPDF%5BNormalDistribution%5B0%2C+1%5D%2C+x%5D%2C+x%2C+s%5D){target="_blank"}

A transformada de Mellin de $Y$ para $c=2$ de $Y=X^2, \; X\sim \text{Normal}(0,\sigma^2)$, é:

$$\mathcal{M}_Y^*(s) = 2 \sigma^{s-1}\dfrac{2^{(2s-4)/2}}{\sqrt{\pi}} \Gamma\left(\dfrac{2s-1}{2}\right) = \sigma^{s-1}\dfrac{2^{s-1}}{\sqrt{\pi}} \Gamma\left(\dfrac{2s-1}{2}\right)$$

Para \( s = 3 \):
$$ \mathbb{E}[Y^2] = \sigma^2 $$

Note que $Y=(\sigma Z)^2$, sendo que $X\sim \text{Normal}(0,1)$, implica que $\mathcal{M}_Y^*(s)=\mathcal{M}_{\sigma Z}^*(s)=\sigma^{s-1}\mathcal{M}_Z^*(s)$, conforme Tabela 2 de Park, 1986, p. 122.

__Tabela 2 de Park, 1986, p. 122__

| Variável Aleatória | Transformada de Mellin |
|:--------------------|:---------------------------------------------|
| \(X\)              | \(\mathcal{M}_X(s)\) |
| \(X^b\)            | \(\mathcal{M}_X(bs - b + 1)\) |
| \(1/X\)            | \(\mathcal{M}_X(2 - s)\) |
| \(aX\)             | \(a^{s-1} \mathcal{M}_X(s)\) |
| \(XY\)             | \(\mathcal{M}_X(s) \mathcal{M}_Y(s)\) |
| \(X/Y\)            | \(\mathcal{M}_X(s) \mathcal{M}_Y(2 - s)\) |
| \(aX^bY^c\)        | \(a^{s-1} \mathcal{M}_X(bs - b + 1) \mathcal{M}_Y(cs - c + 1) \) |

<!-- | \(e^{-X}\)         | \(\mathcal{L}_X(s - 1)\) | -->

Sendo que:

- $s=2,3,\ldots$
- \(a, b, c\): constantes positivas
- \(X, Y\): variáveis aleatórias independentes
- \(\mathcal{L}_X(s)\): transformada de Laplace de pdf \(f(x)\)

## lognormal

A transformada de Mellin da distribuição lognormal, $X\sim \text{lognormal}(\mu,\sigma^2)$, é dada por:

$$\mathcal{M}_X(s) = \exp\left( (s-1)\mu  + (s-1)^2\dfrac{\sigma^2 }{2} \right)$$

Esperança e variância (\( s = 2, 3 \)):

Para \( s = 2 \):

$$ \mu_X=\mathcal{M}_X(2) = \exp\left( \mu (2-1) + \dfrac{\sigma^2 (2-1)^2}{2} \right) = \exp\left( \mu + \dfrac{\sigma^2}{2} \right) $$

Para \( s = 3 \):

$$ \mathcal{M}_X(3) = \exp\left( \mu (3-1) + \dfrac{\sigma^2 (3-1)^2}{2} \right) = \exp\left( 2\left(\mu + \sigma^2 \right) \right)$$

A variância é:

$$\begin{align}
\sigma^2_X&=\mathcal{M}_X(3) - \mu^2\\
\sigma^2_X&=\exp\left( 2\left(\mu + \sigma^2 \right) \right) - \left(\exp\left( \mu + \dfrac{\sigma^2}{2} \right)\right)^2\\
\sigma^2_X&=\left(\exp\left(\sigma^2 \right)-1\right)  \exp\left(\sigma^2 +2\mu \right)
\end{align}$$

Portanto, o coeficiente de variação de Pearson é:

$$\kappa_X=\sqrt{e^{\sigma^2}-1}$$

Note que o coeficiente de variação de Pearson não depende do parâmetro $\mu$.

Medições que são distribuídas lognormalmente exibem coeficiente de variação de Pearson (CV) estacionário (https://en.wikipedia.org/wiki/Coefficient_of_variation).

A transformada de Mellin de $Y=X^c$, $c>0$ e $X\sim \text{lognormal}(\mu,\sigma^2)$, é dada por:

$$\begin{align}
\mathcal{M}_{Y}(s)&=\mathbb{E}\left(Y^{s-1}\right)=\mathbb{E}\left(\left(X^c\right)^{s-1}\right)= \mathcal{M}_{X}(c(s-1)+1) \\
\mathcal{M}_{Y}(s)&=\exp\left( c (s-1)\mu + c^2(s-1)^2\dfrac{\sigma^2 }{2} \right)
\end{align}$$

Para $c=2$, tem-se que esperança e variância (\( s = 2, 3 \)) são:

Para \( s = 2 \):

$$ \mu_Y=\mathcal{M}_Y(2) = \exp\left( 2\mu (2-1) + 4(2-1)^2\dfrac{\sigma^2 }{2} \right) = \exp\left( 2\mu + 2\sigma^2 \right) $$

Para \( s = 3 \):

$$ \mathcal{M}_Y(3) = \exp\left( 2(3-1)\mu  + 4(3-1)^2\dfrac{\sigma^2 }{2} \right) = \exp\left( 4\mu + 8\sigma^2 \right)$$

A variância é:

$$\begin{align}
\sigma^2_Y&=\mathcal{M}_Y(3) - \mu^2_Y\\
\sigma^2_Y&=\exp\left( 4\mu + 8\sigma^2 \right) - \left(\exp\left( 2\mu + 2\sigma^2 \right)\right)^2\\
\sigma^2_Y&=\left(\exp\left(4\sigma^2 \right)-1\right)  \exp\left(4\sigma^2 +4\mu \right)
\end{align}$$

Portanto, o coeficiente de variação de Pearson é:

$$\kappa_Y=\sqrt{e^{4\sigma^2}-1}$$

$$\begin{array}{|c|c|l|}
\hline
\text{Probabilidade} & f(x) & \text{Transformada de Mellin} \\
\hline
\text{Uniforme} & \dfrac{1}{b - a} & \dfrac{b^s - a^s}{s(b - a)} \\
a \leq x \leq b & & \\
\hline
\text{Exponencial} & ae^{-ax} & \left( \dfrac{1}{a} \right)^{s-1} \Gamma(s) \\
x > 0 & & \\
\hline
\text{Gamma} & \dfrac{a (ax)^b e^{-ax}}{\Gamma(b)} & \left( \dfrac{1}{a} \right)^{s-1} \dfrac{\Gamma(b + s - 1)}{\Gamma(b)} \\
0 < x < \infty & & \\
b > -1 & & \\
\hline
\text{Beta Padrão} & \text{B}(a, b) x^{a-1}(1 - x)^{b-1} & \dfrac{\Gamma(a + b) \Gamma(a + s - 1)}{\Gamma(a) \Gamma(a + b + s - 1)} \\
0 \leq x \leq 1 & & \\
a > 0, b > 0 & & \\
\hline
\text{Triangular} & \dfrac{2(x - L)}{(H - L)(M_0 - L)} & \dfrac{2}{(H - L)s(s + 1)} \left[ \dfrac{H H^s - M_0^s}{H - M_0} - \dfrac{L (M_0^s - L^s)}{M_0 - L} \right] \\
L \leq x \leq M_0 & & \\
M_0 \leq x \leq H & & \\
\hline
\text{Beta Generalizada} & \text{B}(a, b) \dfrac{(y - L)^{a-1} (H - y)^{b-1}}{(H-L)^{a+b-1}} & \sum_{k=0}^{s-1} \dfrac{(s-1)}{k} L^{s-1-k}(H - L)^{k} M_x(k + 1) \\
L \leq y \leq H & & \\
\hline
\text{Normal Padrão} & \dfrac{1}{\sqrt{2\pi}} \exp \left( -\dfrac{x^2}{2} \right) & \dfrac{2^{\frac{s - 1}{2}} \Gamma\left( \dfrac{s}{2} \right)}{\sqrt{\pi}} \\
- \infty < x < \infty & & \\
\hline
\end{array}$$

# Método de aproximação de esperança e variância de fator de decaimento exponencial

Conforme Park & Sharp-Bete (1990, p. 408), a aproximação da esperança e variância do fator de decaimento exponencial (desconto contínuo) é:

$$\begin{align}
\mathbb{E}\left(\exp(-rT)\right)&\approx\left(1+\dfrac{r^2\sigma_T^2}{2}-\dfrac{r^3\mu_{3,T}}{6}\right)\exp(-r\mu_T)\\
\mathbb{V}\left(\exp(-rT)\right)&\approx r^2\sigma^2_T\left(1-\dfrac{r^2\sigma_T^2}{4}\right)\exp(-2r\mu_T)
\end{align}$$

# Transformada de Laplace: fator de decaimento exponencial

Suponha que a variável aleatória $T$ tem função de densidade de probabilidade $f(t)$.

A transformada de Laplace, sendo $r>0$, é dada por:

$$\mathcal{L}(r)=\int_{0}^{\infty}{f(t)\exp(-rt)dt}=\mathbb{E}\left(\exp(-rT)\right)$$

A transformada de Laplace é a esperança da variável aleatória fator de decaimento exponencial $\exp(-rT)$.

$$\mathbb{E}\left(\exp(-rT)\right)=\mathcal{L}(r)$$

a variância da variável aleatória fator de decaimento exponencial $\exp(-rT)$ é:

$$\mathbb{V}\left(\exp(-rT)\right)=\mathcal{L}(2r)-\left(\mathcal{L}(r)\right)^2$$


$$\begin{array}{|c|c|c|}
\hline
\text{Função de Densidade de } T& \text{Transformada de Laplace} \, \mathbb{E}(e^{-rT}) \\
\hline
\text{Normal Generalizada} & \exp\left( \dfrac{1}{2}(r^2 \sigma^2-2r \mu ) \right) \\
\dfrac{1}{\sigma \sqrt{2 \pi}} \exp\left( -\dfrac{1}{2}\left(\dfrac{x - \mu}{ \sigma} \right)^2\right) & -\infty < x < \infty \\
\hline
\text{Normal Padrão} & \exp\left( \dfrac{1}{2}r^2 \right) \\
\dfrac{1}{\sqrt{2 \pi}} \exp\left( -\dfrac{1}{2}x^2 \right) & -\infty < x < \infty \\
\hline
\text{Uniforme} & \dfrac{\exp(-ra) - \exp(-rb)}{r(b - a)} \\
\dfrac{1}{b - a} & a \leq x \leq b \\
\hline
\text{Gamma} & \left( 1 + \dfrac{r}{a} \right)^{-b} \\
\dfrac{a^b}{\Gamma(b)} x^{b-1} \exp\left( -ax \right) & 0 < x < \infty \\
\hline
\text{Exponencial} & \left( 1 + \dfrac{r}{a} \right)^{-1} \\
a\exp(-ax) & 0 < x < \infty \\
\hline
\end{array}$$

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoW.png")
```

[`LaplaceTransform[PDF[UniformDistribution[a, b], x], x, r]`](https://www.wolframalpha.com/input?i=LaplaceTransform%5BPDF%5BUniformDistribution%5Ba%2C+b%5D%2C+x%5D%2C+x%2C+r%5D){target="_blank"}

[`LaplaceTransform[2 PDF[NormalDistribution[\mu,\sigma], x], x, r]`](https://www.wolframalpha.com/input?i=+LaplaceTransform%5B2+PDF%5BNormalDistribution%5B%5Cmu%2C%5Csigma%5D%2C+x%5D%2C+x%2C+r%5D){target="_blank"}

[`LaplaceTransform[2 PDF[NormalDistribution[0,1], x], x, r]`](https://www.wolframalpha.com/input?i=+LaplaceTransform%5B2+PDF%5BNormalDistribution%5B0%2C1%5D%2C+x%5D%2C+x%2C+r%5D){target="_blank"}

[`LaplaceTransform[PDF[GammaDistribution[b,a], x], x, r]`](https://www.wolframalpha.com/input?i=+LaplaceTransform%5BPDF%5BGammaDistribution%5Bb%2Ca%5D%2C+x%5D%2C+x%2C+r%5D){target="_blank"}

[`LaplaceTransform[PDF[ExponentialDistribution[1,a], x], x, r]`](https://www.wolframalpha.com/input?i=+LaplaceTransform%5BPDF%5BExponentialDistribution%5B1%2Ca%5D%2C+x%5D%2C+x%2C+r%5D){target="_blank"}

[`LaplaceTransform[PDF[ChiSquareDistribution[\nu], x], x, r]`](https://www.wolframalpha.com/input?i=+LaplaceTransform%5BPDF%5BChiSquareDistribution%5B%5Cnu%5D%2C+x%5D%2C+x%2C+r%5D){target="_blank"}

# Relações entre distribuições de probabilidade

* [Relationships among probability distributions: Wikipedia](https://en.wikipedia.org/wiki/Relationships_among_probability_distributions){target="_blank"}

* [`Univariate Distribution Relationships` ](https://www.math.wm.edu/~leemis/chart/UDR/UDR.html){target="_blank"}

```{r echo=FALSE, out.width="80%", fig.cap="Relações da distribuição normal com outras distribuições discretas e contínuas."}
knitr::include_graphics("./image/Normal_Relacoes.png")
```

```{r eval=TRUE, echo=FALSE, warning=FALSE, fig.align="left",  out.width="100%"}
knitr::include_graphics("./image/DistribRelations.png")
```

# Sistemas de Distribuição

> Kleiber & Kotz, 2003

* [List of probability distributions: Wikipedia ](https://en.wikipedia.org/wiki/List_of_probability_distributions){target="_blank"}

* [ProbOnto: ontology of probability distributions: Wikipedia ](https://en.wikipedia.org/wiki/ProbOnto){target="_blank"}

* [`distributional`: R package ](https://cran.r-project.org/package=distributional){target="_blank"}

* [`fitdistrplus`: R package: Fit of a Parametric Distribution to Non-Censored or Censored Data  ](https://cran.r-project.org/package=fitdistrplus){target="_blank"}

## Sistema de Pearson

> Podladchikova et al., 2003;  Andreev et al., 2007

* [Pearson distribution: Wikipedia ](https://en.wikipedia.org/wiki/Pearson_distribution){target="_blank"}

* [`PearsonDS`: R Package ](https://cran.r-project.org/web/packages/PearsonDS/PearsonDS.pdf){target="_blank"}

* [`cpd`: An R Package for Complex Pearson Distributions](https://mdpi-res.com/d_attachment/mathematics/mathematics-10-04101/article_deploy/mathematics-10-04101-v3.pdf?version=1668069285){target="_blank"}

O sistema de distribuições estatísticas mais amplamente conhecido é o celebrado sistema de Pearson, derivado por Karl Pearson na década de 1890 em conexão com seu trabalho sobre evolução. Ele contém muitas das distribuições univariadas contínuas mais conhecidas. 

Várias distribuições bem conhecidas pertencem à família de Pearson, como as distribuições Normal (Gaussiana), Gama, Beta e t de Student. O sistema foi introduzido por Karl Pearson em 1895, p. 381, que desenvolveu um conjunto de funções de densidade de probabilidade com quatro parâmetros como soluções para a equação diferencial:

$$
\dfrac{f^{\prime}(x)}{f(x)} =- \dfrac{a + (x - \mu)}{b_0 + b_1 (x - \mu) + b_2 (x - \mu)^2}
$$

sendo que \( f \) é uma função densidade e \( a, b_0, b_1 \) e \( b_2 \) são os parâmetros da distribuição.

$$
\dfrac{f^{\prime}(x)}{f(x)} = \dfrac{a + (x - \mu)}{b_0 + b_1 x + b_2 x^2}\\
x\dfrac{f^{\prime}(x)}{f(x)} = -\dfrac{x(a + (x - \mu))}{b_0 + b_1 x + b_2 x^2}\\
\dfrac{d\ln (f(x))}{d\ln (x)} = -\dfrac{x(a + (x - \mu))}{b_0 + b_1 x + b_2 x^2}\\
\eta(x) = -\dfrac{x(a + (x - \mu))}{b_0 + b_1 x + b_2 x^2}
$$

__Parâmetros e Momentos Centrais__

Os parâmetros \( a, b_0, b_1 \) e \( b_2 \) da distribuição são:

com:

\[
b_0 = \dfrac{4\beta_2 - 3\beta_1}{10\beta_2 - 12\beta_1 - 18\sigma^2},
\]

\[
a = b_1 = \dfrac{\sigma \sqrt{\beta_1} (\beta_2 + 3)}{10\beta_2 - 12\beta_1 - 18\sigma^2},
\]

\[
b_2 = \dfrac{2\beta_2 - 3\beta_1 - 6}{10\beta_2 - 12\beta_1 - 18\sigma^2}.
\]

onde os dois índices de momento 

$$\beta_1 = \dfrac{\mu_3}{\mu_2^{3/2}}= \dfrac{\mu_3}{\sigma^{3}}$$ 

e 

$$\beta_2 = \dfrac{\mu_4}{\mu_2^2}= \dfrac{\mu_4}{\sigma^4}$$

representam assimetria e curtose padronizados, respectivamente.

A solução mais proeminente é proporcional à função densidade de probabilidade (p.d.f.) normal padrão, que é obtida para \( a=b_1 = b_2 = 0 \) e $b_0=1$. Todas as soluções são unimodais; no entanto, os máximos podem estar localizados nas extremidades do suporte.

__Tabela 2.3: Distribuições de Pearson__

| Tipo | Densidade | Suporte |
|:-----|:----------|:--------|
| I    | \((1 + x)^{m_1}(1 - x)^{m_2}\) | \(-1 \leq x \leq 1\) |
| VI   | \(x^{m_2}(1 + x)^{m_1}\) | \(0 \leq x \leq 1\) |
| IV   | \((1 + x^2)^m \exp(v \arctan x)\) | \(-\infty < x < \infty\) |
| Normal | \(\exp\left(-\frac{x^2}{2}\right)\) | \(-\infty < x < \infty\) |
| II   | \((1 - x^2)^m\) | \(-1 \leq x \leq 1\) |
| VII  | \((1 + x^2)^m\) | \(-\infty < x < \infty\) |
| III  | \(x^m \exp(-x)\) | \(0 \leq x < \infty\) |
| V    | \(x^m \exp\left(-\frac{1}{x}\right)\) | \(0 \leq x < \infty\) |
| VIII | \((1 + x)^m\) | \(0 \leq x \leq 1\) |
| IX   | \((1 + x)^m\) | \(0 \leq x \leq 1\) |
| X    | \(\exp(x)\) | \(0 \leq x < \infty\) |
| XI   | \(x^m\) | \(1 \leq x < \infty\) |
| XII  | \([(g + x)(g - x)]^h\) | \(-g \leq x \leq g\) |

Nas aplicações, a variável \( x \) é frequentemente substituída por \(\dfrac{z - m}{s}\) para maior flexibilidade. A tabela contém várias distribuições familiares: o Tipo I é a distribuição beta do primeiro tipo, o Tipo VI é a distribuição beta do segundo tipo (com a distribuição F como um caso especial), o Tipo VII é uma generalização da distribuição t de Student, os Tipos III e V são as distribuições gama e gama inversa (ou invertida, ou recíproca), respectivamente, o Tipo X é a distribuição exponencial, e o Tipo XI é a distribuição de Pareto.

Uma característica chave do sistema de Pearson é que os quatro primeiros momentos (desde que existam) podem ser expressos em termos dos quatro parâmetros \( a \), \( b_0 \), \( b_1 \), \( b_2 \); por sua vez, as razões dos momentos

$$ \beta_1 = \dfrac{\mu_3}{\sigma^3} \quad (\text{assimetria padronizada})$$

e

$$ \beta_2 = \dfrac{\mu_4}{\sigma^4} \quad (\text{curtose padronizada}) $$

fornecem uma taxonomia completa das curvas de Pearson. De fato, Pearson sugeriu selecionar uma densidade apropriada com base nas estimativas de \( \beta_1 \) e \( \beta_2 \), que devem então ser ajustadas pelo seu método dos momentos.

As principais aplicações do sistema de Pearson estão, portanto, na aproximação de distribuições amostrais quando apenas momentos de ordem baixa estão disponíveis e na provisão de uma família de formas não-gaussianas razoavelmente típicas que podem ser usadas, entre outras coisas, em estudos de robustez.

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r}
PearsonDS::pearsonDiagram()
## Generate sample
DATA <- PearsonDS::rpearson(1000,
                            moments=c(mean=1,
                                      variance=2,
                                      skewness=1,
                                      kurtosis=5))
## find Pearson distribution with these parameters
ppar <- PearsonDS::pearsonFitML(DATA)
print(unlist(ppar))
## compare with method of moments estimator
print(unlist(PearsonDS::pearsonFitM(moments=PearsonDS::empMoments(DATA))))
```


Conforme Yang et al. (2019), a fórmula para o critério \(\omega\) é usada para determinar o tipo de distribuição de Pearson à qual uma variável aleatória \(X\) será aproximada, com base nos quatro primeiros momentos centrais: média, variância, assimetria e curtose. O critério \(\omega\) é definido como:

$$
\omega = \dfrac{\left(\beta_1(\beta_2 + 3)\right)^2}{4(4\beta_2 - 3\beta_1^2)(2\beta_2 - 3\beta_1^2 - 6)}
$$

A função densidade para a distribuição de Pearson do tipo IV é:

$$
y = y_0 \left(1 + \left(\dfrac{x - \lambda}{a}\right)^2\right)^{-m} \exp\left(-\nu \arctan \left( \dfrac{x - \lambda}{a} \right)\right)
$$

sendo que:

- \( m = \dfrac{1}{2}(r + 2) \)
- \( \nu = \dfrac{-r(r - 2) \beta_1}{\sqrt{16(r - 1) - \beta_1^2(r - 2)^2}} \)
- \( r = \dfrac{6(\beta_2 - \beta_1^2 - 1)}{2\beta_2 - 3\beta_1^2 - 6} \)
- \( a = \dfrac{\sigma}{4} \sqrt{16(r - 1) - \beta_1^2(r - 2)^2} \) : parâmetro de escala
- \( \lambda = \mu + \dfrac{\nu a}{r} \) : parâmetro de localização
- \( y_0 = \dfrac{N}{a F(r, \nu)} \) : coeficiente de normalização

**Tabela 1: Tipos de distribuições de Pearson**

| Tipo             | Critério \(\omega\) | Função de densidade                                       | Domínio         |
|:------------------|:---------------------|:----------------------------------------------------------|:-----------------|
| **I**            | \(\omega < 0\)      | \( f(x) = y_0 \left(1 + \dfrac{x}{a_1}\right)^{m_1} \left(1 - \dfrac{x}{a_2}\right)^{m_2} \) | \(-a_1 \leq x \leq a_2\) |
| **IV**           | \(0 < \omega < 1\)  | \( f(x) = y_0 \left(1 + \dfrac{x^2}{a^2}\right)^{-m} \exp\left(-\nu \arctan\left(\dfrac{x}{a}\right)\right) \) | \(-\infty < x < \infty\) |
| **VI**           | \(\omega > 1\)      | \( f(x) = y_0 (x - a)^{q_2} x^{-q_1} \)                  | \( a \leq x < \infty \) |
| **Normal**       | \(\omega = 0 (\beta_2 = 3)\) | \( f(x) = y_0 \exp\left(-\dfrac{x^2}{2\mu_2}\right) \)   | \(-\infty < x < \infty\) |
| **II**           | \(\omega = 0 (\beta_2 < 3)\) | \( f(x) = y_0 \left(1 - \dfrac{x^2}{a^2}\right)^m \)      | \(-a \leq x \leq a\) |
| **III**          | \(\omega = \pm \infty\) | \( f(x) = y_0 \left(1 + \dfrac{x}{a}\right)^\gamma a \exp(-\gamma x) \) | \(-a \leq x < \infty\) |
| **V**            | \(\omega = 1\)      | \( f(x) = y_0 x^{-p} \exp\left(-\dfrac{\gamma}{x}\right) \) | \( 0 < x < \infty \) |
| **VII**          | \(\omega = 0 (\beta_2 > 3)\) | \( f(x) = y_0 \left(1 + \dfrac{x^2}{a^2}\right)^{-m} \)  | \(-\infty < x < \infty\) |

|Type   | $\beta_1$| $\beta_2$ |
|:------|:-------------|:--------|
|Normal | 0.0          |3.0 |
|I      | 0.6          | 3.2     |
|II     | 0.0          | 2.6     |
|IV     | 1.4          | 8.6     |
|VI     | 2.0          | 11.2    |
|VII    | 0.0          | 8.4     |

```{r fig.align="left", echo=FALSE}
knitr::include_graphics("./image/logoR.png")
```

```{r}
# Definir os dados
data <- data.frame(
  Type = c("Normal", "I", "II", "IV", "VI", "VII"),
  beta1 = c(0.0, 0.6, 0.0, 1.4, 2.0, 0.0),
  beta2 = c(3.0, 3.2, 2.6, 8.6, 11.2, 8.4)
)

# Função para calcular o critério omega com verificações de denominador
calculate_omega <- function(beta1, beta2) {
  denom1 <- 4 * beta2 - 3 * beta1^2
  denom2 <- 2 * beta2 - 3 * beta1^2 - 6
  
  omega <- (beta1^2 * (beta2 + 3)^2) / (4 * denom1 * denom2)
  return(omega)
  }

# Calcular o critério omega para cada linha
data$omega <- mapply(calculate_omega, data$beta1, data$beta2)

# Verificar a classificação dos tipos de distribuição
data$Type_Calculated <- ifelse(
  data$omega < 0, "I",
  ifelse(data$omega > 1, "VI",
         ifelse(data$omega == 1, "V",
                ifelse(data$omega == 0 & data$beta2 == 3, "Normal",
                       ifelse(data$omega == 0 & data$beta2 < 3, "II",
                              ifelse(data$omega == 0 & data$beta2 > 3, "VII",
                                     ifelse(0 < data$omega & data$omega < 1, "IV", NA)))))))

# Exibir os resultados
data
```


## Sistema de Burr

* [Burr distribution: Wikipedia ](https://en.wikipedia.org/wiki/Burr_distribution){target="_blank"}

Entre os muitos sistemas alternativos de distribuições univariadas contínuas, também encontraremos alguns membros de um sistema introduzido por Irving Burr em 1942. Assim como o sistema de distribuições de Pearson, a família de Burr é definida em termos de uma equação diferencial; diferentemente do sistema de Pearson, essa equação diferencial descreve a função de distribuição e não a densidade. Isso tem a vantagem de formas fechadas para a função de distribuição cumulativa (c.d.f.), às vezes até mesmo para a função quantílica, o que raramente é o caso para os membros da família de Pearson.

__Tabela 2.4: Distribuições de Burr__

| Tipo | c.d.f. | Suporte |
|:------|:--------|:---------|
| I    | \( x \) | \(0 < x < 1\) |
| II   | \((1 + e^x)^p\) | \(-\infty < x < \infty\) |
| III  | \((1 + x^a)^p\) | \(0 < x < \infty\) |
| IV   | \( \left(1 + \left(\dfrac{c}{x}\right)^x\right)^{1/c} \) | \(0 < x < c\) |
| V    | \(\left[1 + c \exp(\tan x)\right]^q\) | \(-\dfrac{\pi}{2} < x < \dfrac{\pi}{2}\) |
| VI   | \(\left[1 + \exp(c \sinh x)\right]^q\) | \(-\infty < x < \infty\) |
| VII  | \(2^q(1 + \tanh x)^q\) | \(-\infty < x < \infty\) |
| VIII | \(\dfrac{2}{\pi} \arctan(e^x)^q\) | \(-\infty < x < \infty\) |
| IX   | \(\dfrac{1}{2} \left(2 + c\left[(1 + e^x)^q - 1\right]\right)\) | \(-\infty < x < \infty\) |
| X    | \([1 - \exp(-x^2)]^a\) | \(0 < x < \infty\) |
| XI   | \(x + \dfrac{1}{2\pi} \sin(2\pi x)^q\) | \(0 < x < 1\) |
| XII  | \(1 - (1 + x^a)^q\) | \(0 < x < \infty\) |

A equação diferencial básica que define as distribuições de Burr é dada por:

$$
F^{\prime}(x) = F(x)[1 - F(x)] g(x)
$$

onde \( g(x) \) é uma função não negativa. Quando \( g(x) = \dfrac{1}{F(1 - F)} \), obtemos a distribuição uniforme.

__Burr XII__

Uma das distribuições de Burr mais conhecidas é a Burr XII, que pode ser adaptada com parâmetros de localização e escala, resultando na forma:

$$
x = \dfrac{z - m}{s}
$$

__Elasticidade e a Proposta de Stoppa__

Stoppa propôs reescrever a equação diferencial das distribuições de Burr na forma:

$$
\dfrac{F^{\prime}(x)}{F(x)} = [1 - F(x)] \cdot g(x, F(x))
$$

e definiu a elasticidade como:

$$
h(x, F) = x\dfrac{F^{\prime}(x)}{F(x)}
$$

A proposta de Stoppa para a elasticidade é:

$$
h(x, F) = \dfrac{1 - [F(x)]^{1/u}}{[F(x)]^{1/u}} \cdot g(x, F(x)), \quad x \geq x_0 > 0
$$

onde \( u > 0 \) e \( g(x, y) \) é positiva em \( 0 < y < 1 \).

__Soluções da Equação Diferencial__

Para \( g(x, y) = g(x) \) e \( F(x) = 0 \) ou \( 1 \), e com a derivada de \( F^{1/u} \) em relação a \( F \):

$$
\dfrac{d F^{1/u}}{d F} = \dfrac{F^{1/u - 1}}{u}
$$

A solução da equação diferencial é:

$$
F(x) = 1 - \exp\left( - \int_0^x \ell(t) \, dt \right)
$$

onde \( \ell(t) = \dfrac{g(t)}{u t} \) é uma função integrável em um subconjunto de \( \mathbb{R}^+ \).

__Exemplos de Distribuições__

Dependendo da escolha de \( g(t) \), obtemos diferentes funções de distribuição acumulada (c.d.f.'s):

1. **I: \( g(t) = \dfrac{b}{t} \)**

   $$ F(x) = (b x)^u, \quad 0 < x < \dfrac{1}{b} $$

2. **II: \( g(t) = \dfrac{b}{1 - b t} \)**

   $$ F(x) = (1 - x b)^u, \quad 0 < x < 1 $$

3. **III: \( g(t) = b \)**

   $$ F(x) = (1 - e^{-b x})^u, \quad 0 < x < 1 $$

4. **IV: \( g(t) = b t \sec^2 t \)**

   $$ F(x) = \left[ 1 - \left(b x - 1\right)^{\dfrac{1}{u b}} \right]^u, \quad 0 < x < b $$

5. **V: \( g(t) = b \sec^2 x \)**

   $$ F(x) = \left( 1 - b \tan x \right)^u, \quad 0 < x < \dfrac{\pi}{2} $$

Esses exemplos ilustram a flexibilidade das distribuições de Burr e suas várias formas dependendo da escolha da função \( g(t) \). Cada forma tem suas próprias propriedades e aplicações em diferentes contextos estatísticos.

### Sistema Logístico Generalizado de Dagum

* [Dagum distribution: Wikipedia ](https://en.wikipedia.org/wiki/Dagum_distribution){target="_blank"}

Dagum atribuiu especial importância ao conceito de elasticidade-renda \(\eta(x, F)\):

\[ \eta(x, F) = \dfrac{x}{F(x)} \dfrac{dF(x)}{dx} = \dfrac{d \log F(x)}{d \log x} \]

Ele observou que a elasticidade-renda observada de uma função de distribuição cumulativa (c.d.f.) se comporta como uma função não linear e decrescente de \( F \). Para representar essa característica da elasticidade-renda, Dagum especificou (no caso mais simples) a equação diferencial:

\[ \eta(x, F) = ap \left[ 1 - (F(x))^{1/p} \right], \quad x \geq 0, \]

sujeito a \( p > 0 \) e \( ap > 0 \), o que leva à distribuição Dagum tipo I:

\[ F(x) = \left( 1 + \left( \dfrac{x}{b} \right)^{-a} \right)^{-p}, \quad x > 0, \]

onde \( a, b, p > 0 \).

O sistema de Pearson é um sistema de propósito geral, não necessariamente derivado de regularidades observadas em uma determinada área de aplicação. O sistema de D’Addario é um sistema de tradução com funções geradoras e de transformação flexíveis, construído para abranger o máximo possível de distribuições de renda. Em contraste, o sistema especificado por Dagum (1980b, c, 1983, 1990a) começa a partir das propriedades características das distribuições empíricas de renda e riqueza. Ele observa que a elasticidade da renda \( \eta(x, F) = \dfrac{d \log \{ F(x) \}}{d \log x} \) da c.d.f. de renda é uma função decrescente e limitada de \( F \), começando de um valor finito e positivo quando \( F(x) \to 0 \) e diminuindo para zero conforme \( F(x) \to 1 \), isto é, para \( x \to \infty \). Esse padrão leva à especificação do seguinte sistema gerador para distribuições de renda e riqueza:

$$ \dfrac{d \log \{ F(x) - \delta \}}{d \log x} = \nu(x) \phi(F) \le k, \quad 0 \leq x_0 < x < \infty, $$

onde \( k > 0 \), \( \nu(x) > 0 \), \( \phi(x) > 0 \), \( \delta < 1 \), e \( \dfrac{d \{ \nu(x) \phi(F) \}}{dx} < 0 \). Essas restrições asseguram que a elasticidade da renda da c.d.f. seja de fato uma função positiva, decrescente e limitada de \( F \) e, portanto, de \( x \). Para cada especificação de \( \phi \) e \( \nu \), obtém-se uma distribuição de renda. A Tabela 2.6 fornece uma seleção de modelos que podem ser deduzidos do sistema de Dagum.

Entre esses modelos, a distribuição Dagum (II) é principalmente usada como um modelo de distribuição de riqueza. Como a distribuição de Fisk também é conhecida como distribuição log-logística (ver Capítulo 6) e as distribuições de Burr III e Burr XII são generalizações dessa distribuição, Dagum (1983) referiu-se ao seu sistema como o sistema logístico-Burr generalizado. Esta coleção de distribuições pode ser ampliada ainda mais introduzindo parâmetros de localização e escala ou usando funções de transformação.

__Tabela 2.6 Sistema Gerador de Dagum__

| Distribuição       | \( \nu(x) \)                 | \( \phi(F) \)          | \((\delta, b)\)  | Suporte               |
|:--------------------|:----------------------------|:---------------------|------------|:-----------------------|
| Pareto (I)         | \( a \)                    | \( \dfrac{1 - F}{F} \) | (0, 0)    | \( 0 < x_0 \leq x < \infty \) |
| Pareto (II)        | \( \dfrac{a x}{x - c} \)   | \( \dfrac{1 - F}{F} \) | (0, 0)    | \( 0 < x_0 \leq x < \infty \) |
| Pareto (III)       | \( \dfrac{b x + a x}{x - c} \) | \( \dfrac{1 - F}{F} \) | (0, \(\infty\)) | \( 0 < x_0 \leq x < \infty \) |
| Benini             | \( 2 a \log x \)           | \( \dfrac{1 - F}{F} \) | (0, 0)    | \( 0 < x_0 \leq x < \infty \) |
| Weibull            | \( b x (x - c)^{a-1} \)    | \( \dfrac{1 - F}{F} \) | (0, \(\infty\)) | \( c \leq x < \infty \)       |
| log-Gompertz       | \( \log a - \log F \)      | \( (0, 0) \)         | (0, 0)    | \( 0 \leq x < \infty \)       |
| Fisk               | \( a \)                    | \( 1 - F \)         | (0, 0)    | \( 0 \leq x < \infty \)       |
| Singh–Maddala      | \( a \)                    | \( \dfrac{(1 - F)^b}{F(1 - F)} \) | (0, \(\infty\)) | \( 0 \leq x < \infty \)       |
| Dagum (I)          | \( a \)                    | \( 1 - F^{1/b} \)   | (0, \(\infty\)) | \( 0 \leq x < \infty \)       |
| Dagum (II)         | \( a \)                    | \( 1 - \left( F^{1/a} - 1 \right)^{1/b} \) | (\(\infty\), \(\infty\)) | \( 0 \leq x < \infty \)       |
| Dagum (III)        | \( a \)                    | \( 1 - \left( F^{1/a} - 1 \right)^{1/b} \) | (-\(\infty\), \(\infty\)) | \( 0 < x_0 \leq x < \infty \) |


# Bibliografia

* Andreev, A et al. (2007) Computational Examples of a New Method for Distribution Selection in the Pearson System. _Journal of Applied Statistics_ 34(4): 487-506. DOI: 10.1080/02664760701231922
* Batschelet, E (1979) _Introduction to mathematics for life scientists_. 3rd ed. NY: Springer.
* Batschelet, E (1978) _Introdução à matemática para biocientistas_. Tradução da 2ª ed. São Paulo: EDUSP e Rio de Janeiro: Interciência.
* Bohrnstedt, GW & Goldberger, AS (1969) On the Exact Covariance of Products of Random Variables. _Journal of the American Statistical Association_ 64(328): 1439–42. https://doi.org/10.2307/2286081
* Craig, CC (1936) On the Frequency Function of xy. _The Annals of Mathematical Statistics_ 7(1): 1–15. http://www.jstor.org/stable/2957506
* Crooks, GE (2022) Field Guide to Probability Distributions. Berkeley Institute for Theoretical Sciences (BITS).  https://threeplusone.com/fieldguide 
* Limpert, E  et al. (2001) Log-normal Distributions across the Sciences: Keys and Clues. _BioScience_ 51(5): 341-52. https://doi.org/10.1641/0006-3568(2001)051[0341:LNDATS]2.0.CO;2
* Giordano, FR et al. (2015) _A first course in mathematical modeling_. 5th ed. OH: Thomson. 
* Hayya, J et al. (1975) A Note on the Ratio of Two Normally Distributed Variables. _Management Science_ 21(11): 1338–41. doi:10.1287/mnsc.21.11.1338 * Hinkley, DV (1969) On the ratio of two correlated normal random variables. _Biometrika_ 56(3): 635–9. doi:10.1093/biomet/56.3.635
* Hinkley, DV (1970) Correction: On the Ratio of Two Correlated Normal Random Variables. _Biometrika_ 57(3): 683. doi:10.2307/2334796
* Marsaglia, G (2006) Ratios of Normal Variables. _Journal of Statistical Software_. 16(1): 1-10.
* Morisugi, H et al. (2009) Confidence Interval for the Ratio of Two Normal Variables: An Application to Value of Time. _Interdisciplinary Information Sciences_ 15(1): 37-43.
* Hool, JN & Maghsoodloo, S (1980) Normal Approximation to Linear Combinations of Independently Distributed Random Variables. _A I I E Transactions_ 12(2): 140–4. doi:10.1080/0569555800897450
* Jamalizadeh, A et al. (2009) Distributions of Ratios of Two Correlated Skew-Normal Variables and of Ratios of Two Linear Functions of Order Statistics from Bivariate Normal Distribution. _Communications in Statistics - Theory and Methods_ 38(12): 2107–15. doi:10.1080/03610920802226550
* Kato, FH (2005) Análise de carteiras em tempo discreto. Dissertação de mestrado orientada por Prof. José de Oliveira Siqueira. https://doi.org/10.11606/D.12.2004.tde-24022005-005812 https://teses.usp.br/teses/disponiveis/12/12139/tde-24022005-005812/pt-br.php
* Kleiber, C & Kotz, S (2003) _Statistical size distributions in economics and actuarial sciences_. NJ: Wiley.
* Ku, HH (1966) Notes on the Use of Propagation of Error Formulas. _JOURNAL OF RESEARCH of the National Bureau of Standards - C. Engineering and Instrumentation_ 70(4): 263-73.
* Nadarajah, S & Kotz, S (2007) Financial Pareto ratios. _Quantitative Finance_ 7(3): 257–260. https://doi.org/10.1080/14697680601067604
* Nadarajah, S (2005) A generalized normal distribution. _Journal of
Applied Statistics_ 32(7): 685-94. DOI: 10.1080/02664760500079464
* Park, CS (1986) The Mellin Transform in Probabilistic Cash Flow Modeling. _The Engineering Economist_ 32(2): 115–34. https://doi.org/10.1080/00137918608902958
* Park, CS & Sharp-Bete, GP (1990) _Advanced engineering economics_. NJ: Wiley.
* Pham-Gia, T et al. (2006) Density of the Ratio of Two Normal Random Variables and Applications. _Communications in Statistics - Theory and Methods_ 35(9): 1569–91. doi:10.1080/0361092060068368
* Podladchikova, O et al. (2003) Classification of probability densities on the basis of Pearson’s curves with application to coronal heating simulations. _Nonlinear Processes in Geophysics_ 10(4/5): 323-33. ff10.5194/npg-10-323-2003ff. ffhal-00331066
* Poularikas, AD (1999) _The Handbook of Formulas and Tables for Signal Processing_. Boca Raton: CRC/Springer/IEEE. 
* Reed, GF et al. (2002) Use of coefficient of variation in assessing variability of quantitative assays. Clinical and diagnostic laboratory immunology 9(6): 1235–39. https://doi.org/10.1128/cdli.9.6.1235-1239.2002
* Rice, JA (2007) _Mathematical Statistics and Data Analysis_. 3rd ed. USA: Duxbury. 
* Shoukri, MM & Aleid, MM (2022) Quasi-Negative Binomial: Properties, Parametric Estimation, Regression Model and Application to RNA-SEQ
Data. _Open Journal of Statistics_ 12: 216-37.
* Silverman, MP (2023) Probability Distribution of SARS-Cov-2 (COVID) Infectivity Following Onset of Symptoms: Analysis from First Principles.
_Open Journal of Statistics_, 13, 233-263. https://doi.org/10.4236/ojs.2023.132013
* Silverman, MP & Lipscombe, TC (2022) Exact Statistical Distribution of the Body Mass Index (BMI): Analysis and Experimental Confirmation. _Open Journal of Statistics_ 12: 324-56. https://doi.org/10.4236/ojs.2022.123022
* Siqueira, JO (2012) _Fundamentos de métodos quantitativos_: aplicados em Administração, Economia, Contabilidade e Atuária usando WolframAlpha e SCILAB. São Paulo: Saraiva. Soluções dos exercícios em  https://www.researchgate.net/publication/326533772_Fundamentos_de_metodos_quantitativos_-_Solucoes.
* Siqueira, AL & Tibúrcio, JD (2011) _Estatística na Área de Saúde: conceitos, metodologia, aplicações e prática computacional_. BH: Coopmed.
* Shanmugalingam, S (1982) On the Analysis of the Ratio of Two Correlated Normal Variables. _The Statistician_ 31(3): 251. doi:10.2307/2987992 
* van Belle, G (2008) _Statistical Rules of Thumb_. 2nd ed. NJ: Wiley.

# Probabilidade em YouTube 

* [Ace Tutors: Mutually Exclusive vs. Independent Events EXPLAINED in 4 minutes](https://www.youtube.com/watch?v=aVqmWW3xmdU){target="_blank"}

* [D!ng: The Monty Hall problem](https://www.youtube.com/watch?v=TVq2ivVpZgQ){target="_blank"}

* [3Blue1Brown: Why π is in the normal distribution (beyond integral tricks)](https://www.youtube.com/watch?v=cy8r7WSuT1I&t=1017s){target="_blank"}

* [3Blue1Brown: But what is the Central Limit Theorem?](https://www.youtube.com/watch?v=zeJD6dqJ5lo){target="_blank"}

# Probabilidade em Wikipedia

[Probability](https://en.wikipedia.org/wiki/Probability){target="_blank"}

[Probability theory](https://en.wikipedia.org/wiki/Probability_theory){target="_blank"}

[Random variable](https://en.wikipedia.org/wiki/Random_variable){target="_blank"}

[Probability density function](https://en.wikipedia.org/wiki/Probability_density_function){target="_blank"}

[Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution){target="_blank"}

[Cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function){target="_blank"}

[List of probability distributions](https://en.wikipedia.org/wiki/List_of_probability_distributions){target="_blank"}

[Relationships among probability distributions](https://en.wikipedia.org/wiki/Relationships_among_probability_distributions){target="_blank"}

[Central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem){target="_blank"}

[Stochastic process](https://en.wikipedia.org/wiki/Stochastic_process){target="_blank"}


